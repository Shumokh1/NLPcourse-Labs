{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMHV78zt5rLb"
      },
      "source": [
        "### Evaluate Topic Model in Python: Latent Dirichlet Allocation (LDA)\\\n",
        "##### A step-by-step guide to building interpretable topic models\n",
        "\n",
        "** **\n",
        "*Preface: This article aims to provide consolidated information on the underlying topic and is not to be considered as the original work. The information and the code are repurposed through several online articles, research papers, books, and open-source code*\n",
        "** **\n",
        "\n",
        "In the previous [article](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0), I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using Gensim implementation.\n",
        "\n",
        "Pursuing on that understanding, in this article, we’ll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic coherence and share the code template in python using Gensim implementation to allow for end-to-end model development.\n",
        "\n",
        "### Why evaluate topic models?\n",
        "\n",
        "![img](https://tinyurl.com/y3xznjwq)\n",
        "\n",
        "We know probabilistic topic models, such as LDA, are popular tools for text analysis, providing both a predictive and latent topic representation of the corpus. However, there is a longstanding assumption that the latent space discovered by these models is generally meaningful and useful, and that evaluating such assumptions is challenging due to its unsupervised training process. Besides, there is a no-gold standard list of topics to compare against every corpus.\n",
        "\n",
        "Nevertheless, it is equally important to identify if a trained model is objectively good or bad, as well have an ability to compare different models/methods. To do so, one would require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if “the correct thing” has been learned about the corpus, an implicit knowledge and “eyeballing” approaches are used. Ideally, we’d like to capture this information in a single metric that can be maximized, and compared.\n",
        "\n",
        "Let’s take a look at roughly what approaches are commonly used for the evaluation:\n",
        "\n",
        "**Eye Balling Models**\n",
        "- Top N words\n",
        "- Topics / Documents\n",
        "\n",
        "**Intrinsic Evaluation Metrics**\n",
        "- Capturing model semantics\n",
        "- Topics interpretability\n",
        "\n",
        "**Human Judgements**\n",
        "- What is a topic\n",
        "\n",
        "**Extrinsic Evaluation Metrics/Evaluation at task**\n",
        "- Is model good at performing predefined tasks, such as classification\n",
        "\n",
        "Natural language is messy, ambiguous and full of subjective interpretation, and sometimes trying to cleanse ambiguity reduces the language to an unnatural form. In this article, we’ll explore more about topic coherence, an intrinsic evaluation metric, and how you can use it to quantitatively justify the model selection.\n",
        "\n",
        "### What is Topic Coherence?\n",
        "\n",
        "Before we understand topic coherence, let’s briefly look at the perplexity measure. Perplexity as well is one of the intrinsic evaluation metric, and is widely used for language model evaluation. It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set.\n",
        "\n",
        "Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
        "\n",
        "However, recent studies have shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated.\n",
        "\n",
        "*Optimizing for perplexity may not yield human interpretable topics*\n",
        "\n",
        "This limitation of perplexity measure served as a motivation for more work trying to model the human judgment, and thus *Topic Coherence*.\n",
        "\n",
        "The concept of topic coherence combines a number of measures into a framework to evaluate the coherence between topics inferred by a model. But before that…\n",
        "\n",
        "#### What is topic coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But,\n",
        "\n",
        "#### What is coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But …\n",
        "\n",
        "### Coherence Measures\n",
        "Let’s take quick look at different coherence measures, and how they are calculated:\n",
        "\n",
        "1. `C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
        "2. `C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson's coherence\n",
        "3. `C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
        "4. `C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
        "5. `C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)\n",
        "6. `C_a` is based on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
        "\n",
        "There is, of course, a lot more to the concept of topic model evaluation, and the coherence measure. However, keeping in mind the length, and purpose of this article, let’s apply these concepts into developing a model that is at least better than with the default parameters. Also, we’ll be re-purposing already available online pieces of code to support this exercise instead of re-inventing the wheel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_JQxNft5rLd"
      },
      "source": [
        "### Model Implementation\n",
        "1. Loading Data\n",
        "2. Data Cleaning\n",
        "3. Phrase Modeling: Bi-grams and Tri-grams\n",
        "4. Data Transformation: Corpus and Dictionary\n",
        "5. Base Model\n",
        "6. Hyper-parameter Tuning\n",
        "7. Final model\n",
        "8. Visualize Results\n",
        "\n",
        "** **\n",
        "\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file\n",
        "\n",
        "** **\n",
        "#### Step 1: Loading Data\n",
        "** **\n",
        "\n",
        "For this tutorial, we’ll use the dataset of papers published in NIPS conference. The NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "T_m46Diy5rLe",
        "outputId": "1074d488-1703-45aa-e37d-e298f74d8701"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "papers"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-15e9efd8-a167-4e40-821c-aba99711603b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15e9efd8-a167-4e40-821c-aba99711603b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-15e9efd8-a167-4e40-821c-aba99711603b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-15e9efd8-a167-4e40-821c-aba99711603b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-57617209-abc5-4425-8f26-4477d0073ffc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-57617209-abc5-4425-8f26-4477d0073ffc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-57617209-abc5-4425-8f26-4477d0073ffc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"/content/NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRR0yim05rLf"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "LxGLtXO65rLf",
        "outputId": "c1bd19c9-62a0-467f-986b-ba629be7c4b7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Analysis of Information in Speech based\\non MANOVA\\nSachin s. Kajarekarl and Hynek Hermansky l ,2\\n1 Department of Electrical and Computer Engineering\\nOGI School of Science and Engineering at OHSU\\nBeaverton, OR\\n2International Computer Science Institute\\nBerkeley, CA\\n{ sachin,hynek} @asp.ogi.edu\\n\\nAbstract\\nWe propose analysis of information in speech using three sources\\n- language (phone), speaker and channeL Information in speech is\\nmeasured as mutual information between the source and the set of\\nfeatures extracted from speech signaL We assume that distribution of features can be modeled using Gaussian distribution. The\\nmutual information is computed using the results of analysis of\\nvariability in speech. We observe similarity in the results of phone\\nvariability and phone information, and show that the results of the\\nproposed analysis have more meaningful interpretations than the\\nanalysis of variability.\\n\\n1\\n\\nIntroduction\\n\\nSpeech signal carries information about the linguistic message, the speaker, the\\ncommunication channeL In the previous work [1, 2], we proposed analysis of information in speech as analysis of variability in a set of features extracted from\\nthe speech signal. The variability was measured as covariance of the features , and\\nanalysis was performed using using multivariate analysis of variance (MANOVA).\\nTotal variability was divided into three types of variabilities, namely, intra-phone\\n(or phone) variability, speaker variability, and channel variability. Effect of each\\ntype was measured as its contribution to the total variability.\\nIn this paper, we extend our previous work by proposing an information-theoretic\\nanalysis of information in speech. Similar to MANOVA, we assume that speech\\ncarries information from three main sources- language, speaker, and channeL We\\nmeasure information from a source as mutual information (MI) [3] between the\\ncorresponding class labels and features. For example, linguistic information is measured as MI between phone labels and features. The effect of sources is measured\\nin nats (or bits). In this work, we show it is easier to interpret the results of this\\nanalysis than the analysis of variability.\\nIn general, MI between two random variables X and Y can be measured using\\nthree different methods [4]. First, assuming that X and Y have a joint Gaussian\\n\\n\\fdistribution. However, we cannot use this method because one of the variables - a set\\nof class labels - is discrete. Second, modeling distribution of X or Y using parametric\\nform , for example, mixture of Gaussians [4]. Third, using non-parametric techniques\\nto estimate distributions of X and Y [5]. The proposed analysis is based on the\\nsecond method, where distribution of features is modeled as a Gaussian distribution.\\nAlthough it is a strong assumption, we show that results of this analysis are similar\\nto the results obtained using the third method [5].\\nThe paper is organized as follows. Section 2 describes the experimental setup.\\nSection 3 describes MAN OVA and presents results of MAN OVA. Section 4 proposes\\ninformation theoretic approach for analysis of information in speech and presents\\nthe results. Section 5 compares these results with results from the previous study.\\nSection 6 describes the summary and conclusions from this work.\\n\\n2\\n\\nExperimental Setup\\n\\nIn the previous work [1 , 2], we have analyzed variability in the features using three\\ndatabases - HTIMIT, OGI Stories and TIMIT. In this work, we present results\\nof MAN OVA using OGI Stories database; mainly for the comparison with Yang's\\nresults [5, 6]. English part of OGI Stories database consists of 207 speakers, speaking\\nfor approximately 1 minute each. Each utterance is transcribed at phone level.\\nTherefore, phone is considered as a source of variability or source of information.\\nThe utterances are not labeled separately by speakers and channels, so we cannot\\nmeasure speaker and channel as separate sources. Instead, we assume that different\\nspeakers have used different channels and consider speaker+channel as a single\\nsource of variability or a single source of information.\\nFigure 1 shows a commonly used time-frequency representation of energy in speech\\nsignal. The y-axis represents frequency, x-axis represents time, and the darkness of\\neach element shows the energy at a given frequency and time. A spectral vector is\\ndefined by the number of points on the y-axis, S(w, t m ). In this work, this vector\\ncontains 15 points on Bark spectrum. The vector is estimated at every 10 ms using a\\n25 ms speech segment. It is labeled by the phone and the speaker and channel label\\nof the corresponding speech segment. A temporal vector is defined by a sequence\\nof points along time at a given frequency, S(wn, t). In this work, it consists of\\n50 points each in the past and the future with respect to the current observation\\nand the observation itself. As the spectral vectors are computed every 10 ms, the\\ntemporal vector represents 1 sec of temporal information. The temporal vectors\\nare labeled by the phone and the speaker and channel label of the current speech\\nsegment. In this work, the analysis is performed independently using spectral and\\ntemporal vectors.\\n\\n3\\n\\nMANOVA\\n\\nMultivariate analysis of variance (MANOVA) [7] is used to measure the variation\\nin the data, {X E R n }, with respect to two or more factors. In this work, we use\\ntwo factors - phone and speaker+channel. The underline model of MAN OVA is\\n\\n(1)\\nwhere, i = 1\\\"\\\" ,p, represents phones, j = 1\\\"\\\" Be, represents speakers and channels. This equation shows that any feature vector, X ijk , can be approximated using\\na sum of X.. , the mean of the data; Xi ., mean of the phone i; Xij., mean of the\\nspeaker and channel j, and phone i; and Eij k, an error in this approximation. Using\\n\\n\\f~r- l0ms\\n\\nIi I\\n\\nJJ I 1M\\nI\\n\\nLL\\n\\nI\\n\\nI~I 11 IH~ifl~\\\" III\\nI'I\\n\\n~i\\n\\nI\\n\\nI I1I11\\n\\n1\\n' 1!\\nI~\\nI ~ [I\\n1111\\n\\n1;1, I\\n\\n?\\n\\n?\\n\\ni\\n\\nI III\\n\\n1II1\\n\\nIHII\\nI\\n\\nn\\n\\nI[l l~\\n\\n'I\\n\\n~\\n\\n11111\\n\\nI\\n\\nIl:\\n\\nI II ,UI\\nI\\n\\n~IJ' n'I ~\\nil [ItJ:ii'\\nl!I I I'\\\"\\\"I~ \\\"'I 'I ~'I\\nu\\n\\n1'1\\nilJU JI II,I \\\"i...i\\n\\n\\\"\\n\\nuJlJ\\n\\n111 1'111\\n\\n1111\\n\\n[III\\n\\nr1111\\n\\nIII\\n\\nII I\\n\\nTemporal Vector\\n(Temporal Domain)\\n\\n1111\\n\\n---:JIIIII_ _ __\\n\\nSpectral Vector\\n(Spectral Domain)\\n\\nFigure 1: Time-frequency representation of logarithmic energies from speech signal\\nthis model, the total covariance can be decomposed as follows\\n~total = ~p\\n\\n+ ~s c + ~re sidual\\n\\n(2)\\n\\nwhere\\nN (X... _\\n\\n\\\"\\\"\\\"' N i\\n~\\n\\n~sc\\n\\nLL -Nij\\nN (X, ZJ\\n\\ni\\n\\n~r esidual\\n\\nX .. )t (X... - X .. )\\n-\\n\\nt\\n\\n-\\n\\n-\\n\\nX .. ) (X,\\nZJ - X z. )\\n\\nj\\n\\n1\\\"\\\"\\\",,,\\\"\\\",,,\\\"\\\",\\n\\n-\\n\\nt\\n\\n-\\n\\nN ~ ~ ~(Xijk - Xij) (Xijk - Xij)\\ni\\n\\nj\\n\\nk\\n\\nand, N is the data size and Nijk refers to the number of samples associated with\\nthe particular combination of factors (indicated by the subscript).\\nThe covariance terms are computed as follows. First, all the feature vectors (X)\\nbelonging to each phone i are collected and their mean (Xi) is computed. The\\ncovariance of these phone means, ~p, is the estimate of phone variability. Next, the\\ndata for each speaker and channel j within each phone i is collected and the mean\\nof the data (X ij ) is computed. The covariance of the means of different speakers\\naveraged over all phones, ~s c, is the estimate of speaker variability. All the variability in the data is not explained using these sources. The unaccounted sources,\\nsuch as context and coarticulation, cause variability in the data collected from\\none speaker speaking one phone through one channel. The covariance within each\\nphone, speaker, and channel is averaged over all phones, speakers, and channels,\\nand the resulting covariance, ~r e sidual, is the estimate of residual variability.\\n3.1\\n\\nResults\\n\\nResults of MAN OVA are interpreted at two levels - feature element and feature\\nvector. Results for each feature element are shown in Figure 2. Table 1 shows the\\nresults using the complete feature vector. The contribution of different sources is\\ncalculated as trace (~source )ltrace(~total). Note that this measure cannot be used\\nto compare variabilities across feature-sets with different number of features. Therefore, we cannot directly compare contribution of variabilities in time and frequency\\ndomains. For comparison, contribution of sources in temporal domain is calculated\\n\\n\\fTable 1: Contribution of sources in spectral and temporal domains\\no contribution\\nsource\\npectral Domain Temporal Domain\\nphone\\n35.3\\n4.0\\nspeaker+channel\\n41.1\\n30.3\\n\\nas trace(EtI',source E) /trace(EtI',totaIE) , where\\neigenvectors of I',total.\\n\\nElOl x 15\\n\\nis a matrix of 15 leading\\n\\nIn spectral domain, the highest phone variability is between 4-6 Barks. The highest\\nspeaker and channel variability is between 1-2 Barks where phone variability is\\nthe lowest. In temporal domain, phone variability spreads for approximately 250\\nms around the current phone. Speaker and channel variability is almost constant\\nexcept around the current frame. This deviation is explained by the difference in\\nt he phonetic context among the phone instances across different speakers. Thus,\\nfeatures for speakers within a phone differ not only because of different speaker\\ncharacteristics but also different phonetic contexts. This deviation is also seen in\\nthe speaker and channel information in the proposed analysis. In the overall results\\nfor each domain, spectral domain has higher variability due to different phones than\\ntemporal domain. It also has higher speaker and channel variability than temporal\\ndomain.\\nThe disadvantage of this analysis is that it is difficult to interpret the results. For\\nexample, how much phone variability is needed for perfect phone recognition? and\\nis 4% of phone variability in temporal domain significant? In order to answer these\\nquestions, we propose an information theoretic analysis.\\n\\nPhone\\nSpeaker+Channel\\n7r,----------------------~\\n\\n6\\n\\n7 ,---~----~--------~\\n\\n,\\n\\n~\\n\\n6\\n5\\n\\n5 '\\nQ)\\n\\n\\\"'\\n\\ng 4 ,.. ,. .. , _ ,_ , _ ,\\n\\n..\\n\\n<1l\\n\\n_ , _ ,_ ' - ,- '\\n...\\n\\n;\\n\\n'?3\\n>\\n\\n2\\n\\nO L-----~----~------~\\n\\n5\\n\\n10\\n\\nFrequency\\n(Critical Band Index)\\n\\n15\\n\\n-250\\n\\n0\\nTime (ms)\\n\\nFigure 2: Results of analysis of variability\\n\\n250\\n\\n\\f4\\n\\nInformation-theoretic Analysis\\n\\nResults of MAN OVA can not be directly converted to MI because the determinant\\nof source and residual covariances do not add to the determinant of total covariance.\\nTherefore, we propose a different formulation for the information theoretic analysis\\nas follows. Let {X E Rn} be a set of feature vectors, with probability distribution\\np(X). Let h(X) be the entropy of X. Let Y = {Y1 , ... , Ym} be a set of different\\nfactors and each Yi be a set of classes within each factor. For example, we can\\nassume that Y1 = {yf} represents phone factor and each yf represent a phone class.\\nLets assume that X has two parts; one completely characterized by Y and another\\npart, Z , characterized by N(X) \\\"\\\"' N(O, Jn xn ), where J is the identity matrix. Let\\nJ (X; Y) be the MI between X and Y. Assuming that we consider all the possible\\nfactors for our analysis,\\nJ(X;Y) = J(X;Y1, ... , Ym) = h(X)-h(X/Yl , ... ,Ym) = h(X)-h(Z) = D(PIIN) ,\\n\\nwhere D() is the kullback-liebler distance [3] between distributions P and N. Using\\nthe chain-rule, the left hand side can be expanded as follows ,\\nm\\n\\nJ(X; Y1 ,?.?, Yn ) = J(X; Yd\\n\\n+ J(X; Y2 /Yd + l: J(X; Yi/Yi - l\\\"'\\\"\\n\\nY2 , Yd?\\n\\n(3)\\n\\ni=3\\n\\nIf we assume that there are only two factors Y1 and Y2 used for the analysis, then\\nthis equation is similar to the decomposition performed using MAN OVA (Equation\\n2). The term on the left hand side is entropy of X which is the total information\\nin X that can be explained using Y . This is similar to the left-hand side term in\\nMANOVA that describes the total variability. On the right hand side, first term\\nis similar to the phone variability, second term is similar to the speaker variability,\\nand the last term which calculates the effect of unaccounted factors (Y3 , ... , Y m ) is\\nsimilar to the residual variability.\\n\\nFirst and second terms on the right hand side of Equation 3 are computed as follows.\\n\\nYd = h(X) -\\n\\n(4)\\nJ(X; Y2 /Yd = h(X/Yd - h(X/Y1, Y2 ).\\n(5)\\nh () terms are estimated using parametric approximation to the total and conditional distribution It is assumed that the total distribution of features is a Gaussian\\ndistribution with covariance ~. Therefore, h (X) = ~ log (2net I~I. Similarly, we\\nassume that the distribution of features of different phones (i) is a Gaussian distribution with covariances ~i' Therefore,\\nJ(X;\\n\\nh(X/Y1) =\\n\\n~\\n\\nh(X/Yd\\n\\nl: p (y~)Iog (2net I~il\\n\\n(6)\\n\\nyiCYi\\n\\nFinally, we assume that the distribution of features of different phones spoken by\\ndifferent speakers is also a Gaussian distribution with covariances ~ij. Therefore,\\nh(X/Y1,Y2 )\\n\\n=~\\n\\np(yLYOlog(2netl~ijl\\n\\nl:\\n\\n(7)\\n\\ny;CY1,y~CY2\\n\\nSubstituting equations 6 and 7 in equations 4 and 5, we get\\n\\nJ(X ' Y;)=~lo\\n,\\n\\n1\\n\\n2\\n\\n1\\nJ(X;Y2 /Yd = -log\\n2\\n\\ng\\n\\nI~I\\n\\nIT.Yi CY;\\nIT?\\n\\nIT\\n\\n.\\n\\n(8)\\n\\n1~ ,' IP(Yil\\nI~'IP(Y;)\\n\\nYicY;'\\ni\\nj\\nYl CY1 'Y2 CY2\\n\\n.\\n\\nj\\n\\nI~i IP(Yi ' Y2)\\n\\n(9)\\n\\n\\fPhone\\nSpeaker +Channel\\n\\n0.6 ,----~--~-~-------,\\n0.5\\nUl\\n\\n1.5\\n\\nca\\n\\n-:2:c\\n\\n,i\\n\\n,\\n0.5\\n\\n\\\\\\n\\n:2: 0.2 , .. ,_ , .. ,_ ,' ,; -\\n\\n\\\\\\n\\\\\\n\\n-' , - ,- ,- ,- ,_ ,_ ,_ 1- '\\\"\\n\\n.... ,. - .'..... , -\\n\\n, _.,\\n\\n\\\"\\n\\n0.1\\n5\\n\\n10\\n\\nFrequency\\n(Critical Band Index)\\n\\n15\\n\\n-250\\n\\n0\\n\\n250\\n\\nTime (ms)\\n\\nFigure 3: Results of information-theoretic analysis\\n\\nTable 2: Mutual information between features and phone and speaker and channel\\nlabels in spectral and temporal domains\\nsource\\nphone\\nspeaker+ channel\\n\\n4 .1\\n\\nResults\\n\\nFigure 3 shows the results of information-theoretic analysis in spectral and temporal domain. These results are computed independently for each feature element.\\nIn spectral domain, phone information is highest between 3-6 Barks. Speaker and\\nchannel information is lowest in that range and highest between 1-2 Barks. Since\\nOGI Stories database was collected over different telephones, speaker+ channel information below 2 Barks ( :=::: 200 Hz ) is due to different telephone channels. In\\ntemporal domain, the highest phone information is at the center (0 ms). It spreads\\nfor approximately 200 ms around the center. Speaker and channel information is\\nalmost constant across t ime except near the center.\\nNote that the nature of speaker and channel variability also deviates from the constant around the current frame. But, at the current frame , phone variability is\\nhigher than speaker and channel variability. The results of analysis of informat ion show that, at the current frame, phone information is lower than speaker and\\nchannel information. This difference is explained by comparing our MI results with\\nresults from Yang et. al. [6] in the next section.\\nTable 2 shows the results for the complete feature vector. Note that there are some\\npractical issues in computing determinants in Equation 4 and 5. They are related\\nto data insufficiency, specifically, in temporal domain where the feature vector is\\n101 points and there are approximately 60 vectors per speaker per phone. We ob-\\n\\n\\fserve that without proper conditioning of covariances, the analysis overestimates\\nMI (l(X ; Yl , Y2 ) > H(Yl , Y2 )). This is addressed using the condition number to\\nlimit the number of eigenvalues used in the calculation of determinants. Our hypothesis is that in presence of insufficient data, only few leading eigen vectors are\\nproperly estimated. We have use condition number of 1000 to estimate determinant\\nof ~ and ~i, and condition number of 100 to estimate the determinant of ~ij. The\\nresults show that phone information in spectral domain is 1.6 nats. Speaker and\\nchannel information is 0.5 nats. In temporal domain, phone information is about\\n1.2 nats. Speaker and channel information is 5.9 nats. Comparison of results from\\nspectral and temporal domains shows that spectral domain has higher phone information than temporal domain. Temporal domain has higher speaker and channel\\ninformation than spectral domain.\\nUsing these results, we can answer the questions raised in Section 3. First question\\nwas how much phone variability is needed for perfect phone recognition? The answer to the question is H(Yd, because the maximum value of leX; Yd is H(Yd?\\nWe compute H(Yl ) using phone priors. For this database, we get H(Yl ) = 3.42\\nnats, that means we need 3.42 nats of information for perfect phone recognition.\\nQuestion about significance of phone information in temporal domain is addressed\\nby comparing it with information-less MI level. The information-less MI is computed as MI between the current phone label and features at 500 ms in the past\\nor in the future . From our results, we get information-less MI equal to 0.0013 nats\\nconsidering feature at 500 ms in the past, and 0.0010 nats considering features at\\n500 ms in the future l . The phone information in temporal domain is 1.2 bits that\\nis greater than both the levels. Therefore it is significant.\\n\\n5\\n\\nResults in Perspective\\n\\nIn the proposed analysis, we estimated MI assuming Gaussian distribution for the\\nfeatures. This assumption is validated by comparing our results with the results\\nfrom a study by Yang, et. al.,[6], where MI was computed without assuming any\\nparametric model for the distribution of features. Note that only entropies can\\nbe directly compared for difference in the estimation technique [3]. However, MI\\nusing Gaussian assumption can be equal to, less or more than the actual MI. In\\nthe comparison of our results with Yang's results , we consider only the nature of\\ninformation observed in both studies. The difference in actual MI levels across the\\ntwo studies is related to the difference in the estimation techniques.\\nIn spectral domain, Yang's study showed higher phone information between 3-8\\nBarks. The highest phone information was observed at 4 Barks. Higher speaker\\nand channel information was observed around 1-2 Barks. In temporal domain, their\\nstudy showed that phone information spreads for approximately 200 ms around the\\ncurrent time frame. Comparison of results from this analysis and our analysis shows\\nthat nature of phone information is similar in both studies. Nature of speaker and\\nchannel information in spectral domain is also similar. We could not compare the\\nspeaker and channel information in temporal domain because Yang's study did not\\npresent these results.\\nIn Section 4.1, we observed difference in the nature of speaker and channel variability, and speaker and channel information at Ii =5 Barks. Comparing MI levels\\nfrom our study to those from Yang's study, we observe that Yang's results show that\\nspeaker and channel information at 5 Barks is less that the corresponding phone\\ninformation. This is consistent with results of analysis of variability, but not with\\nlInformation-less MI calculated using Yang et. al. is 0.019 bits\\n\\n\\fthe proposed analysis of information. As mentioned before, this difference is due\\nto difference in the density estimation techniques used for computing MI. In the\\nfuture work, we plan to model the densities using more sophisticated techniques,\\nand improve the estimation of speaker and channel information.\\n\\n6\\n\\nConclusions\\n\\nWe proposed analysis of information in speech using three sources of information\\n- language (phone), speaker and channel. Information in speech was measured as\\nMI between the class labels and the set of features extracted from speech signal.\\nFor example, linguistic information was measured using phone labels and the features. We modeled distribution of features using Gaussian distribution. Thus we\\nrelated the analysis to previous proposed analysis of variability in speech. We observed similar results for phone variability and phone information. The speaker\\nand channel variability and speaker and channel information around the current\\nframe was different. This was shown to be related to the over-estimation of speaker\\nand channel information using unimodal Gaussian model. Note that the analysis of\\ninformation was proposed because its results have more meaningful interpretations\\nthan results of analysis of variability. For addressing the over-estimation, we plan\\nto use more complex models ,such as mixture of Gaussians, for computing MI in\\nthe future work.\\nAcknowledgments\\n\\nAuthors thank Prof. Andrew Fraser from Portland State University for numerous\\ndiscussions and helpful insights on this topic.\\n\\nReferences\\n[1] S. S. Kajarekar , N. Malayath and H. Hermansky, \\\"Analysis of sources of variability in speech,\\\" in Proc. of EUROSPEECH, Budapest, Hungary, 1999.\\n[2] S. S. Kajarekar, N. Malayath and H. Hermansky, \\\"Analysis of speaker and\\nchannel variability in speech,\\\" in Proc. of ASRU, Colorado, 1999.\\n[3] T. M. Cover and J. A. Thomas, Elements of Information theory, John Wiley &\\nSons, Inc., 1991.\\n[4] J. A. Bilmes, \\\"Maximum Mutual Information Based Reduction Strategies for\\nCross-correlation Based Joint Distribution Modelling ,\\\" in Proc. of ICASSP,\\nSeattle, USA, 1998.\\n[5] H. Hermansky H. Yang, S. van Vuuren, \\\"Relevancy of Time-Frequency Features\\nfor Phonetic Classification Measured by Mutual Information,\\\" in ICASSP '99,\\nPhoenix, Arizona, USA, 1999.\\n[6] H. H. Yang, S. Sharma, S. van Vuuren and H. Hermansky, \\\"Relevance of TimeFrequency Features for Phonetic and Speaker-Channel Classification,\\\" Speech\\nCommunication, Aug. 2000.\\n[7] R. V. Hogg and E. A. Tannis, Statistical Analysis and Inference, PRANTICE\\nHALL, fifth edition, 1997.\\n\\n\\f\",\n          \"Semiparametric Principal Component Analysis\\n\\nHan Liu\\nDepartment of Operations Research\\nand Financial Engineering\\nPrinceton University, NJ 08544\\nhanliu@princeton.edu\\n\\nFang Han\\nDepartment of Biostatistics\\nJohns Hopkins University\\nBaltimore, MD 21210\\nfhan@jhsph.edu\\n\\nAbstract\\nWe propose two new principal component analysis methods in this paper utilizing\\na semiparametric model. The according methods are named Copula Component\\nAnalysis (COCA) and Copula PCA. The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. The COCA and Copula PCA accordingly estimate the leading\\neigenvectors of the correlation and covariance matrices of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefficient estimator,\\nSpearman?s rho, is exploited in estimation. We prove that, under suitable conditions, although the marginal distributions can be arbitrarily continuous, the COCA\\nand Copula PCA estimators obtain fast estimation rates and are feature selection\\nconsistent in the setting where the dimension is nearly exponentially large relative\\nto the sample size. Careful numerical experiments on the synthetic and real data\\nare conducted to back up the theoretical results. We also discuss the relationship\\nwith the transelliptical component analysis proposed by Han and Liu (2012).\\n\\n1\\n\\nIntroduction\\n\\nThe Principal Component Analysis (PCA) is introduced as follows. Given a random vector X ? Rd\\nwith covariance matrix ? and n independent observations of X, the PCA reduces the dimension of\\nthe data by projecting the data onto a linear subspace spanned by the k leading eigenvectors of ?,\\nsuch that the principal modes of variations are preserved. In practice, ? is unknown and replaced by\\nPd\\nthe sample covariance S. By spectral decomposition, ? = j=1 ?j uj uTj with eigenvalues ?1 ?\\n. . . ? ?d and the corresponding orthornormal eigenvectors u1 , . . . , ud . PCA aims at recovering the\\nfirst k eigenvectors u1 , . . . , uk .\\nAlthough the PCA method as a procedure is model free, its theoretical and empirical performances\\nrely on the distributions. With regard to the empirical concern, the PCA?s geometric intuition is\\ncoming from the major axes of the contours of constant probability of the Gaussian [10]. [5] show\\nthat if X is multivariate Gaussian, then the distribution is centered about the principal component\\naxes and is therefore ?self-consistent? [8]. We refer to [10] for more good properties that the PCA\\nenjoys under the Gaussian model, which we wish to preserve while designing its generalization.\\nWith regard to the theoretical concern, firstly, the PCA generally fails to be consistent in high dimensional setting. Given u\\nb1 the dominant eigenvector of S, [9] show that the angle between u\\nb1\\nand u1 will not converge to 0, i.e. lim inf n?? E?(b\\nu1 , u1 ) > 0, where we denote by ?(b\\nu1 , u 1 )\\nthe angle between the estimated and the true leading eigenvectors. This key observation motivates\\nregularizing ?, resulting in a series of methods with different formulations and algorithms. The statistical model is generally further specified such that u1 is sparse, namely supp(u1 ) := {j : u1j 6=\\n0} and card(supp(u1 )) = s < n. The resulting estimator u\\ne1 is:\\nu\\ne1 = arg max v T Sv subject to kvk2 = 1, card(supp(v)) ? s.\\n\\n(1.1)\\n\\nv?Rd\\n\\nTo solve Equation (1.1), a variety of algorithms are proposed: greedy algorithms [3], lasso-type\\nmethods including SCoTLASS [11], SPCA [25] and sPCA-rSVD [19], a number of power methods\\n[12, 23, 16], the biconvex algorithm PMD [21] and the semidefinite relaxation DSPCA [4]. Secondly, it is realized that the distribution where the data are drawn from needs to be specified, such\\n1\\n\\n\\fthat the estimator u\\ne1 converges to u\\n?1 in a fast rate. [9, 1, 16, 18, 20] all establish their results under\\na strong Gaussian or sub-Gaussian assumption in order to obtain a fast rate under certain conditions.\\nIn this paper, we first explore the use of the PCA conducted on the correlation matrix ?0 instead of\\nthe covariance matrix ?, and then propose a high dimensional semiparametric scale-invariant principal component analysis method, named the Copula Component Analysis (COCA). In this paper,\\nthe population version of the scale-invariant PCA is built as the estimator of the leading eigenvector\\nof the population correlation matrix ?0 . Secondly, to handle the non-Gaussian data, we generalize the distribution family from the Gaussian to the larger Nonparanormal family [15]. A random\\nvariable X = (X1 , . . . , Xd )T belongs to a Nonparanormal family if and only if there exists a set\\nof univariate monotone functions {fj0 }dj=1 such that (f10 (X1 ), . . . , fd0 (Xd ))T is multivariate Gaussian. The Nonparanormal can have arbitrary continuous marginal distributions and can be far away\\nfrom the sub-Gaussian family. Thirdly, to estimate ?0 robustly and efficiently, instead of estimating\\nthe normal score transformation functions {fbj0 }dj=1 as [15] did, realizing that {fj0 }dj=1 preserve the\\nranks of the data, we utilize the nonparametric correlation coefficient estimator, Spearman?s rho, to\\nestimate ?0 . [14, 22] prove that the corresponding estimators converge to ?0 in a parametric rate.\\nIn theory, we analyze the general case that X is following the Nonparanormal and ?1 is weakly\\nsparse, here ?1 is the leading eigenvector of ?0 . We obtain the estimation consistency of the COCA\\nestimator to ?1 using the Spearman?s rho correlation coefficient matrix. We prove that the estimation\\nconsistency rates are close to the parametric rate under Gaussian assumption and the feature selection consistency can be achieved when d is nearly exponential to the sample size. In this paper, we\\nalso propose a scale variant PCA procedure, named the Copula PCA. The Copula PCA estimates\\nthe leading eigenvector of the latent covariance matrix ?. To estimate the leading eigenvectors of\\n?, instead of ?0 , in a fast rate, we prove that extra conditions are required on the transformation\\nfunctions.\\n\\n2\\n\\nBackground\\n\\nWe start with notations: Let M = [Mjk ] ? Rd?d and v = (v1 , ..., vd )T ? Rd . Let v?s subvector\\nwith entries indexed by I be denoted by vI , M ?s submatrix with rows indexed by I and columns\\nindexed by J be denoted by MIJ . Let MI? and M?J be the submatrix of M with rows in I and\\nall columns, and the submatrix of M with columns in J and all rows. For 0 < q ? ?, we\\nPd\\ndefine the `q and `? vector norm as kvkq := ( i=1 |vi |q )1/q and kvk? := max1?i?d |vi |, and\\nkvk0 := card(supp(v))?kvk2 . We define the matrix `max norm as the elementwise\\nmaximum value:\\nPn\\nkM kmax := max{|Mij |} and the `? norm as kM k? := max1?i?m j=1 |Mij |. Let ?j (M ) be\\nthe toppest j?th eigenvalue of M. In special, ?min (M ) := ?d (M ) and ?max (M ) := ?1 (M ) are\\nthe smallest and largest eigenvalues of M . The vectorized matrix of M , denoted by vec(M ), is\\nT\\nT T\\ndefined as: vec(M ) := (M?1\\n, . . . , M?d\\n) . Let Sd?1 := {v ? Rd : kvk2 = 1} be the d-dimensional\\n`2 sphere. For any two vectors a, b ? Rd and any two squared matrices A, B ? Rd?d , denote the\\ninner product of a and b, A and B by ha, bi := aT b and hA, Bi := Tr(AT B).\\n2.1\\n\\nThe Models of the PCA and Scale-invariant PCA\\n\\nPd\\nLet ?0 be the correlation matrix of ?, and by spectral decomposition, ? = j=1 ?j uj uTj and ?0 =\\nPd\\nT\\nj=1 ?j ?j ?j . Here ?1 ? ?2 ? . . . ? ?d > 0 and ?1 ? ?2 ? . . . ? ?d > 0 are the eigenvalues\\n0\\nof ? and ? , with u1 , . . . , ud and ?1 , . . . , ?d the corresponding orthonormal eigenvectors. The next\\nproposition claims that the estimators {b\\nu1 , . . . , u\\nbd } and {?b1 , . . . , ?bd }, the eigenvectors of the sample\\ncovariance and correlation matrices S and S 0 , are the MLEs of {u1 , . . . , ud } and {?1 , . . . , ?d }:\\nProposition 2.1. Let x1 . . . xn ? N (?, ?) and ?0 be the correlation matrix of ?. Then the estimators of PCA, {b\\nu1 , . . . , u\\nbd }, and the estimators of the scale-invariant PCA, {?b1 , . . . , ?bd }, are the\\nMLEs of {u1 , . . . , ud } and {?1 , . . . , ?d }.\\nProof. Use Theorem 11.3.1 in [2] and the functional invariance property of the MLE.\\nProposition 2.2. For any 1 ? i ? d, we have supp(ui ) = supp(?i ) and sign(uij ) =\\nsign(?ij ), ? 1 ? j ? d.\\nProof. For 1 ? i ? d, ui = (?i1 /?1 , ?i2 /?2 , . . . , ?id /?d ), where (?12 , . . . , ?d2 )T := diag(?).\\nIt is easy to observe that the scale-invariant PCA is a safe procedure for dimension reduction when\\nvariables are measured in different scales. Although there seems no theoretical advantage of scaleinvariant PCA over the PCA under the Gaussian model, in this paper we will show that under a more\\ngeneral Nonparanormal (or Gaussian Copula) model, the scale-invariant PCA will pose much less\\nconditions to make the estimator achieve good theoretical performance.\\n2\\n\\n\\f2.2 The Nonparanormal\\nWe first introduce two definitions of the Nonparanormal separately defined in [15] and [14].\\nDefinition 2.1 [15]. A random variable X = (X1 , ..., Xd )T with population marginal means and\\nstandard deviations ? = (?1 , . . . , ?d )T and ? = (?1 . . . . , ?d )T is said to follow a Nonparanormal\\ndistribution N P Nd (?, ?, f ) if and only if there exists a set of univariate monotone transformations\\nf = {fj }dj=1 such that: f (X) = (f1 (X1 ), ..., fd (Xd ))T ? N (?, ?), and ?j2 = ?jj , j = 1, . . . , d.\\nDefinition 2.2 [14]. Let f 0 = {fj0 }dj=1 be a set of monotone univariate functions and ?0 ? Rd?d\\nbe a positive definite correlation matrix with diag(?0 ) = 1. We say that a d dimensional random\\nvariable X = (X1 , . . . , Xd )T follows a Nonparanormal distribution, i.e. X ? N P Nd (?0 , f 0 ), if\\nf 0 (X) := (f10 (X1 ), . . . , fd0 (Xd ))T ? N (0, ?0 ).\\nThe following lemma proves that two definitions of the Nonparanormal are equivalent.\\nLemma 2.1. A random variable X ? N P Nd (?0 , f 0 ) if and only if there exist ? = (?1 , . . . , ?d )T ,\\n? = [?jk ] ? Rd?d such that for any 1 ? j, k ? d, E(Xj ) = ?j , Var(Xj ) = ?jj and ?0jk =\\n? ?jk , and a set of monotone univariate functions f = {fj }dj=1 such that X ? N P Nd (?, ?, f ).\\n?jj ??kk\\n\\nProof. Using the connection that fj (x) = ?j + ?j fj0 (x), for j ? {1, 2 . . . , d}.\\nLemma 2.1 guarantees that the Nonparanormal is defined properly. Definition 2.2 is more appealing\\nbecause it emphasizes the correlation and hence matches the spirit of the Copula. However, Definition 2.1 enjoys notational simplicity in analyzing the Copula-based LDA and PCA approaches.\\n2.3 Spearman?s rho Correlation and Covariance Matrices\\nGiven n data points x1 , . .q\\n. , xn ? Rd , where xi = (xi1 , . . . , xid )T , we denote by ?\\nbj :=\\nPn\\nPn\\n1\\n1\\nand ?\\nbj =\\nbj )2 , the marginal sample means and standard devii=1 xij\\ni=1 (xij ? ?\\nn\\nn\\nations. Because the Nonparanormal distribution preserves the rank of the data, it is natural to use the\\nnonparametric rank-based correlation coefficient estimator, Spearman?s rho, to estimate\\nthe latent\\nPn\\n1\\nr\\n= n+1\\ncorrelation. In detail, let rij be the rank of xij among\\nx\\n,\\n.\\n.\\n.\\n,\\nx\\nand\\nr\\n?\\n:=\\nij\\n1j\\nnj\\nj\\ni=1\\nn\\n2 ,\\nPn\\n(rij ??\\nrj )(rik ??\\nrk )\\ni=1\\nwe consider the following statistics: ?bjk = ?Pn\\n, and the correlation maPn\\n2\\n2\\nrj )\\ni=1 (rij ??\\n\\n?\\n\\nrk )\\ni=1 (rik ??\\n\\nbjk = 2 sin( ? ?bjk ). The Lemma 2.2, coming from [14], claims that the estimation\\ntrix estimator: R\\n6\\ncan reach the parametric rate.\\n21\\nLemma 2.2 ([14]). When x1 , . . . , xn ?i.i.d N P Nd (?0 , f 0 ), for any n ? log\\nd + 2,\\n!\\nr\\nb ? ?0 kmax ? 8? log d ? 1 ? 2/d2 .\\nP kR\\n(2.1)\\nn\\nb\\nb\\nWe denote by R := [Rjk ] the Spearman?s rho correlation coefficient matrix. In the following let\\nbjk ] be the Spearman?s rho covariance matrix.\\nSb := [Sbjk ] = [b\\n?j ?\\nbk R\\n\\n3\\n\\nMethods\\n\\nIn Figure 1, we randomly generate 10,000 samples from three different \\u0010types of Nonparanormal\\n\\u0011\\n1 0.5 and transdistributions. We suppose that X ? N P N2 (?0 , f 0 ). Here we set ?0 = 0.5\\n1\\nformation functions as follows: (A) f10 (x) = x3 and f20 (x) = x1/3 ; (B) f10 (x) = sign(x)x2 and\\nf20 (x) = x3 ; (C) f10 (x) = f20 (x) = ??1 (x). It can be observed that there does not exist a nice\\ngeometric explanation now. For example, researchers might wish to conduct PCA separately on\\ndifferent clusters in (A) and (B). For (C), the data look very noisy and a nice major axis might be\\nconsidered not existing.\\nHowever, under the Nonparanormal model and realizing that there is a latent Gaussian distribution\\nbehind, the geometric intuition of the PCA naturally comes back. In the next section, we will present\\nthe model of the COCA and Copula PCA motivated from this observation.\\n3.1 COCA Model\\nWe firstly present the model of the Copula Component Analysis (COCA) method, where the idea\\nof scale-invariant PCA is exploited and we wish to estimate the leading eigenvector of the latent\\ncorrelation matrix. In particular, the following model M0 (q, Rq , ?0 , f 0 ) is considered:\\n(\\nx1 , . . . , xn ?i.i.d N P Nd (?0 , f 0 ),\\nM0 (q, Rq , ?0 , f 0 ) :\\n(3.1)\\n?1 ? Sd?1 ? Bq (Rq ),\\n3\\n\\n\\fB\\n\\nC\\n\\n?1.5\\n\\n?1.0\\n\\n?0.5\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n0.8\\n0.4\\n0.2\\n0.0\\n\\n?40\\n\\n?20\\n\\n?1.5 ?1.0 ?0.5\\n\\n0\\n\\n0.0\\n\\n0.6\\n\\n0.5\\n\\n20\\n\\n1.0\\n\\n40\\n\\n1.5\\n\\n1.0\\n\\nA\\n\\n?2\\n\\n?1\\n\\n0\\n\\n1\\n\\n2\\n\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nFigure 1: Scatter plots of three Nonparanormals, X ? N P N2 (?0 , f 0 ). Here ?012 = 0.5 and\\nthe transformation functions have the form as follows: (A) f10 (x) = x3 and f20 (x) = x1/3 ; (B)\\nf10 (x) = sign(x)x2 and f20 (x) = x3 ; (C) f10 (x) = f20 (x) = ??1 (x).\\nwhere ?1 is the leading eigenvectors of the latent correlation matrix ?0 we are interested in estimating, 0 ? q ? 1 and the `q ball Bq (Rq ) is defined as:\\nwhen q = 0,\\n\\nB0 (R0 ) := {v ? Rd : card(supp(v)) ? R0 };\\n\\nwhen 0 < q ? 1,\\n\\nd\\n\\nBq (Rq ) := {v ? R :\\n\\nkvkqq\\n\\n? Rq }.\\n\\n(3.2)\\n(3.3)\\n\\nInspired by the model M0 (q, Rq , ?0 , f 0 ), we consider the following COCA estimator ?e1 , which\\nmaximizes the following equation with the constraint that ?e1 ? Bq (Rq ) for some 0 ? q ? 1:\\nb subject to v ? Sd?1 ? Bq (Rq ).\\n?e1 = arg max v T Rv,\\n\\n(3.4)\\n\\nv?Rd\\n\\nb is the estimated Spearman?s rho correlation coefficient matrix. The corresponding COCA\\nHere R\\nestimator ?e1 can be considered as a nonlinear dimensional reduction procedure and has the potential\\nto gain more flexibility compared with the classical PCA. In Section 4 we will establish the theoretical results on the COCA estimator and will show that it can estimate the latent true dominant\\neigenvector ?1 in a fast rate and can achieve feature selection consistency.\\n3.1.1 Copula PCA Model\\nIn contrast, we provide another model inspired from the classical PCA method, where we wish to\\nestimate the leading eigenvector of the latent covariance matrix. In particular, the following model\\nM(q, Rq , ?, f ) is considered:\\n(\\nx1 , . . . , xn ?i.i.d N P Nd (0, ?, f ),\\nM(q, Rq , ?, f ) :\\n(3.5)\\nu1 ? Sd?1 ? Bq (Rq ),\\nwhere u1 is the leading eigenvector of the covariance matrix ? and it is what we are interested in\\nestimating. The corresponding Copula PCA estimator is:\\nb subject to v ? Sd?1 ? Bq (Rq ),\\nu\\ne1 = arg max v T Sv,\\n\\n(3.6)\\n\\nv?Rd\\n\\nwhere Sb is the Spearman?s rho covariance coefficient matrix. This procedure is named the Copula\\nPCA. In Section 4, we will show that the Copula PCA requires a much stronger condition than\\nCOCA to make u\\ne1 converge to u1 in a fast rate.\\n3.2 Algorithms\\nIn this section we provide three sparse PCA algorithms, where the Spearman?s rho correlation and\\nb and Sb can be directly plugged in to obtain sparse estimators.\\ncovariance matrices R\\nPenalized Matrix Decomposition (PMD) is proposed by [21]. The main idea of the PMD is a bib\\nconvex optimization algorithm to the following problem: arg maxu,v uT ?v,\\nsubject to kuk22 ?\\n2\\n1, kvk2 ? 1, kuk1 ? ?, kvk1 ? ?. The COCA with PMD and Copula PCA with PMD are listed in\\nb Initialize v ? Sd?1 ; (2) Iterate until convergence:\\nthe following: (1) Input: A symmetric matrix ?.\\nTb\\nb subject\\n(a) u ? arg maxu?Rd u ?v subject to kuk1 ? ? and kuk22 ? 1.(b) v ? arg maxv?Rd uT ?v\\n2\\nb is either R\\nb or S,\\nb corresponding to the COCA with\\nto kvk1 ? ? and kvk2 ? 1; (3) Output: v. Here ?\\nPMD and Copula PCA with PMD. ? is the tuning parameter. [21] suggest using the first leading\\n4\\n\\n\\fb to be the initial value of v. The PMD can be considered as a solver to Equation\\neigenvector of ?\\n(3.4) and Equation (3.6) with q = 1.\\nThe SPCA algorithm is proposed by [25]. The main idea of the SPCA algorithm is to exploit a\\nregression approach to PCA and then utilize lasso and elastic net [24] to calculate a sparse estimator\\nto the leading eigenvector. The COCA with SPCA and Copula PCA with SPCA are listed as follows:\\nb Initialize u ? Sd?1 . (2). Iterate until convergence: (a) v ?\\n(1) Input: A symmetric matrix ?.\\nTb\\nb\\nb 2 . (3) Output: v/kvk2 . Here\\n?vk\\narg minv?Rd (u ? v) ?(u ? v) + ?1 kvk22 + ?2 kvk1 ; (b) u ? ?v/k\\nb is either R\\nb or S,\\nb corresponding to the COCA with SPCA and Copula PCA with SPCA. ?1 ? R\\n?\\nb to be the\\nand ?2 ? R are two tuning parameters. [25] suggest using the first leading eigenvector of ?\\ninitial value of v. The SPCA can be considered as a solver to Equations (3.4) and (3.6) with q = 1.\\nThe Truncated Power method (TPower) is proposed by [23]. The main idea is to utilize the power\\nmethod, but truncate the vector to a `0 ball in each iteration. Actually, TPower can be generalized\\nto a family of algorithms to solve Equation (3.4) when 0 ? q ? 1. We name it the `q Constraint\\nTruncated Power Method (qTPM). Especially, when q = 0, the algorithm qTPM coincides with\\n[23]?s method. The TPower can be considered as a general solver to Equation (3.4) and Equation\\n(3.6) with q ? [0, 1]. In detail, we utilize the classical power method, but in each iteration t we\\nproject the intermediate vector xt to the intersection of the d-dimension sphere Sd?1 and the `q ball\\n1/q\\nwith the radius Rq . Detailed algorithms are presented in the long version of this paper [6].\\n\\n4\\n\\nTheoretical Properties\\n\\nIn this section we provide the theoretical properties of the COCA and Copula PCA methods. Especially, we are interested in the high dimensional case when d > n.\\n4.1 Rank-based Correlation and Covariance Matrices Estimation\\nb to ?0\\nThis section is devoted to the statement of our result on quantifying the convergence rate of R\\nb\\nand S to ?. In particular, we establish the results on the `max convergence rates of the Spearman?s\\nrho correlation and covariance matrices to ? and ?0 . For COCA, Lemma 2.2 is enough. For Copula\\nPCA, however, we still need to quantify the convergence rate of Sb to ?.\\nDefinition 4.1 Subgaussian Transformation Function Class. Let Z ? R be a random variable\\nfollowing the standard Gaussian distribution. The Subgaussian Transformation Function Class\\nTF(K) is defined as the set of functions {g0 : R ? R} which satisfies that: E|g0 (Z)|m ?\\nm! m\\n+\\n2 K , ?m?Z .\\nHere it is easy to see that for any function g0 : R ? R, if there exists a constant L < ? such that\\ng0 (z) ? L or g00 (z) ? L or g000 (z) ? L, ? z ? R, then g0 ? TF(K) for some constant K.\\nThen we have the following result, which states that ? can also be recovered in the parametric rate.\\nLemma 4.1. When x1 , . . . , xn ?i.i.d N P Nd (?, ?, f ), 0 < 1/c0 < min{?j } < max{?j } < c0 <\\nj\\n\\nj\\n\\n?, for some constant c0 and g := {gj = fj?1 }dj=1 satisfies for all j = 1, . . . , K, gj2 ? T F (K)\\n21\\nwhere K < ? is some constant, we have for any 1 ? j, k ? d, for any n ? log\\nd + 2,\\nP(|Sbjk ? ?jk | > t) ? 2 exp(?c1 nt2 ),\\n(4.1)\\nwhere c1 is a constant only depending on the choice of K.\\nRemark 4.1. The Lemma 4.1 claims that, under certain constraint on the transformation functions,\\nthe latent covariance matrix ? can be recovered using the Spearman?s rho covariance matrix. However, in this case, the marginal distributions of the Nonparanormal are required to be sub-gaussian\\nand cannot be arbitrarily continuous. This makes the Copula PCA a less favored method.\\n4.2 COCA and Copula PCA\\nThis section is devoted to the statement of our main result on the upper bound of the estimated error\\nof the COCA estimator and Copula PCA estimator.\\nTheorem 4.1 (Upper bound for the COCA). Let ?e1 be the global solution to Equation (3.4) and\\nthe Model M0 (q,p\\nRq , ?0 , f 0 ) holds. For any two vectors v1 ? Sd?1 and v2 ? Sd?1 , let\\n21\\n| sin ?(v1 , v2 )| = 1 ? (v1T v2 )2 , then we have, for any n ? log\\nd + 2,\\n2?q !\\n\\u0012\\n\\u0013\\n64? 2\\nlog d 2\\n2\\n2\\ne\\nP sin ?(?1 , ?1 ) ? ?q Rq\\n?\\n? 1 ? 1/d2 ,\\n(4.2)\\n(?1 ? ?2 )2\\nn\\n\\n5\\n\\n\\fwhere ?q = 2 ? I(q = 1) + 4 ? I(q = 0) + (1 +\\n\\n?\\n\\n3)2 ? I(0 < q < 1).\\n\\nb to ?0 . Detailed\\nProof. The key idea of the proof is to utilize the `max norm convergence result of R\\nproofs are presented in the long version of this paper [6].\\n\\u0011\\n\\u0010\\nGenerally, when Rq and ?1 , ?2 do not scale with (n, d), the rate is OP ( logn d )1?q/2 , which is the\\nparametric rate [16, 20, 18] obtain. When (n, d) goes to infinity, the two dominant eigenvalues ?1\\nand ?2 will typically go to infinity and will at least be away from zero. Hence, our rate shown in\\n\\u0010\\n\\u0011 2?q\\n2\\n64? 2 ?2\\nEquation (4.2) is better than the seemingly more state-of-art rate: ?q Rq2 (?1 ??21)2 ? logn d\\n.\\nThe COCA is significantly different from [20] and [18]?s results in the sense that: (1) In theory,\\nthe Nonparanormal family can have arbitrary continuous marginal distributions, where a fast rate\\ncannot be obtained using the techniques built for either Gaussian or sub-Gaussian distributions;\\nb to estimate ?0 ,\\n(2) In methodology, we utilize the Spearman?s rho correlation coefficient matrix R\\ninstead of using the sample correlation matrix S 0 . This procedure has been shown to lose little in\\nrate and will be much more robust under the Nonparanormal model. Given Theorem 4.1, we can\\nimmediately obtain a feature selection consistency result.\\nCorollary 4.1 (Feature Selection Consistency of the COCA). Let ?e1 be the global solution to\\nb 0 :=\\nEquation (3.4) and the Model M0 (0, R0 , ?0 , f 0 ) holds.q Let ?0 := supp(?1 ) and ?\\n?\\nlog d\\n2R0 ?\\nsupp(?e1 ). If we further have minj??0 |?1j | ? 16?1 ??\\nn , then for any n ? 21/ log d + 2,\\n2\\n0\\n0\\n2\\nb\\nP(? = ? ) ? 1 ? 1/d .\\nSimilarly, we can give an upper bound for the estimation rate of the Copula PCA to the true leading\\neigenvalue u1 of the latent covariance matrix ?. The next theorem provides the detail result.\\nTheorem 4.2 (Upper bound for Copula PCA). Let u\\ne1 be the global solution to Equation (3.6) and\\nthe Model M(q, Rq , ?, f ) holds. If g := {gj = fj?1 }dj=1 satisfies gj2 ? T F (K) for all 1 ? j ? d,\\nand 0 < 1/c0 < min{?j } < max{?j } < c0 < ?, then we have, for any n ? 21/ log d + 2,\\nj\\n\\nj\\n\\n\\u0013 2?q !\\n4\\nlog d 2\\nP sin ?(e\\nu1 , u 1 ) ?\\n?\\n? 1 ? 1/d2 ,\\nc1 (?1 ? ?2 )2\\nn\\n?\\nwhere ?q = 2 ? I(q = 1) + 4 ? I(q = 0) + (1 + 3)2 ? I(0 < q < 1) and c1 is a constant defined in\\nEquation (4.1), only depending on K.\\nCorollary 4.2 (Feature Selection Consistency of the Copula PCA). Let u\\ne1 be the global solution\\nb := supp(e\\nto Equation (3.6) and the Model M(0, R0 , ?, f ) holds. Let ? := supp(u1 ) and ?\\nu1 ).\\n?1 d\\n2\\nIf g := {gj = fj }j=1 satisfies gj ? T F (K) for all 1 ? j ? d, and 0 < 1/c0 < minj {?j } <\\nq\\n?\\nlog d\\n0\\nmaxj {?j } < c0 < ?, and we further have minj?? |u1j | ? ?c41 (?2R\\nn , then for any\\n1 ??2 )\\nb = ?) ? 1 ? 12 .\\nn ? 21 + 2, P(?\\n2\\n\\nlog d\\n\\n5\\n\\n?q Rq2\\n\\n\\u0012\\n\\nd\\n\\nExperiments\\n\\nIn this section we investigate the empirical usefulness of the COCA method. Three sparse PCA\\nalgorithms are considered: PMD proposed by [21], SPCA proposed by [25] and Truncated Power\\nmethod (TPower) proposed by [23]. The following three methods are considered: (1) Pearson:\\nthe classic high dimensional PCA using the Pearson sample correlation matrix; (2) Spearman:\\nthe COCA using the Spearman?s rho correlation coefficient matrix; (3) Oracle: the classic high\\ndimensional PCA using the Pearson sample correlation matrix of the data from the latent Gaussian\\n(perfect without contaminations).\\n5.1 Numerical Simulations\\nIn the simulation study we randomly sample n data points x1 , . . . , xn from the Nonparanormal\\ndistribution X ? N P Nd (?0 , f 0 ). Here we consider the setup of d = 100. We follow the\\nsame generating scheme as in [19, 23] and [7]. A covariance matrix ? is firstly synthesized\\nthrough the eigenvalue decomposition, where the first two eigenvalues are given and the corresponding eigenvectors are pre-specified to be sparse. In detail, we suppose that the first two dominant eigenvectors of ?, u1 and u2 , are sparse in the sense that only the\\n? first s = 10 entries of\\nu1 and the second s = 10 entries of u2 are nonzero and set to be 1/ 10. ?1 = 5, ?2 = 2,\\n6\\n\\n\\f?3 = . . . = ?d = 1. The remaining eigenvectors are chosen arbitrarily. The correlation matrix\\n?0 is accordingly generated from ?, with ?1 = 4, ?2 = 2.5, ?3 , . . . , ?d ? 1 and the two dominant eigenvectors sparse. To sample data from the Nonparanormal, we also need the transformation\\nfunctions: f 0 = {fj0 }dj=1 . Here two types of transformation functions are considered: (1) Linear\\n0\\ntransformation (or no transformation): flinear\\n= {h0 , h0 , . . . , h0 }, where h0 (x) := x; (2)\\nNonlinear transformation: there exist five univariate monotone functions h1 , h2 , . . . , h5 : R ? R\\n0\\nand fnonlinear\\n= {h1 , h2 , h3 , h4 , h5 , h1 , h2 , h3 , h4 , h5 , . . .}, where Rh?1\\nh?1\\n1 (x) := x,\\n2 (x) :=\\n1/2\\n3\\n?(x)? ?(t)?(t)dt\\nsign(x)|x|\\n?1\\n?1\\n?1\\nx\\n?R\\n,\\nh3 (x) := ?R 6\\n, h4 (x) := ?R\\n, h5 (x) :=\\nR\\n2\\n?R\\n\\n|t|?(t)dt\\nR\\nexp(x)? exp(t)?(t)dt\\n.\\nR\\n(exp(y)? exp(t)?(t)dt)2 ?(y)dy\\n\\nt ?(t)dt\\n\\n(?(y)? ?(t)?(t)dt) ?(y)dy\\n\\nHere ? and ? are defined to be the probability density and cu-\\n\\nmulative distribution functions of the standard Gaussian. h1 , . . . , h5 are defined such that for any\\n?1\\nZ ? N (0, 1), E(h?1\\nj (Z)) = 0 and Var(hj (Z)) = 1 ? j ? {1, . . . , 5}. We then generate\\nn = 100, 200 or 500 data points from:\\n0\\n0\\n[Scheme 1] X ? N P Nd (?0 , flinear\\n) where flinear\\n= {h0 , h0 , . . . , h0 } and ?0 is defined as above.\\n0\\n0\\n[Scheme 2] X ? N P Nd (?0 , fnonlinear\\n) where fnonlinear\\n= {h1 , h2 , h3 , h4 , h5 , . . .}.\\n\\nTo evaluate the robustness of different methods, we adopt a similar data contamination procedure as\\nin [14]. Let r ? [0, 1) represents the proportion of samples being contaminated. For each dimension,\\nwe randomly select bnrc entries and replace them with either 5 or -5 with equal probability. The\\nfinal data matrix we obtained is X ? Rn?d . The PMD, SPCA and TPower algorithms are then\\nemployed on X to computer the estimated leading eigenvector ?e1 .\\nUnder the Scheme 1 and Scheme 2 with different levels of contamination (r = 0 or 0.05), we\\nrepeatedly generate the data matrix X for 1,000 times and compute the averaged False Positive Rates\\nand False Negative Rates using a path of tuning parameters ?. The feature selection performances\\nof different methods are then evaluated. The corresponding ROC curves are presented in Figure 2.\\nMore quantitative results are provided in the long version of this paper [6]. It can be observed that\\nwhen r = 0 and X is exactly Gaussian, Pearson,Spearman and Oracle can all recover the sparsity\\npattern perfectly. However, when r > 0, the performances of Pearson significantly decrease, while\\nSpearman is still very close to the Oracle. In Scheme 2, even when r = 0, Pearson cannot recover\\nthe support set of ?1 , while Spearman can still recover the sparsity pattern almost perfectly. When\\nr > 0, the performance of Spearman is still very close to the Oracle.\\nr=0\\n\\n0.4\\n\\n0.6\\nFPR\\n\\n0.8\\n\\n1.0\\n\\n0.6\\nFPR\\n\\n0.8\\n\\n1.0\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\n1.0\\n0.6\\nTPR\\n0.4\\n0.4TPower0.6\\n\\n0.8\\n\\n0.0\\n\\nFPR\\n\\n0.4\\n\\n0.6\\nFPR\\n\\n0.8\\n\\n1.0\\n\\n0.2\\n\\n0.4TPower0.6\\n\\n0.8\\n\\n1.0\\n\\nFPR\\n\\n0.8\\nTPR\\n\\n0.6\\n\\n0.8\\nTPR\\n0.4\\n0.2\\n\\nPearson\\nSpearman\\nOracle\\n\\n0.0\\n\\n1.0\\n\\n0.2\\nPearson\\nSpearman\\nOracle\\n0.0\\n\\nFPR\\n\\n0.2\\n\\n0.6\\n\\n0.8\\n0.6\\n0.4\\n\\n0.2\\n0.0\\n\\nFPR\\n\\n0.4\\n0.2\\n\\nPearson\\nSpearman\\nOracle\\n\\n0.0\\n\\n1.0\\n\\n1.0\\n\\n0.8\\n\\n0.2\\nPearson\\nSpearman\\nOracle\\n0.0\\n\\n0.8\\n\\n1.0\\n0.6\\nTPR\\n0.4\\n0.4 SPCA 0.6\\n\\nTPR\\n\\n0.6\\nTPR\\n0.4\\n\\n0.2\\n\\n0.4\\n\\nFPR\\n\\n0.4\\n0.2\\n\\n0.2\\n0.0\\n\\n0.2\\nPearson\\nSpearman\\nOracle\\n0.0\\n\\nPearson\\nSpearman\\nOracle\\n\\n0.0\\n\\n1.0\\n\\n0.2\\n\\n0.8\\n\\n1.0\\n\\n0.4 SPCA 0.6\\n\\nTPower\\n\\n0.8\\n\\n1.0\\n0.6\\n0.4\\n0.2\\n0.2\\n\\n0.8\\n\\n1.0\\n0.8\\n\\n0.2\\n\\nPearson\\nSpearman\\nOracle\\n0.0\\n\\n0.6\\nTPR\\n0.4\\n0.2\\n\\n0.0\\n\\nPearson\\nSpearman\\nOracle\\n0.0\\n\\nTPR\\n\\n1.0\\n\\nFPR\\n\\n0.0\\n\\n0.2\\n\\n0.4\\n\\nTPR\\n\\n0.6\\n\\n0.8\\n\\n0.8\\n\\n1.0\\n\\n0.4 PMD 0.6\\n\\nr = 0.05\\n\\nTPower\\n\\n0.8\\n\\n1.0\\n0.6\\n0.2\\n\\n0.0\\n\\nPearson\\nSpearman\\nOracle\\n0.0\\n\\nFPR\\n\\n1.0\\n\\n0.2\\n\\n0.4\\n\\nTPR\\n\\n1.0\\n\\n0.0\\n\\n0.8\\n\\n1.0\\n\\n0.6\\n\\n0.0\\n\\n0.4PMD\\n\\nr=0\\n\\nSPCA\\n\\n0.8\\n\\n1.0\\n0.8\\n0.6\\nTPR\\n0.4\\n0.2\\n\\n0.0\\n\\n0.0\\n\\nPearson\\nSpearman\\nOracle\\n0.0\\n\\nr = 0.05\\n\\nSPCA\\n\\n0.2\\n\\n0.2\\n\\n0.4\\n\\nTPR\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nPMD\\n\\nPearson\\nSpearman\\nOracle\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\nFPR\\n\\n0.8\\n\\n1.0\\n\\nPearson\\nSpearman\\nOracle\\n\\n0.0\\n\\nr = 0.05\\n\\nPMD\\n\\n0.0\\n\\nr=0\\n\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nFPR\\n\\nFigure 2: ROC curves for the PMD, SPCA and Truncated Power method (the left two, the middle\\ntwo, the right two) with linear (no) and nonlinear transformation (top, bottom) and data contamination at different levels (r = 0, 0.05). Here n = 100 and d = 100.\\n5.2 Large-scale Genomic Data Analysis\\nIn this section we investigate the performance of Spearman compared with the Pearson using\\none of the largest microarray datasets [17]. In summary, we collect in all 13,182 publicly available\\nmicroarray samples from Affymetrixs HGU133a platform. The raw data contain 20,248 probes and\\n13,182 samples belonging to 2,711 tissue types (e.g., lung cancers, prostate cancer, brain tumor etc.).\\nThere are at most 1,599 samples and at least 1 sample belonging to each tissue type. We merge the\\nprobes corresponding to the same gene. There are remaining 12,713 genes and 13,182 samples. This\\ndataset is non-Gaussian (see the long version of this paper [6]). The main purpose of this experiment\\nis to compare the performance of the COCA with the classical high dimensional PCA. We utilize the\\nTruncated Power method proposed by [23] to achieve the sparse estimated dominant eigenvectors.\\n7\\n\\n\\fWe adopt the same idea of data-preprocessing as in [14]. In particular, we firstly remove the batch\\neffect by applying the surrogate variable analysis proposed by [13]. We then extract the top 2,000\\ngenes with the highest marginal standard deviations. There are, accordingly, 2,000 genes left and the\\ndata matrix we are focusing is 2, 000 ? 13, 182. We then explore several tissue types with the largest\\nsample size: (1) Breast tumor, 1,599 samples; (2) B cell lymphoma, 213 samples; (3) Prostate tumor,\\n148 samples; (4) Wilms tumor, 143 samples.\\nbreast tumor\\n\\nprostate tumor\\n\\nwilms tumor\\n\\n?\\n\\n?\\n\\n? ?\\n??\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n0\\n\\n?\\n\\n?\\n? ?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n??\\n?\\n?\\n?\\n? ?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n??\\n?\\n?\\n?\\n?\\n?\\n? ??\\n?\\n? ?\\n? ??\\n? ?\\n?\\n? ? ?\\n?\\n??\\n?\\n??\\n?\\n? ?\\n??\\n??\\n?\\n?\\n?\\n? ??\\n?\\n?\\n?\\n?\\n?\\n???\\n?\\n?\\n?\\n???\\n?\\n?\\n?\\n??\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ??\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?? ?\\n??\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n??\\n?\\n?? ?\\n??\\n??\\n?\\n? ?\\n? ?\\n? ? ? ??\\n??\\n??\\n?\\n?\\n? ? ?\\n??\\n?\\n?\\n? ? ?\\n? ??\\n??\\n? ? ? ??\\n?\\n?\\n? ?\\n??\\n?\\n?\\n??\\n??\\n?\\n??\\n?\\n? ?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?? ? ?\\n?\\n?\\n?\\n? ??\\n??\\n?\\n?\\n??\\n? ?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n? ?\\n?\\n?\\n? ? ?? ?\\n???\\n?? ?\\n??\\n?\\n?\\n??\\n? ?\\n? ? ?? ?\\n??\\n?\\n?\\n?\\n? ??\\n?\\n? ???\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n??\\n???? ?? ?\\n?\\n? ??\\n???? ?? ??\\n? ?\\n?\\n?\\n? ?? ?\\n?\\n?\\n?\\n??\\n?\\n??\\n?\\n?\\n?\\n? ?\\n?\\n? ? ?\\n? ? ?\\n?\\n?\\n? ??\\n?\\n??\\n?\\n?\\n?\\n?\\n??\\n? ? ?? ? ? ?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n???\\n? ? ?\\n?\\n?\\n?\\n?\\n??\\n?\\n? ?\\n?\\n?\\n? ? ?\\n? ? ?\\n? ?\\n?\\n?\\n? ? ?? ?\\n?? ? ?\\n? ? ?\\n?\\n?? ? ?\\n??\\n?\\n?\\n?\\n?\\n?\\n?\\n??\\n?\\n?\\n?\\n? ?\\n??\\n??\\n?\\n??\\n?\\n???\\n? ?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n? ?\\n? ?\\n?\\n?\\n?\\n? ?\\n??\\n?\\n? ?\\n?\\n? ?\\n?\\n??\\n??\\n? ??? ? ?\\n? ?\\n? ? ?\\n?\\n?\\n? ? ??\\n? ?? ?\\n??\\n?? ?? ? ?\\n?\\n??? ? ? ?\\n?\\n?\\n?\\n?\\n??\\n??\\n? ?\\n? ??\\n??\\n?? ?\\n?\\n?\\n?? ?\\n? ?\\n?\\n? ?\\n? ? ? ?? ? ? ?\\n?\\n?? ? ?? ?\\n?\\n?\\n?\\n??\\n?\\n? ?\\n?\\n??\\n?\\n?? ? ?\\n? ??\\n? ? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ??\\n??\\n???\\n??\\n?\\n? ? ? ??\\n??\\n?\\n? ?\\n?\\n?\\n?\\n?\\n? ?? ?\\n??? ?\\n?\\n?\\n?\\n? ??\\n? ? ?? ?\\n??\\n??\\n?\\n?\\n? ? ??\\n?\\n?\\n? ?\\n? ? ???\\n?? ?\\n?? ? ? ?\\n? ?\\n?\\n?\\n?? ?\\n?\\n? ? ?\\n??\\n?\\n??\\n?? ? ? ? ? ??\\n? ? ??\\n???? ?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n? ? ??\\n?\\n? ??? ? ?\\n?\\n?\\n?\\n? ? ??\\n? ??\\n?\\n?? ?\\n?\\n?? ?\\n?\\n? ? ?? ? ?\\n?\\n? ? ? ?? ?\\n? ? ?? ?\\n?\\n?\\n?\\n??\\n? ?\\n?\\n? ? ?\\n?? ? ??\\n?? ? ?\\n?\\n?? ?\\n?\\n?\\n?\\n?? ?\\n?? ?\\n? ? ?\\n??\\n??\\n?\\n?? ?\\n??\\n?\\n? ?\\n?\\n?\\n?\\n? ? ?\\n?\\n? ??\\n?\\n?\\n? ?\\n??\\n?\\n???\\n? ?\\n? ?\\n? ???\\n?? ?\\n? ?? ?\\n?\\n?\\n?\\n?\\n?? ? ?\\n?\\n?\\n???\\n?\\n?\\n?\\n??\\n??\\n?\\n?\\n?\\n?\\n?\\n?\\n???\\n?\\n? ?\\n?\\n?\\n?\\n??\\n? ?\\n? ?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n??\\n??\\n?\\n??\\n? ? ?\\n?\\n?\\n?\\n?? ?? ? ?\\n?\\n? ??\\n?\\n? ?\\n?\\n??\\n?\\n?\\n?\\n? ? ?\\n?\\n? ?\\n?\\n?? ? ??\\n?? ?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n??\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ? ?\\n??\\n??\\n?\\n?\\n??\\n?\\n??\\n?\\n?\\n?\\n?\\n?\\n? ??\\n??\\n?\\n?? ? ?\\n?\\n??\\n?\\n?? ?\\n?\\n?\\n?? ?\\n?\\n?\\n? ??\\n?\\n?\\n? ? ?? ?\\n??\\n???\\n?? ?\\n? ?\\n?\\n?\\n??\\n? ?\\n?\\n? ?\\n??\\n??\\n? ?\\n?\\n??\\n? ?\\n?\\n? ??\\n?? ?\\n? ??\\n?\\n?\\n?\\n? ?\\n?\\n??\\n?\\n?\\n?\\n? ?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n? ??\\n?\\n??\\n? ??\\n?\\n? ??\\n? ?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ??\\n?\\n?\\n?\\n?\\n?\\n?\\n???\\n?? ??\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n? ?\\n?\\n?\\n?? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?? ?\\n?\\n?\\n?\\n?\\n??\\n? ?\\n?\\n? ? ??\\n?\\n?? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n??\\n?\\n?\\n?\\n?\\n??\\n?\\n?\\n? ?\\n?\\n???\\n?\\n?\\n?\\n?? ?\\n?\\n?\\n?\\n??\\n?\\n? ??\\n?\\n?\\n?\\n?\\n? ? ??\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n? ? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n??\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n??\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n??\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n? ?\\n\\n?\\n??\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n??\\n\\n? ?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n???\\n\\n?\\n? ??\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?10\\n\\n?5\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n?20\\n\\n?15\\n\\n?10\\n\\n?5\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n??\\n\\n?\\n\\n??\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n? ?\\n\\n?\\n\\n?\\n\\n?\\n\\n??\\n\\n??\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n??\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n? ??\\n\\n?\\n\\n?\\n?\\n\\n?\\n? ??\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n??\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n??\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n??\\n?\\n\\n?\\n?\\n\\n??\\n? ?\\n\\n?\\n\\n?\\n? ?\\n?\\n?\\n\\n?\\n?\\n?\\n??\\n\\n?\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n??\\n?\\n\\n?\\n\\n?\\n\\n?15\\n\\n?10\\n\\n?5\\n\\n0\\n\\n5\\n\\n10\\n\\n?10\\n\\n0\\n\\n10\\n\\n20\\n\\nPrincipal Component 1\\n\\nPrincipal Component 1\\n\\nPrincipal Component 1\\n\\nPrincipal Component 1\\n\\nb cell lymphoma\\n\\nbreast tumor\\n\\nprostate tumor\\n\\nwilms tumor\\n\\n2\\n\\n4\\n\\n?15\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?20\\n\\n?\\n?\\n??\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n??\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n? ?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n? ?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?10\\n\\n?\\n\\n?\\n?\\n??\\n\\n?\\n?\\n\\n?\\n??\\n\\n?? ?\\n?\\n\\n? ??\\n\\n?\\n?\\n\\n?\\n??\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n10\\n\\n5\\n\\n?\\n\\n?10\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n\\n5\\n\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n0\\n\\n?\\n\\n?\\n\\n?? ?\\n?\\n\\n??\\n?\\n?\\n\\n?\\n\\nPrincipal Component 2\\n\\n?\\n\\n??\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?5\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?10\\n\\n?\\n\\n? ?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n0\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n10\\n\\n10\\n\\n? ?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n??\\n\\n?\\n\\nPrincipal Component 2\\n\\n?\\n??\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?5\\n\\n?\\n? ?\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n? ?\\n??\\n?\\n\\n?\\n\\n? ?\\n\\n0\\n\\n??\\n?\\n\\n?\\n\\n?\\n?\\n\\n??\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?10\\n\\nPrincipal Component 2\\n\\n?\\n?\\n?\\n\\n???\\n? ?\\n?\\n\\n?\\n\\n?\\n?\\n? ? ?\\n?\\n?\\n?\\n?? ?\\n? ?\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n??\\n\\n?\\n??\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n? ??\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n? ?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n? ?\\n\\n?\\n?\\n?\\n\\n?\\n\\nPrincipal Component 2\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n??\\n?\\n??\\n?\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n??\\n? ?\\n\\n?\\n\\n15\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n\\n?\\n\\n10\\n\\n20\\n\\n20\\n\\n20\\n\\nb cell lymphoma\\n\\n?\\n\\n?2\\n\\n?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n??\\n\\n?\\n?\\n\\n? ?\\n\\n?\\n\\n?\\n?\\n? ??\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n??\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n? ??\\n\\n?\\n\\n?\\n\\n?\\n?\\n???\\n\\n?\\n?\\n?\\n\\n??\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n??\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n? ? ??\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?? ?\\n?\\n?\\n? ?? ??\\n?\\n?\\n?\\n??\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ? ? ?\\n?? ?\\n?\\n? ?\\n?\\n? ?\\n? ?\\n?\\n?\\n? ??\\n?\\n?\\n? ? ??\\n??\\n?\\n?\\n?\\n?\\n?\\n??\\n?? ?\\n? ?\\n?\\n?\\n? ? ? ????\\n?\\n?\\n?\\n?\\n?? ?\\n?\\n?\\n??\\n?\\n?\\n?\\n? ?\\n?\\n? ??\\n? ?\\n? ?\\n?\\n? ?\\n?\\n?\\n? ???\\n?\\n??\\n?? ? ? ? ?\\n? ?\\n?\\n?\\n? ??\\n?\\n?\\n? ??\\n? ? ??\\n?\\n? ?\\n? ?\\n?\\n??\\n?\\n?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?? ? ? ? ?\\n?\\n?\\n?\\n?\\n? ??\\n? ??\\n??\\n?\\n???\\n??\\n??? ?\\n??\\n??\\n?\\n?\\n?\\n?\\n? ??\\n? ?? ?\\n?\\n?\\n?\\n? ????\\n??\\n?\\n?\\n? ?? ? ? ? ?\\n? ?\\n? ?\\n??\\n?\\n?\\n?\\n?\\n?\\n????? ?\\n??\\n?? ??? ???\\n?\\n?\\n????? ??? ?\\n??\\n?\\n? ?\\n??\\n? ?? ?\\n?\\n??\\n??\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ?\\n??\\n??\\n?\\n?\\n?\\n??\\n? ? ?? ? ??\\n?\\n?\\n?\\n? ? ?\\n?\\n?\\n? ?\\n?\\n?\\n?\\n??\\n? ??\\n?\\n?\\n?? ? ? ?\\n??\\n?\\n?? ? ?? ? ?? ? ??\\n??\\n?? ?? ???\\n?\\n?\\n?\\n? ? ?? ?\\n?\\n?\\n? ? ? ??\\n??? ?\\n?\\n??\\n?\\n?? ?\\n? ??\\n? ??\\n?\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?? ?\\n?\\n? ?\\n?\\n?\\n?\\n??\\n? ?\\n?\\n??\\n? ? ? ?\\n??\\n?\\n? ? ?\\n??\\n? ?? ? ? ??? ????? ??\\n? ?? ?\\n??\\n??\\n?\\n?\\n? ?\\n??\\n?\\n?\\n????\\n?\\n??\\n?\\n? ?? ?? ? ? ? ??\\n?\\n??? ??\\n?????? ?? ?\\n? ?\\n? ?\\n? ? ?\\n?\\n??\\n? ??? ?\\n?? ?\\n?\\n?\\n? ??\\n?\\n? ??\\n?\\n?\\n? ? ?? ? ?\\n?\\n?\\n?\\n?\\n?\\n?? ? ?\\n?\\n??\\n?? ? ?? ? ? ??? ? ? ? ? ?? ?\\n?? ?\\n?\\n?\\n?\\n?? ??\\n? ?\\n?\\n?\\n?\\n?\\n? ?\\n?\\n??\\n??\\n?\\n?\\n?\\n??\\n?\\n? ?\\n?\\n?? ? ?\\n??? ???\\n? ?\\n?\\n??\\n?? ?\\n?\\n?? ? ?\\n?\\n?\\n?\\n?\\n?\\n??\\n? ??\\n? ?\\n? ? ???\\n? ? ?? ? ?\\n?\\n?\\n?\\n? ? ??\\n?\\n? ???\\n?\\n?\\n? ? ?\\n?? ?\\n?\\n? ??? ? ?\\n? ? ??\\n?\\n? ?\\n? ? ??\\n?\\n?\\n? ? ? ? ? ? ??\\n? ? ? ??\\n?\\n? ?\\n?\\n??\\n?\\n?\\n?\\n??\\n? ?? ? ? ? ?? ?\\n?\\n?? ? ?\\n?? ? ????\\n?\\n??\\n?\\n?\\n?\\n?\\n?\\n? ??? ? ?? ?? ??\\n?\\n? ?? ?\\n??\\n?? ?\\n? ?? ? ?\\n?? ?\\n? ??\\n?\\n? ? ? ?? ?\\n?? ?\\n??\\n?\\n? ????\\n?\\n? ?\\n?\\n??\\n? ? ?\\n?\\n?\\n? ?\\n? ??\\n?\\n??\\n?\\n????? ?? ? ?? ? ?\\n? ??\\n? ? ?\\n? ?\\n?\\n?\\n? ?\\n?\\n?\\n???\\n?\\n? ??\\n?\\n?\\n?\\n?\\n??\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?? ? ?? ?\\n?\\n? ? ?? ?\\n? ?\\n?\\n? ?\\n? ??\\n? ?\\n?\\n?\\n?? ?\\n?\\n?\\n? ? ?\\n? ?\\n??\\n? ??\\n?? ?? ?\\n?? ? ?\\n?\\n? ?\\n?\\n??\\n? ? ????\\n?\\n?? ?\\n?\\n??\\n?\\n?\\n?\\n?? ?\\n? ?? ? ?\\n?\\n?\\n?\\n?\\n?? ?\\n? ?\\n?\\n?\\n? ?? ?\\n??\\n? ? ? ? ? ???\\n?\\n??\\n? ??\\n???\\n? ?\\n?\\n?\\n?\\n?? ?\\n?? ??\\n? ? ? ???\\n? ??\\n?\\n?\\n? ?? ? ??\\n? ?\\n?? ?\\n? ?? ? ?\\n??\\n? ?\\n?\\n?\\n?? ? ?\\n?\\n?? ?\\n??\\n? ?\\n? ?\\n?? ? ?\\n? ?\\n?\\n???\\n? ??\\n?\\n? ?\\n??\\n?? ? ?\\n? ??\\n? ?\\n?\\n?\\n?\\n??\\n?\\n??\\n?? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?? ? ? ?\\n?\\n??? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ? ?\\n??\\n?\\n?\\n?\\n?\\n?\\n? ? ?\\n?\\n?\\n? ?\\n?? ? ?\\n?\\n?\\n? ??\\n?\\n?\\n? ?\\n??\\n? ? ?\\n?\\n?\\n?\\n?\\n?? ? ? ? ?\\n?\\n?\\n?\\n?\\n?\\n??\\n??\\n?\\n?\\n?\\n?\\n?\\n?\\n?? ?\\n?\\n?\\n?\\n?\\n?\\n\\n?4\\n\\n??\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n??\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n? ?\\n\\n?\\n\\n2\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n\\n?\\n???\\n?? ?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n?\\n\\n?\\n\\n?\\n??\\n?\\n\\n?\\n\\n?\\n? ?\\n\\n?\\n?\\n??\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n??\\n\\n??\\n\\n? ? ?\\n?? ?\\n? ??\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?? ?\\n?\\n\\n??\\n?\\n\\n?\\n?\\n??\\n\\n?\\n\\n?\\n? ?\\n?\\n?\\n?\\n???\\n? ?\\n? ?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n??\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n? ?\\n\\n?\\n?\\n\\n?\\n?? ?\\n\\n?\\n?? ?\\n\\n??\\n\\n??\\n\\n?\\n?\\n\\n? ?\\n?\\n?? ?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n? ?\\n\\n?\\n\\n?\\n\\n?\\n?\\n??\\n?\\n? ?\\n? ?\\n?\\n?\\n? ?\\n?\\n? ?\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n? ?\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n??\\n? ??\\n??\\n\\n?\\n\\n?\\n\\n?\\n\\n??\\n?\\n??\\n\\n?? ?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n??\\n?\\n\\n?\\n? ?\\n?\\n\\n??\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n??\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n?\\n\\n??\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n??\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?4\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n2\\n\\n?\\n?? ?\\n??\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n? ?\\n??\\n\\n0\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n0\\n\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n?? ? ?\\n?\\n? ?\\n?\\n?\\n?\\n?\\n??\\n? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n? ? ?\\n?\\n??? ?\\n?\\n?\\n?\\n?\\n?\\n?\\n??\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n?? ?\\n? ?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\nPrincipal Component 2\\n\\n?\\n\\n??\\n\\n?\\n\\n?\\n\\n?2\\n\\n?\\n\\n?\\n? ?\\n\\n?\\n\\n?\\n?\\n?\\n\\n? ?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n\\n0\\n\\n0\\n\\n?\\n\\n?\\n?\\n\\n? ?\\n?\\n?\\n?? ? ?\\n?\\n? ?\\n??\\n? ? ? ??\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?2\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n??\\n?\\n\\n?\\n?\\n\\n?\\n\\nPrincipal Component 2\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n???\\n\\n?\\n?\\n\\n??\\n?\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\nPrincipal Component 2\\n\\n2\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n\\n?2\\n\\n?\\n?\\n\\n?\\n?\\n\\nPrincipal Component 2\\n\\n?\\n\\n?\\n?\\n??\\n\\n?\\n?\\n?\\n\\n?\\n\\n?\\n? ?\\n? ?\\n\\n4\\n\\n?\\n\\n?\\n?\\n\\n?\\n\\n?\\n?\\n\\n?2\\n\\n0\\n\\nPrincipal Component 1\\n\\n2\\n\\n4\\n\\n?4\\n\\n?2\\n\\n0\\n\\n2\\n\\n4\\n\\n?4\\n\\n?4\\n\\n?\\n\\n?4\\n\\n?4\\n\\nPrincipal Component 1\\n\\n?2\\n\\n0\\n\\nPrincipal Component 1\\n\\n2\\n\\n?2\\n\\n0\\n\\n2\\n\\n4\\n\\n6\\n\\nPrincipal Component 1\\n\\nFigure 3: The scatter plots of the first two principal components of the dataset. The Spearman\\nversus Pearson are compared (top to bottom). b cell lymphoma, breast tumor, prostate tumor and\\nWilms tumor are explored (from left to right). Each black point represents a sample and each red\\npoint represents a sample belonging to the corresponding tissue type.\\nFor each tissue type listed above, we apply the COCA (Spearman) and the classic high dimensional\\nPCA (Pearson) on the data belonging to this specific tissue type and obtain the first two dominant\\nsparse eigenvectors. Here we set R0 = 100 for both eigenvectors. For COCA, we do a normal score\\ntransformation on the original dataset. We subsequently project the whole dataset to the first two\\nprincipal components using the obtained eigenvectors. The according 2-dimension visualization is\\nillustrated in Figure 3. In Figure 3 each black point represents a sample and each red point represents\\na sample belonging to the corresponding tissue type. It can be observed that, in 2D plots learnt by\\nthe COCA, the red points are averagely more dense and more close to the border of the sample\\ncluster. The first phenomenon indicates that the COCA has the potential to preserve more common\\ninformation shared by samples from the same tissue type. The second phenomenon indicates that\\nthe COCA has the potential to differentiate samples from different tissue types more efficiently.\\n\\n6\\n\\nDiscussion and Comparison with Related Work\\n\\nA similar principal component analysis procedure is proposed by [7], in which they advocate the\\nuse of the transformed Kendall?s tau correlation matrix (instead of the Spearman?s rho correlation\\nmatrix as in the current paper) for estimating the sparse leading eigenvectors. Though both papers\\nare working on principal component analysis, the core ideas are quite different: Firstly, the analysis in [7] is based on a different distribution family called transelliptical, while COCA and Copula\\nPCA are based on the Nonparanormal family. Secondly, by improving the modeling flexibility, in\\n[7] there does not exist a scale-variant variant since it is hard to quantify the transformation functions. In contrast, by introducing the subgaussian transformation function family, the current paper\\nprovides sufficient conditions for Copula PCA to achieve parametric rates. Thirdly, the method in\\n[7] cannot explicitly conduct data visualization, due to the fact that the latent elliptical distribution\\nis unspecified and accordingly they cannot accurately estimate the marginal transformations. For\\nCopula PCA, we are able to provide the projection visualization such as in the experiment part of\\nthis paper. Moreover, via quantifying a sharp convergence rate in estimating the marginal transformations, we can provide the convergence rates in estimating the principal components. Due to space\\nlimit, we refer to the longer version of this paper [6] for more details. Finally, we recommend using\\nthe Spearman?s rho instead of the Kendall?s tau in estimating the correlation coefficients provided\\nthat the Nonparanormal model holds. This is because Spearman?s rho is statistically more efficient\\nthan Kendall?tau within the Nonparanormal family. This research was supported by NSF award\\nIIS-1116730.\\n\\n8\\n\\n\\fReferences\\n[1] A.A. Amini and M.J. Wainwright. High-dimensional analysis of semidefinite relaxations for\\nsparse principal components. In Information Theory, 2008. ISIT 2008. IEEE International\\nSymposium on, pages 2454?2458. IEEE, 2008.\\n[2] T.W Anderson. An introduction to multivariate statistical analysis, volume 2. Wiley New\\nYork, 1958.\\n[3] A. d?Aspremont, F. Bach, and L.E. Ghaoui. Optimal solutions for sparse principal component\\nanalysis. The Journal of Machine Learning Research, 9:1269?1294, 2008.\\n[4] A. d?Aspremont, L. El Ghaoui, M.I. Jordan, and G.R.G. Lanckriet. A direct formulation for\\nsparse PCA using semidefinite programming. Computer Science Division, University of California, 2004.\\n[5] B. Flury. A first course in multivariate statistics. Springer Verlag, 1997.\\n[6] F. Han and H. Liu. High dimensional semiparametric scale-invariant principal component\\nanalysis. Technical Report, 2012.\\n[7] F. Han and H. Liu. Tca: Transelliptical principal component analysis for high dimensional\\nnon-gaussian data. Technical Report, 2012.\\n[8] T. Hastie and W. Stuetzle. Principal curves. Journal of the American Statistical Association,\\npages 502?516, 1989.\\n[9] I.M. Johnstone and A.Y. Lu. On consistency and sparsity for principal components analysis in\\nhigh dimensions. Journal of the American Statistical Association, 104(486):682?693, 2009.\\n[10] I.T. Jolliffe. Principal component analysis, volume 2. Wiley Online Library, 2002.\\n[11] I.T. Jolliffe, N.T. Trendafilov, and M. Uddin. A modified principal component technique based\\non the lasso. Journal of Computational and Graphical Statistics, 12(3):531?547, 2003.\\n[12] M. Journ?ee, Y. Nesterov, P. Richt?arik, and R. Sepulchre. Generalized power method for sparse\\nprincipal component analysis. The Journal of Machine Learning Research, 11:517?553, 2010.\\n[13] J.T. Leek and J.D. Storey. Capturing heterogeneity in gene expression studies by surrogate\\nvariable analysis. PLoS Genetics, 3(9):e161, 2007.\\n[14] H. Liu, F. Han, M. Yuan, J. Lafferty, and L. Wasserman. High dimensional semiparametric\\ngaussian copula graphical models. Annals of Statistics, 2012.\\n[15] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high\\ndimensional undirected graphs. The Journal of Machine Learning Research, 10:2295?2328,\\n2009.\\n[16] Z. Ma. Sparse principal component analysis and iterative thresholding. Arxiv preprint\\narXiv:1112.2432, 2011.\\n[17] Matthew McCall, Benjamin Bolstad, and Rafael Irizarry. Frozen robust multiarray analysis\\n(frma). Biostatistics, 11:242?253, 2010.\\n[18] D. Paul and I.M. Johnstone. Augmented sparse principal component analysis for high dimensional data. Arxiv preprint arXiv:1202.1242, 2012.\\n[19] H. Shen and J.Z. Huang. Sparse principal component analysis via regularized low rank matrix\\napproximation. Journal of multivariate analysis, 99(6):1015?1034, 2008.\\n[20] V.Q. Vu and J. Lei. Minimax rates of estimation for sparse pca in high dimensions. Arxiv\\npreprint arXiv:1202.0786, 2012.\\n[21] D.M. Witten, R. Tibshirani, and T. Hastie. A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis. Biostatistics,\\n10(3):515?534, 2009.\\n[22] L. Xue and H. Zou. Regularized rank-based estimation of high-dimensional nonparanormal\\ngraphical models. Annals of Statistics, 2012.\\n[23] X.T. Yuan and T. Zhang. Truncated power method for sparse eigenvalue problems. Arxiv\\npreprint arXiv:1112.2679, 2011.\\n[24] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the\\nRoyal Statistical Society: Series B (Statistical Methodology), 67(2):301?320, 2005.\\n[25] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics, 15(2):265?286, 2006.\\n9\\n\\n\\f\",\n          \"84\\n\\nWilson and Bower\\n\\nComputer Simulation of Oscillatory Behavior\\nin Cerebral Cortical Networks\\n\\nMatthew A. Wilson and James M. Bower!\\nComputation and Neural Systems Program\\nDivision of Biology, 216-76\\nCalifornia Institute of Technology\\nPasadena, CA 9 1125\\n\\nABSTRACT\\nIt has been known for many years that specific regions of the work-\\n\\ning cerebral cortex display periodic variations in correlated cellular\\nactivity. While the olfactory system has been the focus of much of\\nthis work, similar behavior has recently been observed in primary\\nvisual cortex. We have developed models of both the olfactory\\nand visual cortex which replicate the observed oscillatory properties of these networks. Using these models we have examined the\\ndependence of oscillatory behavior on single cell properties and network architectures. We discuss the idea that the oscillatory events\\nrecorded from cerebral cortex may be intrinsic to the architecture\\nof cerebral cortex as a whole, and that these rhythmic patterns\\nmay be important in coordinating neuronal activity during sensory\\nprocessmg.\\n\\n1\\n\\nINTRODUCTION\\n\\nAn obvious characteristic of the general behavior of cerebral cortex, as evident in\\nEEG recordings, is its tendency to oscillate. Cortical oscillations have been observed\\nboth in the electric fields generated by populations of cells (Bressler and Freeman\\n1 Please\\n\\naddress correspondence to James M. Bower at above address.\\n\\n\\fComputer Simulation or Oscillatory Behavior in Cerebral Cortical Networks\\n1980) as well as in the activity of single cells (Llinas 1988). Our previous efforts\\nto study this behavior involve the construction of realistic, large scale computer\\nsimulations of one particular cortical area, the piriform (olfactory) cortex (Wilson\\nand Bower 1989). While the oscillatory behavior of this region has been known\\nfor some time (Adrian 1942; Bressler and Freeman 1980), more recent findings\\nof oscillations within visual cortex (Eckhorn et al.,1988; Gray et at. 1989) have\\ngenerated increased interest in the role of oscillations in cerebral cortex in general.\\nIt is particularly intriguing that although these cortical areas receive very different\\nkinds of sensory information, the periodic activity seen in both structures share a\\ncommon principle frequency component in the range of 30-60 Hz. At the same time,\\nhowever, the phase relationships of activity across each cortex differ. Piriform cortex\\ndisplays systematic phase shifts in field potential responses to afferent activation\\n(Freeman 1978; Haberly 1973), while correlations of neuronal activity in visual\\ncortex indicate no such systematic phase shifts (Gray et al. 1989).\\nIn order to compare this oscillatory behavior in these two cortical systems, we have\\ndeveloped a model of visual cortex by modifying the original piriform cortex model\\nto reflect visual cortical network features.\\n\\n2\\n2.1\\n\\nMODEL STRUCTURE\\nCOMMON MODEL FEATURES\\n\\nEach simulation has at its base the three basic cell types found throughout cerebral\\ncortex (Figure 1). The principle excitatory neuron, the pyramidal cell, is modeled\\nhere as five coupled membrane compartments. In addition there are two inhibitory\\nneurons one principally mediating a slow K + inhibition and one mediating a fast CIinhibition. Both are modeled as a single compartment. Connections between modeled cells are made by axons with finite conduction velocities, but no explicit axonal\\nmembrane properties other than delay are included. Synaptic activity is produced\\nby simulating the action-potential triggered release of presynaptic transmitter and\\nthe resulting flow of transmembrane current through membrane channels. Each of\\nthese channels is described with parameters governing the time course and amplitude of synaptically activated conductance changes. The compartmental models of\\nthe cells integrate the transmembrane and axial currents to produce transmembrane\\nvoltages. Excursions of the cell body membrane voltage above a specified threshold trigger action potentials. Details of the modeling procedures are described in\\nWilson and Bower (1989).\\nEach model is intended to represent a 10 mm x 6 mm cortical region. The many millions of actual neurons in these areas are represented by 375 cells of the three types\\nfor a total of 1125 cells. The input to each cortex is prvV'ided by 100 independent\\nfibers.\\n\\n8S\\n\\n\\f86\\n\\nWilson and Bower\\n\\nA\\n\\n10 mm\\n\\nD\\n\\nB\\n\\nrotItrally directed\\n_ciation fiber.\\n\\ncaudally directed\\n_elation flbe,.\\n\\n_elldlon fibe,.\\n\\nFigure 1: In the piriform cortex, input (A) and association fiber (B) projections\\nmake distributed lateral contacts with cells over the extent of the cortex. In the\\nvisual cortex model, input projections make local contact with cells over a 1 mm\\nradius in a point-to-point fashion (C) and association fibers connect to cells within\\na limited radius (D).\\nWhile both the piriform and visual cortex models reflect these basic features of\\ncerebral cortical architecture, both also contain major structural simplifications.\\nThe model referred to as \\\"visual cortex\\\", is particularly simplified. Our objective\\nwas to reproduce cortical oscillations characteristic of visual cortex by modifying\\nthose basic architectural features that differ between these two brain regions.\\n\\n2.2\\n\\nMODEL DIFFERENCES\\n\\nThe principle differences between the model of piriform and visual cortex involve\\nchanges in the topography of input projections, and in the extent of intrinsic connections within each model. In piriform cortex, afferent input from the olfactory bulb\\narrives via a tract ofaxons (LOT) projecting across the surface of the cortex (Fig.\\nlA) with no topographic relationship between the site of origin of individual LOT\\naxons in the olfactory bulb and their region of termination in the cortex (Haberly\\n1985). In contrast, projections from the lateral geniculate nucleus to visual cortex\\nare highly topographic, reflecting the retinotopic organization of many structures\\nin the visual system (Van Essen 1979). In piriform cortex, excitatory intrinsic association connections are sparse, distributed, and non-topographic, extending across\\n\\n\\fComputer Simulation of Oscillatory Behavior in Cerebral Cortical Networks\\n\\nthe entire cortex (Fig. Ie) (Haberly 1985). In the visual cortex, this association\\nfiber system is much more limited in extent (Gilbert 1983).\\n\\n3\\n\\nRESULTS\\n\\nSpace limitations do not allow a complete discussion of previous results modeling\\npiriform cortex. Readers are referred to Wilson and Bower (1989) for additional\\ndetails. Here, we will describe data obtained from the modified piriform cortex\\nmodel which replicate results from visual cortex.\\n\\n1\\n\\n2\\n\\n2-2\\n\\n?\\n1-2\\n-50\\n\\nT.... (moecl\\n\\n1\\n\\n2\\n\\no\\n\\n50\\n\\nTlme(...ecl\\n\\n2-2\\n\\n?\\n1-2\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\n0\\n\\n~\\n\\nTIme (moecl\\n\\n~\\n\\n~\\n\\n~\\n\\nTlme(maecl\\n\\nFigure 2: Comparison of auto and cross correlations from modeled (middle) and\\nactual (right) (modified from Gray et al. 1989) visual cortex. The left column shows\\na diagram of the model with the stimulus region shaded. The numbers indicate the\\nlocation of the recording sites referred to in the auto (2-2) and cross (1-2) correlations. The correlations generated by presentation of a continuous and broken bar\\nstimulus are shown in the upper and lower panels respectively.\\n\\n87\\n\\n\\f88\\n\\nWilson and Bower\\nFigure 2 shows a comparison of auto and cross correlations of neuronal spike activity taken from both simulated and actual (Gray et al. 1989) experimental data. In\\neach case the two recording sites in visual cortex are separated by approximately\\n6 mm. Total cross correlations in the modeled data were computed by averaging\\ncorrelations from 50 individual 500 msec trials. Within each trial simulated activity\\nwas generated by providing input representing bars of light at different locations in\\nthe visual field. In these cases the model produced oscillatory auto and cross correlations with peak energy in the 30-60 Hz range. As in the experimental data, this\\neffect is most clearly seen when the stimulus is a continuous bar of light activating\\ncells between the two recorded sites (fig. 2). A broken bar which does not stimulate\\nthe intermediate region produces a weaker response (fig. 2), again consistent with\\nexperimental evidence.\\nThe oscillatory form of the the cross correlation function suggests coherent firing of\\nneurons at the two recorded locations. In order to determine the degree of synchrony\\nbetween modeled neurons, the difference in phase between the firing of cells in these\\nlocations was estimated by measuring the offset of the dominant peak in the cross\\ncorrelation function. These values were consistent with measurements obtained\\nboth through chi-square fitting of a modified sinc function and measurement of the\\nphase of the peak frequency component in the correlation function power spectra.\\nThese measurements indicate phase shifts near zero ? 3 msec).\\n\\n3.1\\n\\nSTIMULUS EFFECTS\\n\\nAs shown in figure 2, correlations are induced by the presence of a stimulus. However, in both experimental and simulated results these correlations cannot be accounted for through a simple stimulus locking effect. Shuffling the trials with respect\\nto each other prior to calculating cross correlation functions showed oscillations\\nwhich were greatly diminished or completely absent. At the same time, simulations\\nrun in the absence of bar stimuli produced low baseline activity with no oscillations.\\nThese results demonstrate that while the stimulus is necessary to induce oscillatory\\nbehavior, the coherence between distant points is not due to the stimulus alone.\\n\\n3.2\\n\\nFREQUENCY\\n\\nThe visual cortex model generates oscillatory neural activity at a frequency in the\\nrange of 30-60 Hz, consistent with actual data. As found in the model piriform\\ncortex, the frequency of these oscillations is primarily determined by the time course\\nof the fast feedback inhibitory input. Allowing inhibitory cells to inhibit other\\ninhibitory cells within a local region improved frequency locking and produced auto\\nand cross correlations with more pronounced oscillatory characteristics.\\n\\n3.3\\n\\nCOHERENCE\\n\\nIn order to demonstrate the essential role of the association fiber system in establishing coherent activity, simulations were performed in which all long-range (> 1 mm)\\n\\n\\fComputer Simulation of Oscillatory Behavior in Cerebral Cortical Networks\\nassociation fibers were eliminated. Under these conditions the auto correlations at\\neach recording site continued to show strong oscillatory behavior, but oscillations\\nin the cross correlation function were completely eliminated. Increasing the range\\nof association fibers from 1 to 2 mm restored coherent oscillatory behavior. This\\ndemonstrates that long-range association fibers are critical in establishing coherence\\nwhile local circuitry is sufficient for sustaining oscillations.\\n\\nl'h\\n\\nu..u.. dl.M .. ?\\n\\n1. .,. .\\n.l. ,..\\\"\\n\\n...... \\\"M..!' ....\\n\\nh+ ...''*\\n\\n375-SOOmMC\\n\\nni' .. tl\\\",I.,U,\\n\\n0. . . . . . .\\n\\n2!O:375mMC\\n\\n..L\\n\\n,*.....\\n\\n0:125maec\\n\\nJ\\n't\\\"\\\" wt ::\\\"\\n.20\\n\\n'tMw\\np'\\n-50\\n\\n0\\n\\nd, .\\\" 1M\\n\\nO-SOOmHC\\n\\nh\\n\\n50\\n\\nTillie (mNC)\\n\\n:11i~ IJ~\\no\\n\\n10\\n\\n.- .4 _e ... 20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\neo\\n\\no\\n\\n?\\n\\nI\\n\\n10\\n\\n20\\n\\n.\\\"\\n\\n30\\n\\n40\\n\\n50\\n\\nI\\n\\n60\\n\\n&70\\n\\n?\\n\\neo\\n\\nFnoquency (Hz)\\n\\nFigure 3: Time course of cross correlation functions for relative association fiber\\ncoupling strengths of 200 (left) and 300 (right). Upper traces display correlations\\ntaken at successive 125 intervals over the 500 msec period. The bottom-most correlation function covers the entire 500 msec interval. The lower panels display the\\npower spectra of the overall correlation function.\\n\\n89\\n\\n\\f90\\n\\nWilson and Bower\\n3.3.1\\n\\nAssociation Fiber Delay\\n\\nTo examine the dependence of zero-phase coherence between distant sites on association fibers characteristics, the propagation velocity for spikes travelling between\\npyramidal cells was reduced from a mean of 0.86 mls to 0.43 m/s. Under these conditions the phase shift in the cross correlation function for a continuous bar stimulus\\nremained less than 3 msec. This result indicates that the zero-phase coherence is\\nnot a direct function of association fiber delays.\\n3.3.2\\n\\nCoupling Strength\\n\\nAs shown in figure 3, increasing the degree of association fiber coupling by increasing\\nsynaptic weights produced a transition from zero-phase coherence to a coherence\\nwith an 8 msec phase shift. Intermediate shifts were not observed. Figure 3 also\\nillustrates the time course of coherence and phase relationships. There is a tendency\\nfor the initial stimulus onset period (0-125 msec) to show zero-phase preference.\\nLater periods (> 125 msec) reflect the association coupling induced phase shift.\\nFor weak coupling which produces zero-phase behavior, the correlation structure\\ndecays over the 500 msec stimulus period. Increased coupling strength provides\\nmore sustained coherence, as does the addition of mutual inhibition.\\n\\n4\\n\\nDISCUSSION\\n\\nAnalysis of the behavior of the models shows that several components are particularly important in establishing the different phase and frequency relationships. A\\nkey factor in establishing zero-phase coherence appears to be the stimulation of a\\ncellular population which can activate, via association fibers, adjacent regions in a\\nsymmetric fashion. In the case of the continuous bar, this intermediate region lies\\nin the center of the bar. This is consistent with experimental results which indicate\\nreduced coherence with bar stimuli which do not excite this region. The model also\\nindicates that frequency can be effectively modulated by inhibitory feedback. The\\nfact that inhibitory events with similar temporal properties are found throughout\\nthe cerebral cortex suggests that oscillations in the 30-60 Hz range will be found in\\na number of different cortical areas.\\nInterpreting phase coherence from correlation functions produced from the average\\nof many simulation trials pointed out the need to distinguish average phase effects\\nfrom instantaneous phase effects. Instantaneous phase implies that the statistics\\nof the correlation function taken at any point of any trial are consistent with the\\nstatistics of the combined data. Average phase allows for systematic within-trial and\\nbetween-trial variability and is, therefore, a weaker assertion of actual coherence.\\nThis distinction is particularly important for theories which rely on phase encoding\\nof stimulus information. Analysis of our model results indicates that the observed\\nphase relationships are an average rather than an instantaneous effect.\\nBased on previous observations of the behavior of the piriform cortex model, we\\nhave proposed that high frequency oscillations may reflect the gating of intrinsic\\n\\n\\fComputer Simulation of Oscillatory Behavior in Cerebral Cortical Networks\\nnetwork integration intervals. This modulatory role would serve to assure that\\ncells do not fire before they have received the necessary input to initiate another\\nround of cortical activity. While this is dearly only one possible functional role\\nfor oscillations in piriform cortex, the model is being used to extend this idea to\\nprocessing in the visual cortex as well.\\n\\nAcknowledgements\\nThis research was supported by the NSF (EET-8700064), the ONR (Contract\\nN00014-88-K-0513), and the Lockheed Corporation.\\n\\nReferences\\nAdrian, E.D. 1942. Olfactory reactions in the brain of the hedgehog. J. Physiol.\\n(Lond.) 100, 459-472.\\nBressler, S.L. and W.J. Freeman. 1980. Frequency analysis of olfactory system\\nEEG in cat, rabbit and rat. Electroenceph. din. Neurophysiol. 50, 19-24.\\nEckhorn, R., R. Bauer, Jordan, M. Brosch, W. Kruse, M. Munk, and H.J. Reitboeck.\\n1988. Coherent oscillations: A mechanism of feature linking in the visual cortex?\\nBioI. Cybern. 60, 121-130.\\nFreeman, W.J. 1978. Spatial properties of an EEG event in the olfactory bulb and\\ncortex. Electroenceph. din. Neurophysiol. 44,586-605.\\nGilbert, C.D. 1983. Microcircuitry of the visual cortex. Ann. Rev. Neurosci.\\n6,217-247.\\nGray, C.M., P. Konig, A.K. Engel, W. Singer. 1989. Oscillatory responses in cat\\nvisual cortex exhibit inter-columnar synchronization which reflects global stimulus\\nproperties. Nature 338, 334-337.\\nHaberly, L.B. 1985. Neuronal circuitry in olfactory cortex: anatomy and functional\\nimplications. Chern. Senses 10, 219-238.\\nHaberly, L.B. 1973. Summed potentials evoked in opossum prepyriform cortex. J.\\nNeurophysiol. 36, 775-788.\\nKammen, D.M., P.J. Holmes, and C. Koch. 1989. Cortical architecture and oscillations in neuronal networks: Feedback versus local coupling. In: Models of Brain\\nFunction R.M.J. Cotterill, Ed. (Cambridge Univ. Press .)\\nLlinas, R. 1988. The intrinsic electrophysiological properties of mammalian neurons:\\nInsights into central nervous system function. Science 242:1654-1664.\\nWilson, M.A. and J.M Bower. 1989. The simulation of large scale neuronal networks. In Methods in Neuronal Modeling: From Synapses to Networks C. Koch\\nand I. Segev, Eds. (MIT Press, Cambridge, MA.) pp. 291-334.\\nVan Essen, D.C. 1979. Visual areas of the mammalian cerebral cortex. Ann. Rev.\\nNeurosci. 2, 227-263.\\n\\n91\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "papers"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9388f71a-7602-41a8-bb6f-52a7d188187a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>On Neural Networks with Minimal\\nWeights\\n\\nJ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3381</th>\n",
              "      <td>Layer-wise analysis of deep networks with Gaus...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6462</th>\n",
              "      <td>Classifying with Gaussian Mixtures and\\nCluste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>796\\n\\nSPEECH RECOGNITION: STATISTICAL AND\\nNE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1608</th>\n",
              "      <td>Out-of-Sample Extensions for LLE, Isomap,\\nMDS...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9388f71a-7602-41a8-bb6f-52a7d188187a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9388f71a-7602-41a8-bb6f-52a7d188187a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9388f71a-7602-41a8-bb6f-52a7d188187a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-294c2e00-c2a1-42b4-a987-89148653577d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-294c2e00-c2a1-42b4-a987-89148653577d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-294c2e00-c2a1-42b4-a987-89148653577d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             paper_text\n",
              "75    On Neural Networks with Minimal\\nWeights\\n\\nJ ...\n",
              "3381  Layer-wise analysis of deep networks with Gaus...\n",
              "6462  Classifying with Gaussian Mixtures and\\nCluste...\n",
              "808   796\\n\\nSPEECH RECOGNITION: STATISTICAL AND\\nNE...\n",
              "1608  Out-of-Sample Extensions for LLE, Isomap,\\nMDS..."
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'title', 'abstract',\n",
        "                              'event_type', 'pdf_name', 'year'], axis=1)\n",
        "\n",
        "# sample only 100 papers\n",
        "papers = papers.sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7viMIbY5rLf"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "xxBPf_Fh5rLg",
        "outputId": "50b95723-0916-4b87-bd3c-928d2308f09a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>on neural networks with minimal\\nweights\\n\\nj ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3381</th>\n",
              "      <td>layer-wise analysis of deep networks with gaus...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6462</th>\n",
              "      <td>classifying with gaussian mixtures and\\ncluste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>796\\n\\nspeech recognition: statistical and\\nne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1608</th>\n",
              "      <td>out-of-sample extensions for lle isomap\\nmds e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "75      on neural networks with minimal\\nweights\\n\\nj ...\n",
              "3381    layer-wise analysis of deep networks with gaus...\n",
              "6462    classifying with gaussian mixtures and\\ncluste...\n",
              "808     796\\n\\nspeech recognition: statistical and\\nne...\n",
              "1608    out-of-sample extensions for lle isomap\\nmds e...\n",
              "Name: paper_text_processed, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOfCe8Mb5rLg"
      },
      "source": [
        "##### Tokenize words and further clean-up text\n",
        "\n",
        "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DaKoVqH5rLh",
        "outputId": "30232771-e5d5-473b-f370-37d166b5ee54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['on', 'neural', 'networks', 'with', 'minimal', 'weights', 'ehoshua', 'bruck', 'vasken', 'bohossian', 'california', 'institute', 'of', 'technology', 'mail', 'code', 'pasadena', 'ca', 'mail', 'vincent', 'bruck', 'iparadise', 'cal', 'tech', 'edu', 'abstract', 'linear', 'threshold', 'elements', 'are']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUbdRiLp5rLh"
      },
      "source": [
        "** **\n",
        "#### Step 3: Phrase Modeling: Bigram and Trigram Models\n",
        "** **\n",
        "\n",
        "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
        "\n",
        "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold.\n",
        "\n",
        "*The higher the values of these param, the harder it is for words to be combined.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy1tSMVY5rLi"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtjFAI8k5rLi"
      },
      "source": [
        "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
        "\n",
        "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPF7adLJ5rLi",
        "outputId": "f0a8ee50-8443-45fc-db12-67f4a642b44f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# NLTK Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaZOsDY65rLi"
      },
      "outputs": [],
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-XNg1Rm5rLj"
      },
      "source": [
        "Let's call the functions in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FtQ_o3S5rLj",
        "outputId": "050bb212-28ef-481c-e0c1-993664cc752b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZAv-Hbd5rLj",
        "outputId": "d30383ad-2fdf-422b-b361-4023eefccec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['neural_network', 'minimal', 'weight', 'mail', 'code', 'pasadena', 'mail', 'vincent', 'bruck', 'iparadise', 'cal', 'tech', 'abstract', 'linear', 'threshold', 'element', 'basic', 'building_block', 'artificial', 'neural_network', 'linear', 'threshold', 'element', 'compute', 'function', 'weight', 'sum', 'input', 'variable', 'weight']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMtBc1sP5rLj"
      },
      "source": [
        "** **\n",
        "#### Step 4: Data transformation: Corpus and Dictionary\n",
        "** **\n",
        "\n",
        "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhUtkfK75rLj",
        "outputId": "ad4c82ee-948e-44bd-bf9d-0f67137fbc75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 7), (19, 1), (20, 8), (21, 1), (22, 3), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 3), (29, 1)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6arWMKk5rLk"
      },
      "source": [
        "** **\n",
        "#### Step 5: Base Model\n",
        "** **\n",
        "\n",
        "We have everything required to train the base LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior (we'll use default for the base model).\n",
        "\n",
        "chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
        "\n",
        "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GssDSZSM5rLk"
      },
      "outputs": [],
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=10,\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydpwhlK65rLk"
      },
      "source": [
        "** **\n",
        "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
        "\n",
        "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23hqPnx5rLk",
        "outputId": "0412e590-e9c5-48e1-d993-bb1eb6f8967d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.023*\"feature\" + 0.011*\"learn\" + 0.011*\"set\" + 0.009*\"model\" + 0.009*\"use\" '\n",
            "  '+ 0.008*\"class\" + 0.007*\"result\" + 0.007*\"datum\" + 0.007*\"attribute\" + '\n",
            "  '0.006*\"training\"'),\n",
            " (1,\n",
            "  '0.001*\"model\" + 0.001*\"feature\" + 0.001*\"function\" + 0.001*\"use\" + '\n",
            "  '0.001*\"learn\" + 0.001*\"show\" + 0.001*\"datum\" + 0.001*\"set\" + 0.001*\"figure\" '\n",
            "  '+ 0.001*\"time\"'),\n",
            " (2,\n",
            "  '0.011*\"model\" + 0.009*\"network\" + 0.008*\"time\" + 0.007*\"figure\" + '\n",
            "  '0.007*\"show\" + 0.007*\"use\" + 0.005*\"response\" + 0.005*\"result\" + '\n",
            "  '0.005*\"neuron\" + 0.005*\"cell\"'),\n",
            " (3,\n",
            "  '0.009*\"set\" + 0.009*\"function\" + 0.008*\"use\" + 0.007*\"learn\" + '\n",
            "  '0.007*\"model\" + 0.007*\"problem\" + 0.007*\"method\" + 0.006*\"result\" + '\n",
            "  '0.006*\"log\" + 0.006*\"number\"'),\n",
            " (4,\n",
            "  '0.010*\"function\" + 0.009*\"problem\" + 0.009*\"set\" + 0.008*\"learn\" + '\n",
            "  '0.008*\"player\" + 0.007*\"show\" + 0.007*\"optimal\" + 0.007*\"policy\" + '\n",
            "  '0.007*\"probability\" + 0.007*\"arm\"'),\n",
            " (5,\n",
            "  '0.012*\"model\" + 0.012*\"system\" + 0.009*\"learn\" + 0.009*\"network\" + '\n",
            "  '0.008*\"layer\" + 0.008*\"feature\" + 0.008*\"input\" + 0.008*\"unit\" + '\n",
            "  '0.007*\"deep\" + 0.006*\"datum\"'),\n",
            " (6,\n",
            "  '0.019*\"model\" + 0.015*\"image\" + 0.011*\"use\" + 0.009*\"feature\" + '\n",
            "  '0.009*\"network\" + 0.008*\"layer\" + 0.007*\"set\" + 0.006*\"input\" + '\n",
            "  '0.006*\"value\" + 0.006*\"parameter\"'),\n",
            " (7,\n",
            "  '0.014*\"weight\" + 0.011*\"vector\" + 0.011*\"matrix\" + 0.010*\"coreset\" + '\n",
            "  '0.009*\"function\" + 0.009*\"result\" + 0.009*\"size\" + 0.008*\"use\" + '\n",
            "  '0.007*\"compute\" + 0.007*\"assumption\"'),\n",
            " (8,\n",
            "  '0.012*\"use\" + 0.010*\"feature\" + 0.009*\"network\" + 0.009*\"function\" + '\n",
            "  '0.009*\"figure\" + 0.008*\"datum\" + 0.007*\"model\" + 0.007*\"show\" + '\n",
            "  '0.006*\"learn\" + 0.006*\"input\"'),\n",
            " (9,\n",
            "  '0.017*\"model\" + 0.013*\"use\" + 0.011*\"learn\" + 0.008*\"datum\" + 0.006*\"set\" + '\n",
            "  '0.006*\"function\" + 0.006*\"word\" + 0.006*\"show\" + 0.005*\"method\" + '\n",
            "  '0.005*\"training\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2396_il5rLk"
      },
      "source": [
        "#### Compute Model Perplexity and Coherence Score\n",
        "\n",
        "Let's calculate the baseline coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJixp40n5rLk",
        "outputId": "d47d23db-b86c-4363-805b-dfe1a7d9a5f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score:  0.2961800042639747\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRGciXT65rLk"
      },
      "source": [
        "** **\n",
        "#### Step 6: Hyperparameter tuning\n",
        "** **\n",
        "First, let's differentiate between model hyperparameters and model parameters :\n",
        "\n",
        "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
        "\n",
        "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
        "\n",
        "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters:\n",
        "- Number of Topics (K)\n",
        "- Dirichlet hyperparameter alpha: Document-Topic Density\n",
        "- Dirichlet hyperparameter beta: Word-Topic Density\n",
        "\n",
        "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `C_v` as our choice of metric for performance comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xcshfunm5rLl"
      },
      "outputs": [],
      "source": [
        "# supporting function\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
        "\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "\n",
        "    return coherence_model_lda.get_coherence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frxnvbdo5rLl"
      },
      "source": [
        "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./results', exist_ok=True)"
      ],
      "metadata": {
        "id": "gYvp9B61-iRK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcQkoXo85rLl",
        "outputId": "bc1cf5b1-3dab-4490-f894-ae82784ac664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 540/540 [1:51:00<00:00, 12.33s/it]\n",
            "100%|██████████| 540/540 [1:36:24<00:00, 10.71s/it]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        "# Topics range\n",
        "min_topics = 2\n",
        "max_topics = 11\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),\n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "\n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word,\n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "\n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTlkSOY55rLl"
      },
      "source": [
        "** **\n",
        "#### Step 7: Final Model\n",
        "** **\n",
        "\n",
        "Based on external evaluation (Code to be added from Excel based analysis), let's train the final model with parameters yielding highest coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RWW1UwH-5rLl"
      },
      "outputs": [],
      "source": [
        "num_topics = 8\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=num_topics,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=0.01,\n",
        "                                           eta=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjx4IFBC5rLl",
        "outputId": "66fbb2bf-476d-4e56-a58e-f32447281b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.017*\"feature\" + 0.009*\"learn\" + 0.009*\"model\" + 0.008*\"set\" + 0.007*\"use\" '\n",
            "  '+ 0.006*\"datum\" + 0.005*\"word\" + 0.004*\"training\" + 0.004*\"time\" + '\n",
            "  '0.004*\"sample\"'),\n",
            " (1,\n",
            "  '0.004*\"crash\" + 0.003*\"bug\" + 0.003*\"program\" + 0.002*\"utility\" + '\n",
            "  '0.001*\"false_positive\" + 0.001*\"assertion\" + 0.001*\"user\" + 0.001*\"run\" + '\n",
            "  '0.001*\"counter\" + 0.001*\"feature\"'),\n",
            " (2,\n",
            "  '0.012*\"model\" + 0.011*\"network\" + 0.008*\"use\" + 0.006*\"input\" + '\n",
            "  '0.006*\"figure\" + 0.006*\"show\" + 0.006*\"system\" + 0.006*\"image\" + '\n",
            "  '0.005*\"result\" + 0.005*\"time\"'),\n",
            " (3,\n",
            "  '0.009*\"use\" + 0.009*\"function\" + 0.008*\"set\" + 0.008*\"model\" + '\n",
            "  '0.007*\"learn\" + 0.006*\"datum\" + 0.006*\"method\" + 0.006*\"result\" + '\n",
            "  '0.005*\"problem\" + 0.005*\"number\"'),\n",
            " (4,\n",
            "  '0.006*\"function\" + 0.005*\"point\" + 0.005*\"problem\" + 0.005*\"result\" + '\n",
            "  '0.005*\"show\" + 0.004*\"proof\" + 0.004*\"set\" + 0.004*\"learn\" + 0.004*\"use\" + '\n",
            "  '0.004*\"case\"'),\n",
            " (5,\n",
            "  '0.007*\"model\" + 0.006*\"use\" + 0.005*\"feature\" + 0.005*\"analogy\" + '\n",
            "  '0.005*\"deep\" + 0.004*\"network\" + 0.003*\"representation\" + 0.003*\"image\" + '\n",
            "  '0.003*\"layer\" + 0.003*\"visual\"'),\n",
            " (6,\n",
            "  '0.007*\"value\" + 0.005*\"use\" + 0.005*\"model\" + 0.005*\"player\" + '\n",
            "  '0.005*\"figure\" + 0.004*\"learn\" + 0.004*\"arm\" + 0.004*\"policy\" + '\n",
            "  '0.004*\"contrast\" + 0.003*\"function\"'),\n",
            " (7,\n",
            "  '0.007*\"chip\" + 0.007*\"weight\" + 0.005*\"function\" + 0.004*\"threshold\" + '\n",
            "  '0.003*\"board\" + 0.003*\"row\" + 0.002*\"circuit\" + 0.002*\"parallel\" + '\n",
            "  '0.002*\"minimal\" + 0.002*\"implement\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E_XQDqN5rLl"
      },
      "source": [
        "** **\n",
        "#### Step 8: Visualize Results\n",
        "** **"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyLDAvis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVdXzq8AkZRt",
        "outputId": "e039ef3b-b3cd-47a2-9a08-ffb2d0aabc44"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "hExrOXLV5rLm",
        "outputId": "8c813445-f37e-43bb-c64a-324dd28aff72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "3     -0.131555  0.042899       1        1  38.447538\n",
              "2     -0.084106 -0.072617       2        1  30.049795\n",
              "0     -0.065898 -0.003121       3        1  14.765850\n",
              "4     -0.012821  0.069801       4        1   6.873789\n",
              "6      0.011663 -0.010527       5        1   5.242702\n",
              "5      0.046064 -0.048010       6        1   3.117632\n",
              "7      0.108236  0.010456       7        1   1.109374\n",
              "1      0.128417  0.011119       8        1   0.393320, topic_info=                  Term         Freq        Total Category  logprob  loglift\n",
              "569            feature   864.000000   864.000000  Default  30.0000  30.0000\n",
              "147           function   970.000000   970.000000  Default  29.0000  29.0000\n",
              "252            network   731.000000   731.000000  Default  28.0000  28.0000\n",
              "403             weight   305.000000   305.000000  Default  27.0000  27.0000\n",
              "241              model  1538.000000  1538.000000  Default  26.0000  26.0000\n",
              "...                ...          ...          ...      ...      ...      ...\n",
              "4156             count     0.338288    38.827049   Topic8  -7.6425   0.7953\n",
              "928   cross_validation     0.327415    35.985466   Topic8  -7.6752   0.8387\n",
              "54                code     0.352480    84.371982   Topic8  -7.6014   0.0603\n",
              "1245             score     0.331902    67.084212   Topic8  -7.6615   0.2294\n",
              "147           function     0.342657   970.120231   Topic8  -7.6297  -2.4101\n",
              "\n",
              "[557 rows x 6 columns], token_table=      Topic      Freq                 Term\n",
              "term                                      \n",
              "9397      1  0.239866           abnormally\n",
              "9397      2  0.239866           abnormally\n",
              "9397      3  0.239866           abnormally\n",
              "3345      1  0.025819               action\n",
              "3345      2  0.361470               action\n",
              "...     ...       ...                  ...\n",
              "1307      5  0.010344                 word\n",
              "1307      6  0.020687                 word\n",
              "3102      1  0.064899  word_representation\n",
              "3102      2  0.064899  word_representation\n",
              "3102      3  0.778790  word_representation\n",
              "\n",
              "[2156 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 3, 1, 5, 7, 6, 8, 2])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el4751337409906151043562663644\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el4751337409906151043562663644_data = {\"mdsDat\": {\"x\": [-0.13155507117518683, -0.08410556972021418, -0.06589840045488984, -0.012820666743339773, 0.011663005354614588, 0.0460639187523753, 0.10823609774779137, 0.1284166862388495], \"y\": [0.04289920561632295, -0.0726173892194858, -0.003120902294383254, 0.06980065991656079, -0.010527110987982131, -0.048009511752771225, 0.01045571963592958, 0.01111932908580904], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [38.447538239451504, 30.0497950779805, 14.765850005958528, 6.8737888845049815, 5.242702075338537, 3.11763159426225, 1.1093740356909982, 0.39332008681270975]}, \"tinfo\": {\"Term\": [\"feature\", \"function\", \"network\", \"weight\", \"model\", \"use\", \"value\", \"image\", \"layer\", \"figure\", \"learn\", \"deep\", \"response\", \"set\", \"problem\", \"input\", \"threshold\", \"state\", \"proof\", \"point\", \"show\", \"representation\", \"train\", \"system\", \"visual\", \"policy\", \"cell\", \"row\", \"unit\", \"optimal\", \"tensor\", \"spectral_clustere\", \"inducing_input\", \"sketch\", \"structured_prediction\", \"fitc\", \"mgomp\", \"frank_wolfe\", \"boost\", \"green_node\", \"pq\", \"speedup\", \"dag\", \"singularity\", \"lamp\", \"causal_ordere\", \"kernel_embedding\", \"recovery\", \"vfe\", \"low_rank\", \"kernel_embedde\", \"gap_safe\", \"asynchronous\", \"co_regularization\", \"variable_selection\", \"booster\", \"misspecifie\", \"complexity\", \"hash\", \"tensor_power\", \"view\", \"decomposition\", \"regression\", \"gradient\", \"statistical\", \"optimization\", \"convergence\", \"entry\", \"cluster\", \"log\", \"method\", \"matrix\", \"bound\", \"algorithm\", \"function\", \"variable\", \"step\", \"bind\", \"set\", \"obtain\", \"problem\", \"theorem\", \"number\", \"point\", \"size\", \"section\", \"use\", \"error\", \"datum\", \"sample\", \"follow\", \"define\", \"result\", \"learn\", \"model\", \"give\", \"case\", \"show\", \"base\", \"also\", \"first\", \"class\", \"value\", \"time\", \"marker\", \"neuron\", \"stimulus\", \"recurrent\", \"synapse\", \"fcp\", \"student\", \"feedforward\", \"phone\", \"fire\", \"tunneling\", \"synapsis\", \"pseudo_ensemble\", \"highway_network\", \"ccn\", \"feedback\", \"receptive_field\", \"dropout\", \"movement\", \"sparse_refire\", \"spike\", \"teacher\", \"cue_invariant\", \"floating_gate\", \"trajectory\", \"decode\", \"cortical\", \"debt\", \"spatial_phase\", \"voltage\", \"cell\", \"network\", \"channel\", \"response\", \"activity\", \"frequency\", \"system\", \"layer\", \"unit\", \"dynamic\", \"robot\", \"input\", \"filter\", \"image\", \"information\", \"figure\", \"deep\", \"neural\", \"model\", \"state\", \"produce\", \"effect\", \"time\", \"output\", \"show\", \"use\", \"train\", \"result\", \"signal\", \"change\", \"high\", \"noise\", \"learn\", \"datum\", \"performance\", \"value\", \"process\", \"training\", \"feature\", \"function\", \"give\", \"also\", \"buyer\", \"seller\", \"sy\", \"right_motor\", \"timestep\", \"semantic_correlation\", \"nonparanormal\", \"volume_conduction\", \"bilingual\", \"ibp\", \"permutation_complexity\", \"word_representation\", \"crystal\", \"inactive\", \"bag\", \"eeg_source\", \"price\", \"revenue\", \"stick_breake\", \"language\", \"surplus\", \"vjk\", \"level_alignment\", \"lattice\", \"tag\", \"document\", \"auction\", \"price_auction\", \"bcis\", \"calibrator\", \"permutation\", \"metric\", \"sssl\", \"monotonic\", \"word\", \"semantic\", \"sentence\", \"feature\", \"dictionary\", \"ensemble\", \"principal_component\", \"pair\", \"miss\", \"category\", \"validation\", \"cue\", \"hierarchical\", \"learn\", \"object\", \"set\", \"dataset\", \"model\", \"factor\", \"train\", \"datum\", \"correlation\", \"use\", \"training\", \"test\", \"consider\", \"sample\", \"task\", \"time\", \"different\", \"class\", \"method\", \"base\", \"result\", \"classification\", \"show\", \"give\", \"approach\", \"experiment\", \"well\", \"also\", \"distribution\", \"number\", \"function\", \"coreset\", \"apid\", \"learnedge\", \"thompson_sample\", \"demonstration\", \"tournament\", \"knn_graph\", \"order_optimality\", \"agent\", \"feasible_region\", \"bellman_error\", \"mistake_bound\", \"lp\", \"likelihood_ratio\", \"learnhull\", \"questionable\", \"wer\", \"reward_distribution\", \"api\", \"unweighte\", \"shortest_path\", \"crammer\", \"dagger\", \"apid_lspi\", \"belief_state\", \"mistake\", \"cand\", \"hyperplane\", \"infeasible\", \"fu\", \"policy\", \"expert\", \"eigenfunction\", \"reward\", \"day\", \"kxk\", \"proof\", \"unknown\", \"constraint\", \"optimal\", \"action\", \"density\", \"know\", \"cycle\", \"point\", \"learner\", \"depth\", \"problem\", \"assumption\", \"space\", \"function\", \"objective\", \"case\", \"result\", \"show\", \"let\", \"solution\", \"sample\", \"set\", \"theorem\", \"graph\", \"state\", \"give\", \"learn\", \"estimate\", \"use\", \"distribution\", \"compute\", \"number\", \"first\", \"define\", \"follow\", \"lars_td\", \"weibull\", \"lcp\", \"player\", \"blcp\", \"gender\", \"best_arm\", \"wei_bull\", \"image_identification\", \"investor\", \"portfolio\", \"communication_round\", \"larq\", \"lc_td\", \"asset_allocation\", \"inducer\", \"policy_iteration\", \"qmax\", \"warm_start\", \"homotopy_path\", \"female\", \"temporal_binde\", \"sajda\", \"tdsi\", \"linear_complementarity\", \"dof\", \"contour\", \"actor_critic\", \"illusory\", \"male\", \"arm\", \"policy\", \"contrast\", \"eeg\", \"reward\", \"reinforcement\", \"action\", \"natural_image\", \"coordinate\", \"direction\", \"value\", \"figure\", \"fixed_point\", \"state\", \"rat\", \"use\", \"model\", \"round\", \"learn\", \"image\", \"probability\", \"problem\", \"function\", \"show\", \"system\", \"set\", \"parameter\", \"time\", \"bind\", \"optimal\", \"step\", \"result\", \"single\", \"recollection\", \"analogy\", \"animation\", \"risk_prediction\", \"disentangle\", \"processing_stage\", \"mortality\", \"ldeep\", \"complication\", \"ladd\", \"analogy_make\", \"coronary_artery\", \"interference\", \"lmul\", \"patient\", \"late_visual\", \"populational\", \"response_latency\", \"item\", \"preoperative\", \"reilly\", \"recollect\", \"renal_failure\", \"surgery\", \"rich_recollection\", \"slot\", \"lure\", \"sprite\", \"bypass_operation\", \"stroke\", \"cal\", \"character\", \"latency\", \"rotation\", \"mlp\", \"bootstrap\", \"query\", \"deep\", \"risk\", \"visual\", \"pattern\", \"component\", \"representation\", \"early\", \"feature\", \"model\", \"use\", \"layer\", \"study\", \"task\", \"image\", \"network\", \"variability\", \"memory\", \"response\", \"train\", \"figure\", \"learn\", \"input\", \"show\", \"unit\", \"training\", \"high\", \"output\", \"board\", \"dsp\", \"minimal_weight\", \"chip\", \"sense_amplifi\", \"instruction\", \"follower\", \"analog_vlsi\", \"anna_chip\", \"interconnect\", \"bus\", \"anna\", \"sacking\", \"sgn\", \"aggregator\", \"unity\", \"inverter\", \"hardware\", \"lisp\", \"lisp_interpret\", \"rbfs\", \"sequencer\", \"radial_basis\", \"amplifier\", \"interconnects_architecture\", \"photodiode\", \"pes\", \"cyclic_line\", \"rail\", \"pd_array\", \"basis_function\", \"transistor\", \"minimal\", \"threshold\", \"weight\", \"analog\", \"character\", \"row\", \"circuit\", \"root\", \"function\", \"processor\", \"store\", \"implement\", \"parallel\", \"processing\", \"basis\", \"voltage\", \"application\", \"program\", \"input\", \"size\", \"vector\", \"linear\", \"bug\", \"crash\", \"assertion\", \"helimination\", \"determinism\", \"ccrypt\", \"eof\", \"debug\", \"utility\", \"pointer\", \"imbalance\", \"toss\", \"buggy\", \"abnormally\", \"false_positive\", \"tz\", \"examplei\", \"gun\", \"knob\", \"countdown\", \"counterexamplei\", \"storagec\", \"liblit\", \"debugging\", \"predicate\", \"heap\", \"smoking\", \"uv\", \"qf\", \"pinpoint\", \"program\", \"counter\", \"user\", \"file\", \"od\", \"deterministic\", \"successful\", \"subgradient\", \"feature_selection\", \"false\", \"software\", \"run\", \"array\", \"feature\", \"classification\", \"count\", \"cross_validation\", \"code\", \"score\", \"function\"], \"Freq\": [864.0, 970.0, 731.0, 305.0, 1538.0, 1335.0, 567.0, 522.0, 358.0, 630.0, 1092.0, 146.0, 264.0, 1020.0, 635.0, 616.0, 100.0, 358.0, 212.0, 480.0, 850.0, 237.0, 309.0, 427.0, 139.0, 101.0, 237.0, 100.0, 213.0, 211.0, 175.40287360132245, 68.97886332184987, 48.65625140311848, 71.15549573366594, 45.068802160597905, 36.70321332591714, 36.58433732099291, 34.88111007833785, 59.3008668588396, 29.592871550524695, 28.649518496594098, 37.33740730114439, 29.7089965764004, 26.72439499641842, 25.831540767954174, 26.089645207043514, 23.896950368324838, 53.81810742519007, 22.913462315905036, 31.297567587245055, 22.96621850929961, 22.156102333415294, 35.0396194444339, 22.103982216139165, 23.624559298436417, 21.239336057306016, 20.273900928664204, 189.5170695823732, 18.436244917696758, 18.437912360139737, 214.9325363068955, 70.05208626415248, 133.23717501557516, 95.44964654299343, 124.46298452694225, 184.4565683770043, 156.31885638949151, 62.7691391140455, 162.3980906851996, 291.1842979801105, 437.1866659028292, 327.05350114144176, 159.43186508342802, 289.1191574719068, 602.1795618938127, 192.726360536459, 232.48051597430742, 127.50479282659862, 570.5792682248224, 235.1052108380103, 378.310400921817, 146.48302476093974, 360.63671495576733, 289.55263632370264, 209.2344982152163, 215.39433158198298, 624.1518863861806, 233.13664153305066, 444.43855969644085, 295.2737417057844, 272.35711907477986, 251.1033157972405, 416.6410139934935, 464.5179537651136, 560.4197079633212, 296.72824914028376, 254.94153955436883, 344.6709074135795, 243.08516096857943, 241.8491266185469, 228.89583978287735, 220.2276272004641, 232.1222907463162, 237.42245419643817, 86.41947759812841, 160.71922182513816, 111.5011907017308, 65.50889859206583, 46.822440781362126, 39.508306834132725, 39.3128085454846, 44.301886982209744, 50.8429920468381, 37.24645916448646, 32.228558930934994, 39.459856480930405, 33.145270226292254, 28.075155774461567, 27.303102360226283, 75.73244064920426, 31.325714341831855, 30.28266891516863, 50.85478235419147, 26.988775714546364, 52.79134425767559, 36.918971537254336, 23.668298648541416, 29.77615931866248, 65.05111850208854, 28.42952994455964, 67.56112720591716, 21.954403866452136, 20.2442164914636, 39.20476679845234, 204.86378865679643, 614.9152340140391, 55.89176538368279, 221.67127797280097, 122.49495128887953, 79.41116082495091, 312.404791079222, 256.12886053335745, 159.0401239756848, 112.42897089696837, 79.96572243227209, 343.97972974712036, 113.17376451838425, 297.2304679125158, 193.7987279188744, 332.68030567707706, 103.75705085223737, 165.7390134008908, 630.8003505542999, 201.4487836876624, 85.13774353105968, 108.63554693300335, 291.14886490613736, 170.8693496939518, 315.48426002296776, 405.52027874577254, 156.50531140099943, 295.3392511735455, 134.37818847099996, 116.2669590234438, 148.51541625441865, 122.51951678161473, 270.645837210601, 236.89197195725447, 151.95233946451714, 179.79509225325162, 135.79454344565227, 154.86341308472268, 184.10540780822654, 173.8022286391695, 157.475284523749, 151.20916430508433, 39.18510655087225, 26.816470374562428, 24.101376004185244, 19.517021819485578, 20.144210172610716, 17.37483801979, 18.537081053192164, 14.440588837907685, 14.391375563886486, 12.95761415426558, 17.09285001310475, 12.22266859759322, 14.068834500107947, 13.685698118881866, 31.52904632240511, 10.803451851995929, 42.79370288950316, 10.098813972095043, 10.072643749024046, 69.2658976604904, 9.369265831276314, 9.346225092183271, 9.333318764268283, 74.670119621604, 20.203090092044242, 91.91604386234056, 8.647616535760227, 8.643754193061998, 8.62877412927292, 9.158274958028368, 55.3155905358272, 71.04596602551261, 15.822519951200293, 31.479057805615675, 123.7315200396577, 40.165861090886594, 24.97458449027315, 437.64139148633194, 36.84013742961483, 43.212599077307175, 30.809969353291, 76.25196627110167, 51.002062711435855, 38.98887919745923, 31.566042201945525, 58.89758202224071, 36.73880718897727, 249.90295563296772, 73.60525971804523, 209.9139916871457, 79.85728593751533, 245.8773896541676, 62.91893733853025, 90.85451303309472, 167.93495643668066, 54.25897933862155, 173.05493710099535, 100.28762388236893, 73.51370400948754, 82.77111371446887, 96.48079041583239, 65.55335714139105, 98.44260774348265, 80.484539022641, 79.22074843402807, 90.41720829733075, 79.58207629602832, 95.2495754358666, 65.51497414755359, 85.21508580682101, 77.05588223309952, 70.45584444888085, 67.48743218310032, 68.58515128311808, 69.99926807181654, 69.14340005567986, 70.13830007904791, 70.10730269822318, 43.22162490112939, 18.073050862635924, 17.52934742794213, 13.903786847257555, 18.55807335708594, 17.985307833020666, 12.111768164581846, 9.006650441456237, 15.438337774659129, 7.406964875744018, 7.4020142902341375, 8.196402460577369, 15.378805907688578, 7.60390705956482, 6.3417316071075645, 6.338525206144716, 7.095452511177532, 5.848336524598484, 5.797862794158693, 5.756233033615646, 6.4964287945773895, 5.725335998962787, 5.264105014006631, 5.262913286103456, 5.818047550991948, 14.330601935729952, 6.339461512534628, 6.293668596568237, 5.808043780881186, 4.660841730772012, 45.51654414923123, 25.923194305652896, 16.518084868553462, 22.736759269408623, 19.567830651489142, 8.509026195717015, 54.40470196777041, 26.277105206468782, 33.096896919361456, 45.387441488847486, 23.474936052453728, 31.56562485801497, 37.80940087983371, 15.24518601739611, 62.67313831742117, 14.095449124010646, 21.444871694178154, 62.12058983661639, 36.22400241691254, 38.660855669058805, 73.49128404941561, 31.322013002359217, 45.85508097087237, 58.92396950879266, 57.60282612562728, 37.05958950336422, 33.906961943819304, 43.17360565663587, 53.2069243056066, 31.63047609324208, 29.890869044116627, 36.52023110430055, 42.07477082695823, 49.32976348969134, 35.0607669270368, 48.20205167949172, 37.15644591517463, 34.08028983475798, 38.73074645683973, 36.485944353525745, 34.352508899899235, 34.697962191730994, 13.231909144071166, 15.647332625913466, 10.477778142201377, 50.5526071048185, 9.55253006650037, 13.250800468800287, 19.215428984804436, 9.805921820546212, 8.895762945589103, 7.754611905113388, 7.7522281900760195, 7.740079780105957, 7.723122898135581, 7.262981075943827, 6.835587427006507, 6.816982301305058, 14.596349601473658, 7.29446570511624, 8.567495944202102, 5.427644717397702, 5.897870814315958, 4.987738206191217, 4.987865162547661, 4.985257084983818, 4.9692770008421165, 5.887201008117927, 11.852695782364536, 4.544542047843515, 4.527078434898712, 6.296262049739613, 36.66185247672497, 36.583485382355924, 34.96110791445663, 20.487978212522567, 17.80253716619702, 13.159153301766674, 22.55134515123784, 13.824081746384856, 21.68558378969024, 25.676491876947395, 63.5699764232697, 46.24964029342224, 10.734991720356717, 32.37351629095752, 12.103395186092687, 50.674791653382265, 50.575476760199976, 14.404388307176436, 41.19756531192605, 30.817243684516946, 25.311490163268484, 30.682304399935962, 32.425369206954066, 30.666353865980675, 24.15033638271909, 29.70418228513012, 22.724345075919626, 24.69432904776232, 17.842003081829493, 18.118639888069403, 19.094093730487227, 21.225277252570514, 17.622399229819308, 12.947356797595475, 26.148468739587354, 9.8176855021938, 7.337587181323815, 12.893362824101482, 6.593177536562086, 5.932617287609267, 6.100662970428749, 9.443012415493415, 5.758729774722018, 5.063491144261192, 4.528340525470273, 5.556479187597548, 4.390825173363687, 12.609793259539702, 3.79311340972215, 3.792784741698061, 3.793171333393722, 8.347352975490631, 3.4740639692530255, 3.467091000520176, 3.466898680914001, 3.1239173270829226, 3.123638447383275, 3.1165994196317226, 3.1163133808265995, 3.1163465730428936, 3.0310214252644356, 2.7736053746591196, 2.7731221350395434, 5.8751283689878315, 7.4775470425362975, 5.496137690858129, 10.15832420146506, 16.428728261055372, 8.745478640900016, 5.743806944772839, 25.501571968847852, 10.473417259917479, 17.589339010895838, 17.186492240839442, 16.268646757663173, 19.235756245039372, 10.901146477294605, 29.574894719352827, 36.359365506695696, 32.641964843996, 18.544152813090523, 13.859576525523776, 15.291259645077394, 19.022597133654003, 20.266123341452747, 9.728420568506783, 11.895410855624293, 13.581757675141896, 13.91309018812316, 15.178566213308883, 16.26763016468569, 14.428247703219029, 15.169336173868034, 11.855599720904449, 12.415418339265727, 11.859119397487168, 10.676984471621616, 5.925915921447613, 3.1452359769918097, 2.4250776337028506, 13.76542991456146, 2.061590156138315, 2.603381107183601, 1.5140707175557901, 1.8466150293534904, 1.4830271576450162, 2.369016015235384, 1.3207219020726348, 1.1452334016474175, 1.1448713111190485, 1.021004506830228, 1.013876869995169, 1.0135199902355165, 1.0134505133192961, 2.6809889069093624, 0.9779578832834833, 0.9766474328487047, 1.1886210410579443, 1.2689155131339518, 3.093257980070062, 1.1523342898697315, 1.251030070591314, 1.249163689031911, 1.2451314804041953, 1.2443854141188477, 0.8399179038805455, 1.1164445992577188, 2.018775701286607, 1.6997976837398385, 3.5086264184633573, 7.7464070513279015, 13.307829932643548, 3.2546560987063247, 2.17561729216741, 5.466988930429888, 3.916840355613988, 2.9297505422573455, 10.677080015442934, 2.7239034085204277, 2.778153300602585, 3.4701130479051385, 3.5869875795506694, 3.24870516924232, 2.8391978035013845, 2.3844500805840427, 2.943245970053732, 2.2436889219624696, 3.0140364631702194, 2.5734305349598356, 2.5248323584224934, 2.085356356459575, 2.399378053391851, 2.5852595698213556, 0.9104548365173429, 0.3979357313053927, 0.46466138210285707, 0.32889753944105893, 0.3286332941930179, 0.39424216858096817, 1.5221158521581537, 0.3747472614118104, 0.26047376447755366, 0.26041747919283925, 0.2602825340937464, 0.2602365944631152, 0.9823186235718693, 0.20600642056188995, 0.19237322962287326, 0.1922515655519388, 0.1922396187847237, 0.19216902611727918, 0.19216424330497486, 0.19215112649354799, 0.19209287060805583, 0.1920773110727482, 0.19205867247372535, 0.19197438823629376, 0.1919250205813929, 0.34802595591973856, 0.20606586701868532, 0.2560074388689721, 1.9227273308862645, 0.6419083417370156, 0.7768626369668054, 0.3173383237122557, 0.27708482057102674, 0.5765165823951719, 0.5299298146793601, 0.35355443534752606, 0.4465031170195722, 0.3632084496118934, 0.35852868330657206, 0.7364090767924522, 0.46237864637275106, 0.6012910574813779, 0.44078443356254926, 0.33828782163467114, 0.3274147239370783, 0.35248049897783656, 0.33190182296219206, 0.3426571772633144], \"Total\": [864.0, 970.0, 731.0, 305.0, 1538.0, 1335.0, 567.0, 522.0, 358.0, 630.0, 1092.0, 146.0, 264.0, 1020.0, 635.0, 616.0, 100.0, 358.0, 212.0, 480.0, 850.0, 237.0, 309.0, 427.0, 139.0, 101.0, 237.0, 100.0, 213.0, 211.0, 183.12284526164206, 73.07205142659697, 51.94656199673251, 76.59719395620256, 48.892226124142546, 39.89841759259874, 39.89203998246405, 38.08252424106076, 65.1707152284546, 32.62978525464192, 31.676563684449853, 41.339555514692705, 32.95434177755409, 29.753608819658943, 28.86605283296157, 29.29642549893832, 26.98892630173779, 60.959758697578266, 25.98839184741688, 35.50206137893282, 26.059503960087383, 25.157523361557637, 39.806298556647846, 25.153334732680598, 26.8843556722326, 24.23739061449859, 23.28177114672022, 218.16154213059542, 21.423926929395225, 21.42647457437425, 259.4286849517611, 82.70120543775627, 160.55021807775714, 114.14325815805209, 150.9031188297534, 228.7396595167367, 195.22878042426657, 75.13425238003657, 209.94158654541678, 393.4875814968294, 648.7602363381363, 477.30404142927756, 215.01863642855258, 421.30622486341224, 970.1202307497473, 275.13995710149504, 344.3367718819993, 173.36935663854592, 1020.534055870458, 359.7895505449741, 635.9509625530147, 207.9341591747369, 616.1625960055428, 480.5698505161074, 322.13210641936485, 340.8492320756235, 1335.997820358537, 382.06337776425033, 894.8665145864737, 524.414045607943, 479.95302215221955, 430.26216162529244, 894.1340827697728, 1092.6315631878188, 1538.8397625608175, 593.5390422339051, 477.2954599866658, 850.6794635965139, 460.6973950940713, 507.02452902499044, 483.82165142839864, 421.96470557933793, 567.9941566657027, 687.7294346228846, 89.7523551250628, 169.0765747572017, 117.3381609054364, 69.79553741140973, 50.63152427264411, 42.74964437642781, 42.57680203596409, 48.13452363375946, 55.30911589728667, 40.70729865009455, 35.3533564384749, 43.3225253114952, 36.546480624279845, 31.181276963520993, 30.351930327571512, 84.42692013005302, 35.05045243498161, 33.886665467226265, 56.95472166364469, 30.239544780097244, 59.217954827454136, 41.55086254308631, 26.792077525019103, 33.72594846230102, 73.7451590012826, 32.35201299749457, 76.88856170436756, 25.01401668849399, 23.259664542972217, 45.06776308381187, 237.3421361647659, 731.5588457887056, 64.51621892132273, 264.4706961853661, 143.9929623227868, 96.12213926416484, 427.020462446856, 358.03804532294413, 213.5632736720682, 149.68022764477783, 102.61275625832305, 616.6878561769837, 160.74004370017434, 522.0579703422288, 313.8599812724354, 630.2763893600697, 146.43653410341946, 270.3237022164286, 1538.8397625608175, 358.15895051250305, 115.64941601721637, 162.47501453159296, 687.7294346228846, 331.5126176961076, 850.6794635965139, 1335.997820358537, 309.88553192305943, 894.1340827697728, 242.8970330868983, 195.72438314332777, 311.75258970319027, 216.81514439227283, 1092.6315631878188, 894.8665145864737, 358.120816857548, 567.9941566657027, 292.2963604660856, 463.4248804501736, 864.684755661501, 970.1202307497473, 593.5390422339051, 507.02452902499044, 43.56463452446143, 30.055006370545627, 27.957406782592034, 22.744405831552868, 23.50915148494364, 20.552073832366563, 22.069596883099045, 17.630700851577057, 17.594101181117665, 16.156788624898315, 21.497303330117653, 15.40851291742272, 17.738649284136823, 17.377319331131552, 40.175127173840494, 13.9742026517617, 55.483065582692454, 13.246530772585272, 13.239113444686595, 92.08754415673333, 12.514276937558552, 12.507291964927758, 12.491866437468495, 99.98194769532158, 27.051617703688965, 125.15188084717279, 11.784592438313844, 11.784833280221461, 11.783215648072865, 12.570610568106314, 78.2363090688602, 102.62334618678132, 21.87811212046691, 44.4648101083745, 193.3535227165723, 58.75461399391025, 35.63459495210228, 864.684755661501, 57.155219079604976, 72.19257999480793, 48.5331816227071, 149.73167209954568, 92.64198215614938, 65.8939775683383, 51.779734830038755, 121.98678755409374, 64.00271047186429, 1092.6315631878188, 193.9831526060783, 1020.534055870458, 229.562899673848, 1538.8397625608175, 163.62626658274283, 309.88553192305943, 894.8665145864737, 137.71117578511888, 1335.997820358537, 463.4248804501736, 256.72425319418477, 357.8780255195087, 524.414045607943, 231.26314063448243, 687.7294346228846, 418.99177750095424, 421.96470557933793, 648.7602363381363, 460.6973950940713, 894.1340827697728, 244.51879732971037, 850.6794635965139, 593.5390422339051, 378.52453380843286, 287.4467402779372, 385.68567800612436, 507.02452902499044, 438.71725965812067, 616.1625960055428, 970.1202307497473, 49.15773229010317, 21.642957284883913, 21.10697134504746, 17.258375169041805, 23.148702132886562, 22.57240708029154, 15.686751858708504, 12.43505973732352, 22.10795503608778, 10.796962958880046, 10.805084053077096, 12.080419890604974, 22.88580212401336, 11.563327694675941, 9.714915051883109, 9.716284386101256, 11.00057814365919, 9.151593201418368, 9.178941280694504, 9.181238266517344, 10.469051653100102, 9.236407702737381, 8.63917318294942, 8.639168469790063, 9.626926365688544, 23.925754895724854, 10.642689435836752, 10.671019230012138, 10.088678863490102, 8.15018360675829, 101.21052627638524, 54.5040121368622, 33.153513557712515, 49.53864431495313, 47.709396757665395, 15.981121981213224, 212.9192045225809, 81.29984753002317, 123.82568970705773, 211.16210564192764, 77.4613897513796, 131.93021972335723, 189.80988457368252, 41.91639074923044, 480.5698505161074, 37.985431989606866, 82.13087966215951, 635.9509625530147, 231.4169736043727, 270.1945786072372, 970.1202307497473, 185.11962800392584, 477.2954599866658, 894.1340827697728, 850.6794635965139, 313.47085963617207, 246.16904268465697, 524.414045607943, 1020.534055870458, 207.9341591747369, 184.24321727600582, 358.15895051250305, 593.5390422339051, 1092.6315631878188, 315.7253840757479, 1335.997820358537, 438.71725965812067, 306.57630125447685, 616.1625960055428, 483.82165142839864, 430.26216162529244, 479.95302215221955, 16.801975424529033, 19.924577563049322, 14.013613534393883, 69.0114057413098, 13.08927093123036, 18.275240453680468, 26.646215925772413, 13.754884418401291, 12.810123342253826, 11.186802036922137, 11.187913278640847, 11.21770282005133, 11.22272621266555, 10.761556792945246, 10.257671546772684, 10.262609506662622, 23.10274393623317, 11.845255448552633, 14.038709109026692, 8.89786313496402, 9.738632410613734, 8.402690054983715, 8.404605618114553, 8.405347299734602, 8.433990909280531, 10.182678284294427, 20.61414267993804, 7.941968061653093, 7.938451846572806, 11.496758010907369, 81.69546183920409, 101.21052627638524, 116.86256648602179, 58.89589035531505, 49.53864431495313, 32.47086719487398, 77.4613897513796, 38.348427916380096, 79.60703524088257, 118.25266172140051, 567.9941566657027, 630.2763893600697, 28.426513124442916, 358.15895051250305, 37.346240861690546, 1335.997820358537, 1538.8397625608175, 58.43917854763321, 1092.6315631878188, 522.0579703422288, 347.4933706764046, 635.9509625530147, 970.1202307497473, 850.6794635965139, 427.020462446856, 1020.534055870458, 446.54710020303446, 687.7294346228846, 173.36935663854592, 211.16210564192764, 344.3367718819993, 894.1340827697728, 227.70961174146004, 16.546241663794458, 34.25332217346969, 13.869983362001998, 10.84276529223894, 19.821628151401157, 10.17728464709815, 9.423906691578354, 9.892241597780622, 15.413745150737638, 9.533261936001734, 8.816319557296099, 8.005976661410124, 9.994303063087806, 8.090476214051543, 23.551573579562717, 7.336976837273085, 7.336393473121913, 7.337687443501038, 16.251214660844994, 6.942129574792418, 6.955757070492057, 6.9558840093224, 6.586795595788227, 6.588048760494078, 6.6002133003630465, 6.601220491237148, 6.601358046886865, 6.6486471904209505, 6.232244193507408, 6.232514936562129, 13.240115787787188, 16.94985019054754, 12.506229361542017, 24.136558140183645, 41.524724200481536, 28.524817525465412, 16.094326522051546, 146.43653410341946, 46.5287743753417, 139.40442168792038, 139.45615560458032, 149.38222375161897, 237.04417776243938, 70.69574985813064, 864.684755661501, 1538.8397625608175, 1335.997820358537, 358.03804532294413, 170.22163871834127, 231.26314063448243, 522.0579703422288, 731.5588457887056, 62.96675612557308, 144.5827606990703, 264.4706961853661, 309.88553192305943, 630.2763893600697, 1092.6315631878188, 616.6878561769837, 850.6794635965139, 213.5632736720682, 463.4248804501736, 311.75258970319027, 331.5126176961076, 12.033944757324608, 7.99846634818979, 6.1766167276671275, 35.12663270277443, 5.842965847597228, 7.913784118408714, 5.389816062264705, 6.703772712578571, 5.415094670401251, 9.419327173837281, 5.767383884657995, 5.02525529310642, 5.024610718982501, 4.72207677864772, 4.739179770880262, 4.7400336927251905, 4.740648757800053, 13.046865547021328, 4.823974977237757, 4.825866048204772, 5.885213066484267, 6.286202581342941, 15.432989445235908, 5.949609059850951, 6.499172869600261, 6.510440485449947, 6.527703982250347, 6.530450593803095, 4.553013823583642, 6.1474547701073305, 11.321258399270347, 9.954541594169584, 28.739738499658937, 100.99907816312647, 305.81559376892716, 33.04979616868401, 16.94985019054754, 100.34628968552067, 56.777259248370044, 39.33933551079613, 970.1202307497473, 41.42472550496727, 47.21406490278588, 86.67548719055758, 123.65354807095699, 100.18949581614655, 68.51099076001499, 45.06776308381187, 191.47815750620202, 56.93266755671038, 616.6878561769837, 322.13210641936485, 387.1063162197228, 331.13848248821296, 8.716238806401297, 9.92299833262297, 9.059293930004305, 4.367385738236178, 5.3762121041724615, 4.270886618180956, 4.273831894541617, 5.402029193216329, 22.399257316813685, 5.576772631659386, 4.166167375449389, 4.166859993191064, 4.168295151320427, 4.168986688418954, 16.657113374747823, 3.897435796180522, 4.057873640324295, 4.059350605086399, 4.059407495450564, 4.060136710632681, 4.06027641320784, 4.060690537170293, 4.0608736335134905, 4.061263033533947, 4.061558920569644, 4.062558017632228, 4.063124247059236, 7.48894628262034, 4.449436055676832, 5.987510209353283, 56.93266755671038, 17.98255938999494, 24.990763611058114, 8.360825752376531, 7.63079205383441, 27.459857645692978, 28.766038596884847, 14.074699090367128, 25.250240342984128, 16.155878690636317, 20.91688775581094, 192.79058084334102, 52.982431741722834, 864.684755661501, 244.51879732971037, 38.827049202124606, 35.98546604312479, 84.37198245744757, 67.08421195442004, 970.1202307497473], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.974, -6.9073, -7.2563, -6.8762, -7.3329, -7.5382, -7.5414, -7.5891, -7.0584, -7.7535, -7.7859, -7.5211, -7.7496, -7.8555, -7.8895, -7.8795, -7.9673, -7.1554, -8.0093, -7.6975, -8.007, -8.0429, -7.5846, -8.0453, -7.9788, -8.0852, -8.1317, -5.8966, -8.2267, -8.2266, -5.7707, -6.8918, -6.2489, -6.5825, -6.317, -5.9236, -6.0892, -7.0016, -6.051, -5.4671, -5.0607, -5.3509, -6.0694, -5.4742, -4.7405, -5.8798, -5.6923, -6.2929, -4.7944, -5.681, -5.2053, -6.1541, -5.2532, -5.4727, -5.7976, -5.7686, -4.7047, -5.6894, -5.0442, -5.4532, -5.5339, -5.6152, -5.1088, -5.0001, -4.8124, -5.4482, -5.6, -5.2985, -5.6476, -5.6527, -5.7078, -5.7464, -5.6938, -5.6712, -6.4354, -5.815, -6.1806, -6.7124, -7.0483, -7.2181, -7.2231, -7.1036, -6.9659, -7.2771, -7.4218, -7.2193, -7.3937, -7.5597, -7.5876, -6.5674, -7.4502, -7.484, -6.9656, -7.5992, -6.9283, -7.2859, -7.7305, -7.5009, -6.7194, -7.5472, -6.6816, -7.8057, -7.8867, -7.2258, -5.5723, -4.4731, -6.8712, -5.4934, -6.0865, -6.52, -5.1503, -5.3489, -5.8255, -6.1723, -6.513, -5.054, -6.1657, -5.2001, -5.6278, -5.0874, -6.2526, -5.7842, -4.4476, -5.5891, -6.4503, -6.2066, -5.2208, -5.7537, -5.1405, -4.8894, -5.8415, -5.2065, -5.994, -6.1387, -5.8939, -6.0863, -5.2938, -5.427, -5.8711, -5.7028, -5.9835, -5.8521, -5.6791, -5.7367, -5.8353, -5.876, -6.5158, -6.8951, -7.0018, -7.2128, -7.1812, -7.3291, -7.2643, -7.514, -7.5175, -7.6224, -7.3454, -7.6808, -7.5401, -7.5677, -6.7332, -7.8042, -6.4277, -7.8717, -7.8743, -5.9461, -7.9466, -7.9491, -7.9505, -5.871, -7.1782, -5.6632, -8.0268, -8.0272, -8.029, -7.9694, -6.171, -5.9208, -7.4226, -6.7348, -5.366, -6.4911, -6.9662, -4.1027, -6.5775, -6.4179, -6.7562, -5.85, -6.2522, -6.5208, -6.732, -6.1083, -6.5802, -4.663, -5.8854, -4.8374, -5.8038, -4.6792, -6.0422, -5.6748, -5.0605, -6.1903, -5.0305, -5.576, -5.8866, -5.768, -5.6147, -6.0012, -5.5946, -5.796, -5.8118, -5.6796, -5.8073, -5.6276, -6.0018, -5.7389, -5.8395, -5.9291, -5.9721, -5.956, -5.9356, -5.9479, -5.9336, -5.9341, -5.6531, -6.5251, -6.5556, -6.7873, -6.4986, -6.5299, -6.9253, -7.2215, -6.6826, -7.4171, -7.4177, -7.3158, -6.6865, -7.3908, -7.5723, -7.5728, -7.46, -7.6533, -7.662, -7.6692, -7.5482, -7.6746, -7.7586, -7.7588, -7.6585, -6.7571, -7.5727, -7.5799, -7.6602, -7.8803, -5.6014, -6.1643, -6.615, -6.2955, -6.4456, -7.2784, -5.423, -6.1508, -5.92, -5.6042, -6.2635, -5.9674, -5.7869, -6.6952, -5.2815, -6.7736, -6.354, -5.2904, -5.8298, -5.7647, -5.1223, -5.9752, -5.594, -5.3432, -5.3659, -5.807, -5.8959, -5.6542, -5.4453, -5.9654, -6.0219, -5.8216, -5.68, -5.521, -5.8624, -5.5441, -5.8043, -5.8908, -5.7628, -5.8226, -5.8828, -5.8728, -6.566, -6.3983, -6.7993, -5.2256, -6.8918, -6.5645, -6.1929, -6.8656, -6.963, -7.1003, -7.1006, -7.1022, -7.1044, -7.1658, -7.2265, -7.2292, -6.4678, -7.1615, -7.0006, -7.4571, -7.374, -7.5416, -7.5416, -7.5421, -7.5453, -7.3758, -6.676, -7.6347, -7.6385, -7.3086, -5.5469, -5.549, -5.5944, -6.1288, -6.2693, -6.5715, -6.0328, -6.5222, -6.072, -5.903, -4.9965, -5.3145, -6.7751, -5.6713, -6.6551, -5.2232, -5.2251, -6.4811, -5.4302, -5.7205, -5.9173, -5.7249, -5.6697, -5.7254, -5.9643, -5.7573, -6.0252, -5.942, -6.267, -6.2517, -6.1992, -6.0934, -6.2794, -6.0679, -5.365, -6.3447, -6.6358, -6.0721, -6.7428, -6.8484, -6.8204, -6.3836, -6.8781, -7.0068, -7.1185, -6.9139, -7.1493, -6.0944, -7.2956, -7.2957, -7.2956, -6.5069, -7.3835, -7.3855, -7.3856, -7.4897, -7.4898, -7.4921, -7.4922, -7.4922, -7.5199, -7.6087, -7.6089, -6.8581, -6.6169, -6.9248, -6.3105, -5.8298, -6.4603, -6.8807, -5.3901, -6.28, -5.7615, -5.7847, -5.8396, -5.6721, -6.24, -5.2419, -5.0354, -5.1432, -5.7087, -5.9999, -5.9016, -5.6832, -5.6199, -6.3538, -6.1527, -6.0201, -5.996, -5.909, -5.8397, -5.9596, -5.9096, -6.156, -6.1099, -6.1557, -6.2607, -5.8162, -6.4497, -6.7097, -4.9734, -6.8721, -6.6387, -7.1808, -6.9822, -7.2015, -6.7331, -7.3174, -7.46, -7.4603, -7.5748, -7.5818, -7.5821, -7.5822, -6.6094, -7.6178, -7.6192, -7.4228, -7.3574, -6.4663, -7.4538, -7.3716, -7.3731, -7.3763, -7.3769, -7.77, -7.4854, -6.8931, -7.065, -6.3403, -5.5483, -5.0072, -6.4155, -6.8182, -5.8968, -6.2303, -6.5206, -5.2275, -6.5935, -6.5738, -6.3514, -6.3182, -6.4173, -6.552, -6.7266, -6.516, -6.7874, -6.4923, -6.6503, -6.6694, -6.8606, -5.6834, -5.6088, -6.6524, -7.4801, -7.3251, -7.6706, -7.6714, -7.4894, -6.1385, -7.5401, -7.9039, -7.9041, -7.9046, -7.9048, -6.5765, -8.1385, -8.2069, -8.2076, -8.2076, -8.208, -8.208, -8.2081, -8.2084, -8.2085, -8.2086, -8.209, -8.2093, -7.6141, -8.1382, -7.9212, -5.9049, -7.0019, -6.8111, -7.7064, -7.8421, -7.1094, -7.1936, -7.5983, -7.3649, -7.5714, -7.5844, -6.8646, -7.33, -7.0673, -7.3778, -7.6425, -7.6752, -7.6014, -7.6615, -7.6297], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9128, 0.8982, 0.8904, 0.8822, 0.8744, 0.8724, 0.8693, 0.8681, 0.8615, 0.8582, 0.8554, 0.8541, 0.8522, 0.8485, 0.8448, 0.8399, 0.8342, 0.8313, 0.83, 0.8298, 0.8295, 0.8288, 0.8283, 0.8266, 0.8266, 0.8238, 0.8175, 0.8151, 0.8057, 0.8057, 0.7677, 0.7899, 0.7694, 0.777, 0.7632, 0.7407, 0.7336, 0.7761, 0.6991, 0.6548, 0.5612, 0.5778, 0.6568, 0.5794, 0.479, 0.5999, 0.5631, 0.6486, 0.3744, 0.5304, 0.4365, 0.6056, 0.4202, 0.4492, 0.5244, 0.4969, 0.1948, 0.4619, 0.256, 0.3815, 0.3893, 0.4173, 0.1922, 0.1005, -0.0542, 0.2626, 0.3288, 0.0524, 0.3165, 0.2156, 0.2074, 0.3056, 0.061, -0.1077, 1.1645, 1.1516, 1.1513, 1.1389, 1.1241, 1.1235, 1.1226, 1.1193, 1.1181, 1.1135, 1.1098, 1.1089, 1.1046, 1.0974, 1.0965, 1.0936, 1.09, 1.0899, 1.089, 1.0886, 1.0874, 1.0841, 1.0783, 1.0778, 1.0769, 1.0731, 1.073, 1.0718, 1.0635, 1.0629, 1.0552, 1.0286, 1.0588, 1.0258, 1.0406, 1.0113, 0.8898, 0.8674, 0.9075, 0.9161, 0.953, 0.6185, 0.8515, 0.639, 0.7202, 0.5633, 0.8578, 0.7131, 0.3105, 0.6269, 0.896, 0.7998, 0.3428, 0.5395, 0.2104, 0.0101, 0.5192, 0.0946, 0.6103, 0.6815, 0.4608, 0.6315, -0.1932, -0.1268, 0.345, 0.052, 0.4357, 0.1062, -0.3445, -0.5172, -0.1245, -0.0076, 1.8069, 1.7988, 1.7644, 1.7598, 1.7584, 1.7449, 1.7384, 1.7133, 1.7119, 1.6922, 1.6836, 1.6812, 1.6811, 1.674, 1.6705, 1.6555, 1.6532, 1.6415, 1.6395, 1.6281, 1.6234, 1.6215, 1.6214, 1.6209, 1.6209, 1.6042, 1.6033, 1.6029, 1.6013, 1.5961, 1.5662, 1.5451, 1.5888, 1.5675, 1.4664, 1.5325, 1.5574, 1.2319, 1.4737, 1.3996, 1.4584, 1.2381, 1.316, 1.3881, 1.4179, 1.1847, 1.3578, 0.4376, 0.9438, 0.3315, 0.8569, 0.0789, 0.9571, 0.6859, 0.2398, 0.9815, -0.131, 0.3823, 0.6623, 0.4487, 0.2199, 0.6522, -0.0311, 0.2631, 0.2402, -0.0578, 0.1569, -0.3265, 0.5958, -0.388, -0.1287, 0.2316, 0.4638, 0.1859, -0.0672, 0.0652, -0.2602, -0.7145, 2.5488, 2.4972, 2.4917, 2.4613, 2.4564, 2.4503, 2.4188, 2.3549, 2.3184, 2.3006, 2.2992, 2.2896, 2.2799, 2.2583, 2.2509, 2.2503, 2.239, 2.2297, 2.218, 2.2106, 2.2003, 2.1992, 2.1821, 2.1818, 2.1739, 2.1649, 2.1594, 2.1495, 2.1253, 2.1186, 1.8783, 1.9343, 1.9808, 1.8987, 1.7862, 2.0472, 1.313, 1.548, 1.358, 1.1401, 1.4836, 1.2473, 1.064, 1.666, 0.6404, 1.6861, 1.3346, 0.3514, 0.823, 0.7331, 0.0972, 0.9008, 0.3348, -0.0422, -0.015, 0.5423, 0.6951, 0.1804, -0.2764, 0.7944, 0.8588, 0.3943, 0.0308, -0.4204, 0.4797, -0.6446, 0.2087, 0.4807, -0.0894, 0.0927, 0.1497, 0.0504, 2.7095, 2.7067, 2.6576, 2.6371, 2.6333, 2.6268, 2.6214, 2.6099, 2.5837, 2.5819, 2.5815, 2.5773, 2.5746, 2.5551, 2.5424, 2.5392, 2.4892, 2.4635, 2.4545, 2.454, 2.4468, 2.4268, 2.4266, 2.426, 2.4193, 2.4004, 2.3949, 2.3901, 2.3867, 2.3462, 2.1471, 1.9307, 1.7416, 1.8924, 1.9249, 2.0451, 1.7143, 1.928, 1.6479, 1.4211, 0.7584, 0.3362, 1.9745, 0.5447, 1.8216, -0.3237, -0.467, 1.5479, -0.3296, 0.1186, 0.3288, -0.0831, -0.4501, -0.3745, 0.0758, -0.5885, -0.0298, -0.3785, 0.6745, 0.4926, 0.0561, -0.7923, 0.3894, 3.2228, 3.1981, 3.1226, 3.0776, 3.038, 3.034, 3.0053, 2.9847, 2.9781, 2.964, 2.9135, 2.8983, 2.881, 2.8569, 2.8434, 2.8084, 2.8083, 2.8083, 2.8019, 2.7758, 2.7718, 2.7718, 2.7221, 2.7218, 2.7177, 2.7175, 2.7175, 2.6826, 2.6585, 2.6583, 2.6556, 2.6497, 2.6459, 2.6027, 2.5408, 2.2859, 2.4378, 1.7202, 1.9769, 1.398, 1.3745, 1.2508, 0.9566, 1.5986, 0.0927, -0.2772, -0.2437, 0.5076, 0.96, 0.7518, 0.1559, -0.1181, 1.6005, 0.9704, 0.4991, 0.3647, -0.2582, -0.7391, -0.2871, -0.5587, 0.577, -0.1516, 0.199, 0.0325, 3.793, 3.568, 3.5665, 3.5646, 3.4596, 3.3896, 3.2317, 3.2121, 3.2063, 3.1211, 3.0273, 3.0225, 3.0223, 2.9699, 2.9593, 2.9588, 2.9586, 2.919, 2.9055, 2.9038, 2.9017, 2.9012, 2.8941, 2.8598, 2.8537, 2.8504, 2.8446, 2.8435, 2.8111, 2.7955, 2.7772, 2.7339, 2.3983, 1.9335, 1.3667, 2.1834, 2.4484, 1.5915, 1.8275, 1.9041, -0.0079, 1.7796, 1.6685, 1.2834, 0.9612, 1.0726, 1.3179, 1.5622, 0.3261, 1.2676, -0.8197, -0.3283, -0.5312, -0.5662, 4.2483, 4.1933, 3.2407, 3.1427, 3.0899, 2.9745, 2.973, 2.9207, 2.8494, 2.8382, 2.7661, 2.7657, 2.7648, 2.7645, 2.7076, 2.5981, 2.4893, 2.4883, 2.4883, 2.4877, 2.4876, 2.4875, 2.4871, 2.487, 2.4868, 2.4861, 2.4857, 2.4694, 2.466, 2.3861, 2.1502, 2.2056, 2.0673, 2.267, 2.2227, 1.6748, 1.5441, 1.8542, 1.5032, 1.7432, 1.472, -0.0293, 0.797, -1.7327, -0.7802, 0.7953, 0.8387, 0.0603, 0.2294, -2.4101]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 8, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 8, 1, 2, 3, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 8, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 1, 2, 3, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 7, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 7, 1, 2, 3, 6, 1, 2, 3, 7, 1, 2, 3, 5, 1, 2, 3, 4, 6, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 5, 6, 1, 2, 3, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 6, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 7, 1, 2, 3, 4, 6, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 7, 1, 2, 3, 4, 5, 1, 2, 3, 7, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 6, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 6, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 6, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 7, 1, 2, 3, 4, 5, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 4, 5, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 1, 2, 3, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 6, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 7, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3], \"Freq\": [0.2398664411133536, 0.2398664411133536, 0.2398664411133536, 0.025819314711744888, 0.3614704059644284, 0.012909657355872444, 0.2969221191850662, 0.2969221191850662, 0.012909657355872444, 0.020834351565564336, 0.8472636303329497, 0.020834351565564336, 0.0069447838551881125, 0.06944783855188112, 0.02777913542075245, 0.12591337464933766, 0.12591337464933766, 0.12591337464933766, 0.6295668732466884, 0.0452325870198151, 0.1356977610594453, 0.0452325870198151, 0.6784888052972265, 0.0904651740396302, 0.21100697765137924, 0.21100697765137924, 0.21100697765137924, 0.21100697765137924, 0.6859618561147394, 0.09019567658256088, 0.1519085079285236, 0.04509783829128044, 0.02136213392744863, 0.002373570436383181, 0.002373570436383181, 0.4772944623909353, 0.2978159662026084, 0.1380603816833284, 0.043390405671903214, 0.031556658670475066, 0.009861455834523457, 0.0019722911669046916, 0.1680782703435395, 0.336156540687079, 0.1680782703435395, 0.1680782703435395, 0.24205898151893343, 0.5446327084176003, 0.03025737268986668, 0.06051474537973336, 0.06051474537973336, 0.09077211806960003, 0.14916973514386278, 0.29833947028772556, 0.14916973514386278, 0.29833947028772556, 0.029194248515098265, 0.05838849703019653, 0.029194248515098265, 0.029194248515098265, 0.05838849703019653, 0.7590504613925549, 0.11342601564078204, 0.11342601564078204, 0.11342601564078204, 0.11342601564078204, 0.5671300782039103, 0.07209813983912809, 0.07209813983912809, 0.07209813983912809, 0.07209813983912809, 0.720981398391281, 0.19899486527018578, 0.19899486527018578, 0.19899486527018578, 0.19899486527018578, 0.19899486527018578, 0.1846689782666166, 0.1846689782666166, 0.1846689782666166, 0.1846689782666166, 0.1846689782666166, 0.10894502638373314, 0.10894502638373314, 0.10894502638373314, 0.6536701583023988, 0.04620440667313195, 0.04620440667313195, 0.04620440667313195, 0.8316793201163751, 0.11575188092428769, 0.11575188092428769, 0.11575188092428769, 0.5787594046214385, 0.6110357521912667, 0.2245686952497818, 0.08356044474410486, 0.04178022237205243, 0.020890111186026215, 0.005222527796506554, 0.01566758338951966, 0.4306193798325699, 0.30645305558636876, 0.184928568026257, 0.03962755029134079, 0.02113469348871509, 0.01320918343044693, 0.02448116400806291, 0.3794580421249751, 0.012240582004031456, 0.1101652380362831, 0.45290153414916384, 0.02448116400806291, 0.09437090438532247, 0.7927155968367088, 0.07549672350825798, 0.018874180877064495, 0.33115163534588776, 0.33115163534588776, 0.11038387844862925, 0.11038387844862925, 0.09748801133280824, 0.09748801133280824, 0.09748801133280824, 0.6824160793296576, 0.6438594269006748, 0.10803010518467698, 0.06913926731819327, 0.15556335146593483, 0.017284816829548317, 0.004321204207387079, 0.8792578378065455, 0.050243305017516886, 0.025121652508758443, 0.025121652508758443, 0.08485656209448694, 0.08485656209448694, 0.7637090588503825, 0.1244551131939088, 0.02489102263878176, 0.7965127244410163, 0.02489102263878176, 0.5274611981480404, 0.22357408810390192, 0.17364977716807914, 0.03690057764821682, 0.03038871100441385, 0.008682488858403958, 0.0021706222146009894, 0.5400593333937637, 0.2773277657967976, 0.07298099099915725, 0.029192396399662902, 0.029192396399662902, 0.014596198199831451, 0.04378859459949436, 0.2649882101616327, 0.1766588067744218, 0.0883294033872109, 0.0883294033872109, 0.1766588067744218, 0.1766588067744218, 0.08486647701839771, 0.08486647701839771, 0.7637982931655793, 0.10387531409444589, 0.10387531409444589, 0.10387531409444589, 0.6232518845666754, 0.10387531409444589, 0.09254902554091819, 0.09254902554091819, 0.09254902554091819, 0.6478431787864274, 0.03752878092655523, 0.03752878092655523, 0.03752878092655523, 0.15011512370622093, 0.7130468376045495, 0.056837231393963995, 0.056837231393963995, 0.795721239515496, 0.7383080982809694, 0.011536064035640147, 0.09805654430294125, 0.04614425614256059, 0.10382457632076132, 0.07639844917672603, 0.07639844917672603, 0.07639844917672603, 0.07639844917672603, 0.7639844917672602, 0.08309827078035552, 0.24929481234106657, 0.08309827078035552, 0.08309827078035552, 0.49858962468213314, 0.905314600172435, 0.015344315257159917, 0.04603294577147975, 0.015344315257159917, 0.015344315257159917, 0.8664299030374164, 0.04125856681130554, 0.04125856681130554, 0.04125856681130554, 0.07011438366659167, 0.03505719183329584, 0.5609150693327334, 0.03505719183329584, 0.3155147264996625, 0.7394707856071503, 0.027904557947439634, 0.06045987555278587, 0.13952278973719817, 0.013952278973719817, 0.013952278973719817, 0.2294567696483005, 0.2294567696483005, 0.11472838482415025, 0.11472838482415025, 0.11472838482415025, 0.2294567696483005, 0.23990623593034704, 0.23990623593034704, 0.23990623593034704, 0.17338883972335056, 0.17338883972335056, 0.17338883972335056, 0.17338883972335056, 0.17338883972335056, 0.022954398927380017, 0.022954398927380017, 0.8952215581678207, 0.045908797854760035, 0.16045584366571744, 0.16045584366571744, 0.16045584366571744, 0.48136753099715235, 0.07552804039088615, 0.3021121615635446, 0.07552804039088615, 0.45316824234531683, 0.07955063078138486, 0.07955063078138486, 0.7159556770324638, 0.18792242431367684, 0.09396121215683842, 0.09396121215683842, 0.5637672729410305, 0.5342602672297029, 0.22417979840618904, 0.11523260665738688, 0.09637636193163267, 0.023046521331477377, 0.006285414908584739, 0.16693483085904107, 0.19728661828795763, 0.5918598548638728, 0.015175893714458278, 0.015175893714458278, 0.015175893714458278, 0.8874802832496483, 0.0341338570480634, 0.0341338570480634, 0.0341338570480634, 0.032946833667827906, 0.8895645090313534, 0.032946833667827906, 0.032946833667827906, 0.234143420184242, 0.234143420184242, 0.234143420184242, 0.02527996122793268, 0.8637320086210332, 0.02949328809925479, 0.008426653742644227, 0.06741322994115381, 0.004213326871322113, 0.16349521447497217, 0.5926701524717741, 0.10218450904685761, 0.06131070542811456, 0.06641993088045745, 0.01021845090468576, 0.015499978404182303, 0.8679987906342089, 0.015499978404182303, 0.015499978404182303, 0.07749989202091151, 0.015499978404182303, 0.17699271475998157, 0.058997571586660524, 0.058997571586660524, 0.058997571586660524, 0.058997571586660524, 0.41298300110662367, 0.11799514317332105, 0.05693685520394423, 0.45549484163155385, 0.028468427601972116, 0.05693685520394423, 0.39855798642760965, 0.035225370623317216, 0.5812186152847341, 0.017612685311658608, 0.1761268531165861, 0.10567611186995166, 0.017612685311658608, 0.07045074124663443, 0.5213706196065622, 0.22513731301192458, 0.18721944976781096, 0.05687679486617042, 0.007109599358271303, 0.002369866452757101, 0.42532517391604, 0.2494695731622927, 0.2699178988313331, 0.04089665133808077, 0.01226899540142423, 0.008179330267616153, 0.7716432111698577, 0.17623949884743664, 0.03810583758863495, 0.004763229698579369, 0.004763229698579369, 0.8746355198548043, 0.039756159993400195, 0.039756159993400195, 0.039756159993400195, 0.15407958449425388, 0.6992842680893061, 0.11852275730327222, 0.011852275730327223, 0.011852275730327223, 0.08914481120078596, 0.08914481120078596, 0.08914481120078596, 0.08914481120078596, 0.7131584896062877, 0.8709142690523456, 0.06417263035122546, 0.022918796554009094, 0.03208631517561273, 0.004583759310801819, 0.12975431865786935, 0.06487715932893467, 0.12975431865786935, 0.06487715932893467, 0.5838944339604121, 0.4418196378555699, 0.3414060837974858, 0.05355389549764483, 0.013388473874411208, 0.040165421623233624, 0.10710779099528966, 0.43382348686372196, 0.3098739191883728, 0.08154576820746653, 0.11090224476215449, 0.05218929165277858, 0.0065236614565973225, 0.0032618307282986612, 0.49737616536139306, 0.17883187968050088, 0.23192259396064957, 0.05588496240015652, 0.02794248120007826, 0.005588496240015652, 0.4199451674609662, 0.12113802907527871, 0.15344150349535304, 0.26650366396561315, 0.040379343025092905, 0.00807586860501858, 0.1940415404174365, 0.09702077020871824, 0.09702077020871824, 0.04851038510435912, 0.5821246212523095, 0.2738259218688954, 0.3337253422777163, 0.06845648046722386, 0.017114120116805964, 0.29949710204410435, 0.008557060058402982, 0.7990625135340419, 0.07683293399365787, 0.020488782398308765, 0.08707732519281225, 0.010244391199154382, 0.38941281893236246, 0.21354896522097297, 0.037685111509583466, 0.07537022301916693, 0.2763574844036121, 0.012561703836527821, 0.061028038931811836, 0.020342679643937277, 0.020342679643937277, 0.874735224689303, 0.12490668437994998, 0.12490668437994998, 0.12490668437994998, 0.6245334218997499, 0.1597546449993881, 0.39212503772577084, 0.39212503772577084, 0.00726157477269946, 0.04356944863619676, 0.00726157477269946, 0.013005835690423588, 0.8843968269488041, 0.013005835690423588, 0.013005835690423588, 0.03901750707127077, 0.03901750707127077, 0.5666152966060621, 0.128776203774105, 0.18028668528374703, 0.025755240754821003, 0.051510481509642006, 0.025755240754821003, 0.2462971252621128, 0.2462971252621128, 0.2462971252621128, 0.5560943680555148, 0.22243774722220594, 0.055609436805551485, 0.11121887361110297, 0.055609436805551485, 0.24628865087782173, 0.24628865087782173, 0.24628865087782173, 0.10826719999633964, 0.10826719999633964, 0.10826719999633964, 0.6496031999780378, 0.3023279758233117, 0.10077599194110391, 0.10077599194110391, 0.10077599194110391, 0.3023279758233117, 0.33346796136026874, 0.16673398068013437, 0.33346796136026874, 0.027788996780022397, 0.11115598712008959, 0.027788996780022397, 0.05637407809253391, 0.05637407809253391, 0.7892370932954748, 0.016395218204374153, 0.46726371882466333, 0.4836589370290375, 0.008197609102187076, 0.016395218204374153, 0.008197609102187076, 0.03732446649820923, 0.8957871959570215, 0.03732446649820923, 0.2147131429765392, 0.33399822240794985, 0.04771403177256427, 0.357855238294232, 0.023857015886282133, 0.30625758073996445, 0.15312879036998223, 0.15312879036998223, 0.15312879036998223, 0.9103504540465027, 0.030345015134883423, 0.030345015134883423, 0.030345015134883423, 0.1157518177750662, 0.1157518177750662, 0.1157518177750662, 0.578759088875331, 0.5227325502966301, 0.10454651005932603, 0.3484883668644201, 0.004356104585805251, 0.004356104585805251, 0.013068313757415753, 0.4961633861170642, 0.264843969616541, 0.18773749744969995, 0.030172097804416063, 0.011174851038672617, 0.007822395727070832, 0.0011174851038672617, 0.20960231483944125, 0.1467216203876089, 0.06288069445183238, 0.4192046296788825, 0.1467216203876089, 0.039977585865287386, 0.8795068890363225, 0.039977585865287386, 0.039977585865287386, 0.18511562308026094, 0.18511562308026094, 0.18511562308026094, 0.18511562308026094, 0.18511562308026094, 0.24622881890263593, 0.24622881890263593, 0.24622881890263593, 0.03090997769064456, 0.8654793753380478, 0.03090997769064456, 0.03090997769064456, 0.03090997769064456, 0.03090997769064456, 0.8464205525115879, 0.07255033307242181, 0.06045861089368485, 0.01209172217873697, 0.01209172217873697, 0.06828896942437597, 0.7102052820135101, 0.04097338165462558, 0.006828896942437597, 0.006828896942437597, 0.17755132050337752, 0.5833652651487197, 0.16501567261178923, 0.15571901499985744, 0.07902158970142019, 0.011620822014914734, 0.002324164402982947, 0.002324164402982947, 0.08639793231252775, 0.043198966156263875, 0.043198966156263875, 0.8207803569690136, 0.6215406915257531, 0.0682178807772168, 0.060638116246414936, 0.24255246498565974, 0.007579764530801867, 0.007579764530801867, 0.14610826097761628, 0.5235546018364583, 0.012175688414801357, 0.2556894567108285, 0.060878442074006785, 0.18600456615614236, 0.3720091323122847, 0.18600456615614236, 0.6555023056655672, 0.07283358951839636, 0.07283358951839636, 0.07283358951839636, 0.07283358951839636, 0.03641679475919818, 0.12247350482990013, 0.1924583647327002, 0.6473599541009006, 0.017496214975700017, 0.40334920414903636, 0.331748753708379, 0.19093453450841957, 0.02625349849490769, 0.021480135132197202, 0.023866816813552447, 0.541214033310996, 0.16912938540968625, 0.05073881562290587, 0.016912938540968626, 0.21986820103259214, 0.008456469270484313, 0.05044994247504899, 0.20179976990019596, 0.05044994247504899, 0.05044994247504899, 0.05044994247504899, 0.6558492521756368, 0.4763888253743823, 0.2575690778339962, 0.15727669354465254, 0.08433677769785716, 0.018234978961698844, 0.0022793723702123556, 0.22372815982040054, 0.015980582844314323, 0.735106810838459, 0.015980582844314323, 0.007990291422157162, 0.007990291422157162, 0.09820598982709502, 0.19641197965419005, 0.09820598982709502, 0.5892359389625702, 0.02951013285645226, 0.8853039856935678, 0.02951013285645226, 0.125023967904337, 0.125023967904337, 0.125023967904337, 0.125023967904337, 0.375071903713011, 0.16702272834144918, 0.7482618229696923, 0.013361818267315934, 0.02672363653463187, 0.02672363653463187, 0.013361818267315934, 0.35362804765730593, 0.29704756003213695, 0.05658048762516895, 0.028290243812584475, 0.1131609752503379, 0.1555963409692146, 0.1188537936648798, 0.20374936056836537, 0.3056240408525481, 0.016979113380697117, 0.3395822676139423, 0.016979113380697117, 0.07156043353027601, 0.07156043353027601, 0.7871647688330362, 0.19079856733277661, 0.670872381912021, 0.10463147240829686, 0.006154792494605698, 0.012309584989211395, 0.012309584989211395, 0.3317898714068894, 0.03016271558244449, 0.12065086232977797, 0.5127661649015564, 0.16622206881736373, 0.20777758602170468, 0.5956290799288867, 0.013851839068113644, 0.013851839068113644, 0.838499060073689, 0.02661901778011711, 0.03992852667017567, 0.06654754445029279, 0.013309508890058556, 0.2339820621576538, 0.2339820621576538, 0.2339820621576538, 0.6098464641219057, 0.21724144430093636, 0.12039887274509727, 0.03140840158567755, 0.007852100396419388, 0.010469467195225849, 0.5384426104909389, 0.19637318735551887, 0.10135390315123555, 0.11085583157166388, 0.04117502315518944, 0.012669237893904443, 0.2464344848155702, 0.2464344848155702, 0.2464344848155702, 0.38267958750772096, 0.2991858593242182, 0.23308665784561186, 0.02783124272783425, 0.03478905340979281, 0.02087343204587569, 0.33025091721323435, 0.11008363907107813, 0.05504181953553906, 0.4770291026413385, 0.01834727317851302, 0.2811284579223632, 0.17112167003969933, 0.3850237575893235, 0.02444595286281419, 0.07944934680414611, 0.05500339394133193, 0.12379394759624973, 0.43327881658687406, 0.1856909213943746, 0.12379394759624973, 0.12379394759624973, 0.12006882315107485, 0.36020646945322454, 0.36020646945322454, 0.060034411575537426, 0.060034411575537426, 0.02339200745612286, 0.9356802982449144, 0.02339200745612286, 0.02339200745612286, 0.09261863764916801, 0.09261863764916801, 0.09261863764916801, 0.6483304635441761, 0.22551571393301728, 0.21279431468551374, 0.5065429882187773, 0.00346947252204642, 0.016190871769549958, 0.0346947252204642, 0.00115649084068214, 0.15841433371193295, 0.27722508399588264, 0.35643225085184915, 0.03960358342798324, 0.07920716685596647, 0.03960358342798324, 0.02368912660700115, 0.9001868110660437, 0.011844563303500574, 0.02368912660700115, 0.03553368991050172, 0.011844563303500574, 0.02077510951616946, 0.9141048187114563, 0.02077510951616946, 0.02077510951616946, 0.1026838223106297, 0.1026838223106297, 0.1026838223106297, 0.6161029338637781, 0.1026838223106297, 0.25703009462956616, 0.5283396389607748, 0.07457045955302227, 0.04283834910492769, 0.07298385403061755, 0.023799082836070937, 0.0015866055224047293, 0.23921082190135443, 0.3588162328520316, 0.11960541095067721, 0.11960541095067721, 0.23921082190135443, 0.03732735080744222, 0.7029984402068284, 0.1990792043063585, 0.012442450269147406, 0.031106125672868516, 0.006221225134573703, 0.024565619266354272, 0.9089279128551081, 0.024565619266354272, 0.024565619266354272, 0.024565619266354272, 0.47331490710247803, 0.27696156136127537, 0.13228014870986285, 0.07440758364929785, 0.031003159853874106, 0.008267509294366428, 0.004133754647183214, 0.9273550740233767, 0.025063650649280452, 0.025063650649280452, 0.025063650649280452, 0.025063650649280452, 0.14071370563421465, 0.3869626904940903, 0.03517842640855366, 0.03517842640855366, 0.3869626904940903, 0.03517842640855366, 0.029650759892425365, 0.8895227967727609, 0.029650759892425365, 0.029650759892425365, 0.029650759892425365, 0.566722132054278, 0.1896018897681592, 0.13959699576336995, 0.0729238037569843, 0.022918909752195068, 0.006250611750598655, 0.0020835372501995515, 0.18553508847940495, 0.18553508847940495, 0.18553508847940495, 0.3710701769588099, 0.9190567247709602, 0.026258763564884578, 0.026258763564884578, 0.026258763564884578, 0.07282401383891858, 0.8218710133249383, 0.062420583290501645, 0.01040343054841694, 0.02080686109683388, 0.12269662233999007, 0.12269662233999007, 0.12269662233999007, 0.6134831116999503, 0.6205416410445853, 0.17935921186338513, 0.07215600477262621, 0.07524840497716732, 0.03298560218177198, 0.00721560047726262, 0.011338800749984118, 0.8744898964743667, 0.03974954074883485, 0.03974954074883485, 0.054718842279232995, 0.054718842279232995, 0.10943768455846599, 0.054718842279232995, 0.7113449496300289, 0.5003883129274529, 0.26451503410643135, 0.12973030335156188, 0.07076198364630648, 0.026956946150973896, 0.005054427403307605, 0.0016848091344358685, 0.8322874388994156, 0.10513104491361039, 0.026282761228402597, 0.026282761228402597, 0.008760920409467533, 0.7055890682002872, 0.08684173147080458, 0.032565649301551716, 0.16282824650775857, 0.005427608216925286, 0.9194053765870922, 0.030646845886236406, 0.030646845886236406, 0.030646845886236406, 0.24634482144680775, 0.24634482144680775, 0.24634482144680775, 0.4598805727226823, 0.1532935242408941, 0.07664676212044705, 0.07664676212044705, 0.22994028636134115, 0.8401821038374929, 0.046676783546527385, 0.046676783546527385, 0.24615033081615603, 0.24615033081615603, 0.24615033081615603, 0.22896992845057518, 0.22896992845057518, 0.22896992845057518, 0.2812380892511237, 0.10937036804210366, 0.5781005167939766, 0.015624338291729096, 0.015624338291729096, 0.2469907309296428, 0.4779431027080101, 0.17962962249428568, 0.02566137464204081, 0.03528439013280611, 0.038492061963061215, 0.03207052748897683, 0.8979747696913513, 0.03207052748897683, 0.03207052748897683, 0.11238653425343384, 0.11238653425343384, 0.11238653425343384, 0.11238653425343384, 0.5619326712671692, 0.18742352130478965, 0.09371176065239482, 0.09371176065239482, 0.5622705639143689, 0.061893487822137896, 0.061893487822137896, 0.8046153416877927, 0.12596914604095263, 0.12596914604095263, 0.12596914604095263, 0.6298457302047632, 0.24901449146496138, 0.5689023381930272, 0.08236633179225646, 0.0019154960881920106, 0.05938037873395233, 0.0363944256756482, 0.0019154960881920106, 0.07806326085101215, 0.07806326085101215, 0.07806326085101215, 0.7025693476591094, 0.24002876262073697, 0.24002876262073697, 0.24002876262073697, 0.27689489586871985, 0.5537897917374397, 0.023074574655726654, 0.057686436639316635, 0.011537287327863327, 0.03461186198358998, 0.03461186198358998, 0.05754627517309273, 0.05754627517309273, 0.8056478524232982, 0.05754627517309273, 0.05754627517309273, 0.09744110397562986, 0.09744110397562986, 0.09744110397562986, 0.682087727829409, 0.9432770546601745, 0.01925055213592193, 0.01925055213592193, 0.01925055213592193, 0.01925055213592193, 0.1982420123647503, 0.09912100618237515, 0.09912100618237515, 0.594726037094251, 0.24851846254427312, 0.6181100222254998, 0.07009495097402575, 0.038233609622195866, 0.019116804811097933, 0.0031861341351829887, 0.3453935372109403, 0.5578186704251805, 0.038917581657570734, 0.012972527219190245, 0.016215659023987805, 0.02270192263358293, 0.004864697707196342, 0.1263617992401185, 0.252723598480237, 0.1263617992401185, 0.1263617992401185, 0.3790853977203555, 0.31849408610974583, 0.10616469536991528, 0.10616469536991528, 0.10616469536991528, 0.21232939073983056, 0.30773146677709656, 0.15386573338854828, 0.15386573338854828, 0.15386573338854828, 0.10005700184271213, 0.20011400368542426, 0.10005700184271213, 0.6003420110562727, 0.2109415928262233, 0.2109415928262233, 0.2109415928262233, 0.2109415928262233, 0.08939105176792182, 0.08939105176792182, 0.08939105176792182, 0.7151284141433746, 0.18460158594963835, 0.18460158594963835, 0.061533861983212784, 0.061533861983212784, 0.4922708958657023, 0.8825954644120124, 0.03837371584400054, 0.03837371584400054, 0.8892536046702482, 0.037052233527927005, 0.037052233527927005, 0.06374806008325107, 0.06374806008325107, 0.06374806008325107, 0.7649767209990127, 0.24634136905957685, 0.24634136905957685, 0.24634136905957685, 0.4530849391387494, 0.20020032194502882, 0.12117387907199112, 0.20020032194502882, 0.021073718099476717, 0.010536859049738358, 0.1877214881112028, 0.06257382937040093, 0.1877214881112028, 0.5631644643336083, 0.1048958904846164, 0.1048958904846164, 0.1048958904846164, 0.1048958904846164, 0.6293753429076984, 0.9007119937891583, 0.034642768991890706, 0.034642768991890706, 0.08687385545198134, 0.11945155124647434, 0.7492870032733391, 0.010859231931497668, 0.021718463862995335, 0.08910490918609751, 0.08910490918609751, 0.08910490918609751, 0.08910490918609751, 0.7128392734887801, 0.059516811251854956, 0.059516811251854956, 0.059516811251854956, 0.059516811251854956, 0.7737185462741144, 0.13629591890216017, 0.13629591890216017, 0.13629591890216017, 0.5451836756086407, 0.07996015194436672, 0.15992030388873343, 0.07996015194436672, 0.07996015194436672, 0.15992030388873343, 0.39980075972183354, 0.09001625000771148, 0.14002527778977342, 0.750135416730929, 0.010001805556412387, 0.05585998544361492, 0.715007813678271, 0.1619939577864833, 0.0027929992721807463, 0.0055859985443614925, 0.053066986171434176, 0.0027929992721807463, 0.09292335851031808, 0.09292335851031808, 0.09292335851031808, 0.09292335851031808, 0.6504635095722265, 0.07135918209430285, 0.07135918209430285, 0.07135918209430285, 0.07135918209430285, 0.7135918209430284, 0.1010893223861774, 0.1010893223861774, 0.1010893223861774, 0.1010893223861774, 0.6065359343170644, 0.42557804082039724, 0.2480250517469412, 0.22880539829053617, 0.044845858064945086, 0.03752408531964793, 0.014643545490594314, 0.0009152215931621446, 0.04737771154622049, 0.04737771154622049, 0.04737771154622049, 0.8527988078319688, 0.4475399938758454, 0.02632588199269679, 0.13162940996348396, 0.36856234789775505, 0.02632588199269679, 0.10293450788395346, 0.10293450788395346, 0.10293450788395346, 0.6176070473037207, 0.6029268564847197, 0.07975222969374599, 0.16907472695074147, 0.11803329994674405, 0.025520713501998714, 0.0031900891877498392, 0.08005208869353331, 0.08005208869353331, 0.7204687982417999, 0.24625242995675156, 0.24625242995675156, 0.24625242995675156, 0.08648029584601552, 0.08648029584601552, 0.08648029584601552, 0.6918423667681242, 0.5828377256239614, 0.17213342155733574, 0.1540141140249846, 0.04227838424215264, 0.03623861506470226, 0.006039769177450377, 0.006039769177450377, 0.1185678299581314, 0.1185678299581314, 0.1185678299581314, 0.1185678299581314, 0.592839149790657, 0.2072979243712013, 0.2072979243712013, 0.2072979243712013, 0.2072979243712013, 0.2072979243712013, 0.20721669230168566, 0.20721669230168566, 0.20721669230168566, 0.20721669230168566, 0.20721669230168566, 0.12360211853329467, 0.12360211853329467, 0.12360211853329467, 0.12360211853329467, 0.4944084741331787, 0.7395404929757479, 0.05845165408399382, 0.10927917937442323, 0.07878266420016558, 0.010165505058085882, 0.0025413762645214704, 0.8731887331589039, 0.0281673784889969, 0.0281673784889969, 0.0563347569779938, 0.1747808522648601, 0.04369521306621502, 0.04369521306621502, 0.6554281959932253, 0.04369521306621502, 0.15148398146220687, 0.15148398146220687, 0.15148398146220687, 0.45445194438662057, 0.08698104274711754, 0.17396208549423509, 0.08698104274711754, 0.5218862564827053, 0.011141768910760939, 0.9581921263254407, 0.011141768910760939, 0.011141768910760939, 0.6850979074486881, 0.12361093742957982, 0.13827664187037741, 0.033521610150394524, 0.016760805075197262, 0.0020951006343996577, 0.0020951006343996577, 0.4288201421817136, 0.41498723436940027, 0.027665815624626687, 0.006916453906156672, 0.020749361718470014, 0.08299744687388005, 0.006916453906156672, 0.673592454535444, 0.1541401497792778, 0.13872613480135002, 0.018496817973513335, 0.009248408986756667, 0.003082802995585556, 0.001541401497792778, 0.07795497123470782, 0.16565431387375412, 0.6918503697080319, 0.048721857021692384, 0.009744371404338478, 0.9275033318994128, 0.02506765761890305, 0.02506765761890305, 0.02506765761890305, 0.02506765761890305, 0.41754033357479603, 0.20877016678739801, 0.13918011119159868, 0.03479502779789967, 0.03479502779789967, 0.13918011119159868, 0.16190093122026922, 0.16190093122026922, 0.16190093122026922, 0.32380186244053843, 0.2806502990855337, 0.1187366649977258, 0.5505063558985469, 0.021588484545041052, 0.021588484545041052, 0.8590411731977473, 0.04295205865988736, 0.04295205865988736, 0.08359192881129815, 0.2507757864338944, 0.041795964405649075, 0.585143501679087, 0.041795964405649075, 0.08277857964007583, 0.08277857964007583, 0.08277857964007583, 0.6622286371206066, 0.3371485366743905, 0.16857426833719524, 0.024082038333885034, 0.024082038333885034, 0.04816407666777007, 0.38531261334216055, 0.3639105341728963, 0.4100491911841028, 0.15986069894023658, 0.009097763354322408, 0.03314185221931734, 0.023394248625400477, 0.0006498402395944577, 0.17991755683880184, 0.06746908381455069, 0.6971805327503572, 0.02248969460485023, 0.10611310497096146, 0.10611310497096146, 0.10611310497096146, 0.6366786298257687, 0.01755780681197358, 0.8954481474106526, 0.05267342043592074, 0.01755780681197358, 0.01755780681197358, 0.05215337651809509, 0.521533765180951, 0.026076688259047544, 0.026076688259047544, 0.3650736356266656, 0.09021830626467829, 0.840670581102684, 0.015036384377446381, 0.005467776137253229, 0.019137216480386302, 0.027338880686266148, 0.0013669440343133073, 0.1960612390457968, 0.6140785977660805, 0.08878244786979478, 0.018496343306207245, 0.059188298579863186, 0.018496343306207245, 0.003699268661241449, 0.005914479882479436, 0.9522312610791892, 0.005914479882479436, 0.005914479882479436, 0.017743439647438308, 0.005914479882479436, 0.005914479882479436, 0.3505290196080445, 0.5673035448919668, 0.05073446336432223, 0.018448895768844446, 0.009224447884422223, 0.009224447884422223, 0.04531120370240213, 0.04531120370240213, 0.8609128703456405, 0.04531120370240213, 0.5858843142058441, 0.19962263337207428, 0.11360637671581464, 0.06329498131309673, 0.022721275343162928, 0.012983585910378816, 0.001622948238797352, 0.2886848638537143, 0.2422890821629388, 0.3814764272352653, 0.04639578169077551, 0.025775434272653063, 0.02062034741812245, 0.6158180049799062, 0.10263633416331772, 0.09183250951454743, 0.16745928205593943, 0.005401912324385143, 0.01620573697315543, 0.6531596030069382, 0.18621997192112708, 0.08894088211158307, 0.05002924618776548, 0.008338207697960913, 0.011117610263947884, 0.5241919805677359, 0.13104799514193397, 0.13104799514193397, 0.42147713828407884, 0.1988993236846215, 0.07577117092747485, 0.213106418233523, 0.0852425672934092, 0.004735698182967178, 0.8044079473963581, 0.05683317019648182, 0.08743564645612588, 0.039346040905256646, 0.008743564645612588, 0.08041778818307764, 0.08041778818307764, 0.08041778818307764, 0.7237600936476988, 0.3710266018072113, 0.5158174708051475, 0.036197717249484035, 0.006032952874914005, 0.03318124081202703, 0.03318124081202703, 0.006032952874914005, 0.25378732146086347, 0.14025088817574033, 0.5075746429217269, 0.006678613722654302, 0.026714454890617207, 0.06010752350388872, 0.7682755689750648, 0.07278400127132192, 0.05660977876658372, 0.024261333757107307, 0.048522667514214614, 0.008087111252369102, 0.03234844500947641, 0.481472166994802, 0.3157561653314748, 0.1254067039614368, 0.0246334597067108, 0.0515063248413044, 0.0022394054278828002, 0.04246000788956909, 0.21230003944784548, 0.1273800236687073, 0.04246000788956909, 0.5519801025643982, 0.18643852533638933, 0.566486288522106, 0.08604855015525661, 0.007170712512938051, 0.03585356256469026, 0.12190211271994687, 0.32533789589233225, 0.16266894794616613, 0.16266894794616613, 0.16266894794616613, 0.37138304655689497, 0.42443776749359424, 0.120071210540951, 0.022338829868083908, 0.044677659736167816, 0.013961768667552442, 0.23007220322928557, 0.03834536720488093, 0.7029983987561503, 0.025563578136587287, 0.09303492485952908, 0.04651746242976454, 0.7907968613059971, 0.04651746242976454, 0.30638644237518325, 0.15319322118759163, 0.15319322118759163, 0.15319322118759163, 0.01808020221941493, 0.9220903131901614, 0.01808020221941493, 0.01808020221941493, 0.01808020221941493, 0.3071988760929095, 0.15359943804645476, 0.15359943804645476, 0.15359943804645476, 0.16701432900070345, 0.5010429870021104, 0.16701432900070345, 0.15939394194104162, 0.01449035835827651, 0.02898071671655302, 0.07245179179138254, 0.739008276272102, 0.6034502574153473, 0.19143939200762744, 0.05202157391511615, 0.1310943662660927, 0.012485177739627876, 0.008323451826418584, 0.35863036420849986, 0.17931518210424993, 0.17931518210424993, 0.009880395219655362, 0.13832553307517506, 0.019760790439310724, 0.4544981801041466, 0.36557462312724837, 0.009880395219655362, 0.04328490168787486, 0.04328490168787486, 0.04328490168787486, 0.21642450843937433, 0.649273525318123, 0.13630675667324343, 0.13630675667324343, 0.13630675667324343, 0.5452270266929737, 0.0893821729838689, 0.0893821729838689, 0.0893821729838689, 0.7150573838709512, 0.915503344645815, 0.031569080849855685, 0.031569080849855685, 0.031569080849855685, 0.24621088098353805, 0.24621088098353805, 0.24621088098353805, 0.144048017143198, 0.144048017143198, 0.144048017143198, 0.432144051429594, 0.054070552311655765, 0.054070552311655765, 0.7750112498003994, 0.07209406974887436, 0.01802351743721859, 0.08485482791498668, 0.08485482791498668, 0.7636934512348801, 0.10302230005997923, 0.20604460011995845, 0.6387382603718712, 0.020604460011995843, 0.020604460011995843, 0.5179954934093433, 0.20144269188141126, 0.10647685142303166, 0.08921033497605356, 0.07194381852907544, 0.008633258223489053, 0.5943854514859529, 0.14309279387624793, 0.10692648333609735, 0.09749179362997111, 0.048745896814985555, 0.007862241421771865, 0.001572448284354373, 0.3215914144470046, 0.46528119537013435, 0.14368978092312973, 0.020527111560447103, 0.04447540838096872, 0.0068423705201490345, 0.289451501514856, 0.5389786579931801, 0.08982977633219669, 0.009981086259132965, 0.029943258777398896, 0.009981086259132965, 0.029943258777398896, 0.09825803587847276, 0.09825803587847276, 0.09825803587847276, 0.6878062511493094, 0.7966256770016, 0.048280344060703036, 0.024140172030351518, 0.024140172030351518, 0.024140172030351518, 0.07242051609105454, 0.0778214046377908, 0.7349799326902465, 0.10376187285038774, 0.017293645475064623, 0.034587290950129246, 0.034587290950129246, 0.649890503780531, 0.12295225747199234, 0.035129216420569244, 0.0878230410514231, 0.017564608210284622, 0.035129216420569244, 0.035129216420569244, 0.6152568543252609, 0.02348308604294889, 0.08923572696320578, 0.253617329263848, 0.009393234417179557, 0.004696617208589778, 0.027362415830969093, 0.9029597224219801, 0.027362415830969093, 0.2247475831738599, 0.2247475831738599, 0.2247475831738599, 0.2247475831738599, 0.08442198687426278, 0.08442198687426278, 0.08442198687426278, 0.16884397374852556, 0.5909539081198394, 0.18640108959449578, 0.1242673930629972, 0.0621336965314986, 0.18640108959449578, 0.0621336965314986, 0.37280217918899156, 0.10292000113031467, 0.10292000113031467, 0.10292000113031467, 0.617520006781888, 0.1295925204314444, 0.4535738215100554, 0.0647962602157222, 0.0647962602157222, 0.0647962602157222, 0.0647962602157222, 0.19438878064716658, 0.21963473838366424, 0.21963473838366424, 0.21963473838366424, 0.21963473838366424, 0.02677645666409739, 0.5890820466101425, 0.02677645666409739, 0.02677645666409739, 0.32131747996916865, 0.16991738254896932, 0.16991738254896932, 0.16991738254896932, 0.16991738254896932, 0.16991738254896932, 0.028530302193816026, 0.8844393680082968, 0.028530302193816026, 0.028530302193816026, 0.028530302193816026, 0.028530302193816026, 0.1437631792968057, 0.1437631792968057, 0.1437631792968057, 0.43128953789041713, 0.06043668527990516, 0.06043668527990516, 0.06043668527990516, 0.06043668527990516, 0.785676908638767, 0.8858302781002518, 0.06561705763705569, 0.016404264409263923, 0.016404264409263923, 0.016404264409263923, 0.014327563581973745, 0.9456191964102671, 0.014327563581973745, 0.014327563581973745, 0.014327563581973745, 0.828401241632608, 0.04982864611323958, 0.08097154993401431, 0.006228580764154948, 0.02491432305661979, 0.006228580764154948, 0.14376580289760738, 0.14376580289760738, 0.14376580289760738, 0.4312974086928222, 0.061593673738277316, 0.1539841843456933, 0.030796836869138658, 0.33876520556052525, 0.40035887929880254, 0.15181889060583975, 0.15181889060583975, 0.15181889060583975, 0.4554566718175193, 0.16452629365604993, 0.3923319310259652, 0.2699918665124922, 0.025311737485546142, 0.06749796662812305, 0.08015383537089611, 0.05671705870009349, 0.8394124687613836, 0.022686823480037397, 0.0037811372466728995, 0.026467960726710295, 0.05293592145342059, 0.13628271954888133, 0.13628271954888133, 0.13628271954888133, 0.5451308781955253, 0.4663730060577187, 0.32992814577224705, 0.10624804694360497, 0.0659856291544494, 0.023486410377007418, 0.005592002470716052, 0.0011184004941432104, 0.0754914639287728, 0.0754914639287728, 0.7549146392877281, 0.0754914639287728, 0.02018626092474945, 0.12111756554849672, 0.02018626092474945, 0.4642840012692374, 0.36335269664549014, 0.10927059125016768, 0.10927059125016768, 0.10927059125016768, 0.655623547501006, 0.1515102549708499, 0.1515102549708499, 0.1515102549708499, 0.45453076491254973, 0.043966855296466774, 0.043966855296466774, 0.8793371059293354, 0.4513336162821671, 0.04298415393163496, 0.06447623089745244, 0.08596830786326992, 0.12895246179490488, 0.21492076965817483, 0.09222739523060434, 0.09222739523060434, 0.09222739523060434, 0.6455917666142305, 0.01949075410239531, 0.7796301640958124, 0.009745377051197655, 0.18516216397275545, 0.5083969960425813, 0.10167939920851626, 0.25419849802129063, 0.025419849802129066, 0.0762595494063872, 0.3728783510775875, 0.12429278369252918, 0.041430927897509726, 0.041430927897509726, 0.041430927897509726, 0.41430927897509723, 0.29090073513171416, 0.05133542384677308, 0.34223615897848725, 0.05133542384677308, 0.23956531128494105, 0.4285160929692513, 0.3188956970933963, 0.07972392427334908, 0.08968941480751771, 0.009965490534168635, 0.01993098106833727, 0.049827452670843174, 0.6587458756773935, 0.14523531117296865, 0.10373950798069188, 0.04668277859131135, 0.031121852394207564, 0.005186975399034594, 0.005186975399034594, 0.005186975399034594, 0.19902039300718266, 0.19902039300718266, 0.19902039300718266, 0.19902039300718266, 0.19902039300718266, 0.11898238245048492, 0.11898238245048492, 0.11898238245048492, 0.5949119122524246, 0.562532606574281, 0.15064432515040066, 0.18306145841061347, 0.08199627824642061, 0.009534450958886118, 0.009534450958886118, 0.7304252158956976, 0.16397300765005457, 0.04471991117728761, 0.014906637059095871, 0.029813274118191742, 0.6307774222953169, 0.1642955146443616, 0.09975084817693382, 0.0821477573221808, 0.011735393903168684, 0.002933848475792171, 0.002933848475792171, 0.03327232700173424, 0.03327232700173424, 0.8983528290468246, 0.03327232700173424, 0.10211963949966352, 0.15317945924949528, 0.6807975966644235, 0.034039879833221175, 0.017019939916610587, 0.04865689020760249, 0.04865689020760249, 0.8271671335292423, 0.1711459601310565, 0.1711459601310565, 0.1711459601310565, 0.342291920262113, 0.028062617278073045, 0.05612523455614609, 0.7015654319518261, 0.028062617278073045, 0.16837570366843826, 0.15907855132889576, 0.3181571026577915, 0.15907855132889576, 0.15907855132889576, 0.15907855132889576, 0.5595109704721899, 0.1450221079332471, 0.20577461260798577, 0.051933592705824975, 0.029396373229712253, 0.0068591537535995254, 0.000979879107657075, 0.21177122839717444, 0.21177122839717444, 0.21177122839717444, 0.21177122839717444, 0.09551963569726771, 0.09551963569726771, 0.09551963569726771, 0.5731178141836062, 0.40555816234402137, 0.3702922351836717, 0.0999201269543241, 0.06818079251000939, 0.036441458065694675, 0.017632963580174844, 0.002351061810689979, 0.39111222888429853, 0.551674091268379, 0.008233941660722074, 0.004116970830361037, 0.020584854151805186, 0.020584854151805186, 0.32058374454076055, 0.4303726981506101, 0.11857206989863747, 0.017566232577575922, 0.07904804659909165, 0.035132465155151844, 0.0043915581443939805, 0.9074529467551659, 0.03360936839833947, 0.03360936839833947, 0.6488021399764331, 0.1645287723385213, 0.10554675961339104, 0.049669063347478136, 0.015521582296086916, 0.006208632918434767, 0.00931294937765215, 0.926926905972522, 0.013055308534824254, 0.013055308534824254, 0.03916592560447276, 0.15148713807203673, 0.15148713807203673, 0.15148713807203673, 0.4544614142161102, 0.24611602776453836, 0.24611602776453836, 0.24611602776453836, 0.33465781724890326, 0.4302743364628756, 0.04780825960698618, 0.04780825960698618, 0.04780825960698618, 0.04780825960698618, 0.04780825960698618, 0.5159056500970646, 0.2356104543750374, 0.06499598741380341, 0.13811647325433227, 0.04062249213362713, 0.004062249213362713, 0.366402614405929, 0.27387670167715905, 0.17024767942093672, 0.14434042385688112, 0.025907255564055584, 0.01850518254575399, 0.003701036509150798, 0.03306928087945854, 0.8928705837453805, 0.03306928087945854, 0.04299288143870263, 0.8598576287740527, 0.04299288143870263, 0.9442734760130901, 0.013685122840769422, 0.013685122840769422, 0.013685122840769422, 0.8950265560269423, 0.024189906919647088, 0.024189906919647088, 0.024189906919647088, 0.024189906919647088, 0.0675470811454899, 0.8949988251777412, 0.016886770286372477, 0.016886770286372477, 0.016886770286372477, 0.016886770286372477, 0.1504065370532447, 0.1504065370532447, 0.1504065370532447, 0.1504065370532447, 0.45121961115973414, 0.18283113177110066, 0.045707782942775166, 0.7313245270844027, 0.045707782942775166, 0.21219628852287162, 0.5612033420144368, 0.025128507851392694, 0.1033060878335033, 0.08934580569384068, 0.0027920564279325212, 0.0027920564279325212, 0.8217192657223666, 0.09940152407931854, 0.03313384135977285, 0.01325353654390914, 0.01325353654390914, 0.01325353654390914, 0.6737590026530887, 0.14520668160626912, 0.04356200448188074, 0.06969920717100918, 0.05517853901038227, 0.008712400896376147, 0.0029041336321253826, 0.0755337586748561, 0.0755337586748561, 0.7553375867485609, 0.008522376627377913, 0.9545061822663263, 0.008522376627377913, 0.008522376627377913, 0.017044753254755827, 0.008522376627377913, 0.2462635334671067, 0.2462635334671067, 0.2462635334671067, 0.48714297418273933, 0.27534168105980916, 0.021180129312293013, 0.04236025862458603, 0.06354038793687904, 0.04236025862458603, 0.06354038793687904, 0.1604488733967804, 0.1604488733967804, 0.1604488733967804, 0.48134662019034125, 0.9203917180154618, 0.020453149289232486, 0.020453149289232486, 0.020453149289232486, 0.02348696830624603, 0.915991763943595, 0.02348696830624603, 0.02348696830624603, 0.02348696830624603, 0.29960938212084537, 0.3289828509562224, 0.176240813012262, 0.0763710189719802, 0.029373468835377, 0.0822457127390556, 0.0058746937670754, 0.49734633437320686, 0.07104947633902955, 0.35524738169514775, 0.27810572432682273, 0.27810572432682273, 0.10428964662255853, 0.24334250878596989, 0.06952643108170568, 0.03476321554085284, 0.03476321554085284, 0.15179001193746536, 0.15179001193746536, 0.15179001193746536, 0.4553700358123961, 0.07990873184200868, 0.07990873184200868, 0.7191785865780782, 0.03576869656675955, 0.03576869656675955, 0.8584487176022293, 0.03576869656675955, 0.01975054107822493, 0.9282754306765717, 0.01975054107822493, 0.01975054107822493, 0.01975054107822493, 0.023082680264131787, 0.9002245303011397, 0.023082680264131787, 0.023082680264131787, 0.023082680264131787, 0.13582487281208047, 0.7306441434029156, 0.051519779342513276, 0.01405084891159453, 0.05620339564637812, 0.007025424455797265, 0.07393273193149054, 0.11089909789723582, 0.7393273193149055, 0.03696636596574527, 0.272416952512001, 0.29403734556850897, 0.28538918834590576, 0.03459262889041282, 0.051888943335619234, 0.06486117916952404, 0.11897188353318545, 0.11897188353318545, 0.11897188353318545, 0.5948594176659273, 0.04813377815986595, 0.89047489595752, 0.024066889079932974, 0.024066889079932974, 0.11900950689081892, 0.11900950689081892, 0.11900950689081892, 0.5950475344540945, 0.9556426438763754, 0.016382445323595007, 0.0054608151078650025, 0.0054608151078650025, 0.0054608151078650025, 0.010921630215730005, 0.8400822047285248, 0.046671233596029156, 0.046671233596029156, 0.29214224626946494, 0.3349897757223198, 0.2882470163192054, 0.0038952299502595326, 0.05063798935337392, 0.03116183960207626, 0.0038952299502595326, 0.7021453357132597, 0.048092146281730114, 0.06732900479442215, 0.15389486810153635, 0.024046073140865057, 0.05794288223573949, 0.05794288223573949, 0.05794288223573949, 0.811200351300353, 0.23762593121134157, 0.5940648280283539, 0.019802160934278463, 0.019802160934278463, 0.019802160934278463, 0.029703241401417696, 0.07920864373711385, 0.3446122676572053, 0.4231315185158091, 0.14249789970635496, 0.042167745831472384, 0.036351505027131366, 0.010178421407596783, 0.0014540602010852546, 0.04253662666814865, 0.04253662666814865, 0.8507325333629729, 0.04253662666814865, 0.23998886490884475, 0.23998886490884475, 0.23998886490884475, 0.08860375381703281, 0.044301876908516405, 0.044301876908516405, 0.7974337843532953, 0.13876091514552008, 0.5066386901824802, 0.29365682042424013, 0.006453996053280003, 0.012907992106560006, 0.04517797237296002, 0.3862546176332146, 0.3344662890120015, 0.21578470258838806, 0.010789235129419403, 0.025894164310606566, 0.025894164310606566, 0.013560212135180393, 0.8814137887867256, 0.013560212135180393, 0.06780106067590197, 0.013560212135180393, 0.013560212135180393, 0.10045665996169066, 0.5022832998084533, 0.10045665996169066, 0.10045665996169066, 0.20091331992338132, 0.028285857433092393, 0.9051474378589566, 0.028285857433092393, 0.028285857433092393, 0.25657895403434167, 0.25657895403434167, 0.25657895403434167, 0.07960170167696497, 0.7445100333316136, 0.03277717127875028, 0.028094718238928815, 0.05618943647785763, 0.05618943647785763, 0.2109689645317836, 0.2109689645317836, 0.2109689645317836, 0.2109689645317836, 0.5043059888256142, 0.11070131462025677, 0.04920058427566967, 0.31980379779185286, 0.1089177702365983, 0.1089177702365983, 0.1089177702365983, 0.6535066214195897, 0.46706663026780937, 0.30389271136014523, 0.12949122922488945, 0.03592820232829303, 0.03817371497381134, 0.024700639100701458, 0.0014970084303455428, 0.3601330531579918, 0.40014783684221317, 0.16005913473688527, 0.04001478368422132, 0.04001478368422132, 0.04001478368422132, 0.3125103614370977, 0.2678660240889409, 0.08928867469631363, 0.13393301204447045, 0.08928867469631363, 0.08928867469631363, 0.5341205356597141, 0.13353013391492852, 0.13353013391492852, 0.13353013391492852, 0.1931257476080921, 0.15450059808647368, 0.6180023923458947, 0.01931257476080921, 0.01931257476080921, 0.4084549062298635, 0.31690466862661826, 0.10739547103457618, 0.04929628178636284, 0.11267721551168648, 0.0035211629847402026, 0.0017605814923701013, 0.06352558470731598, 0.7305442241341338, 0.03176279235365799, 0.015881396176828995, 0.15881396176828996, 0.7014611837305956, 0.15991861183495443, 0.08359381982281709, 0.025441597337379114, 0.02180708343203924, 0.0036345139053398736, 0.0036345139053398736, 0.8927124864959403, 0.037196353603997506, 0.037196353603997506, 0.037196353603997506, 0.5166539310262231, 0.2505771565477182, 0.1498296399976047, 0.025832696551311155, 0.03616577517183562, 0.015499617930786695, 0.007749808965393347, 0.8850105129643137, 0.038478717954970154, 0.038478717954970154, 0.038478717954970154, 0.8287441307424339, 0.05781935795877446, 0.06552860568661105, 0.023127743183509783, 0.01541849545567319, 0.007709247727836595, 0.02152012083745802, 0.6599503723487126, 0.15064084586220614, 0.007173373612486007, 0.035866868062430037, 0.1291207250247481, 0.07995335863303932, 0.07995335863303932, 0.7195802276973539, 0.022188809285704162, 0.8653635621424622, 0.022188809285704162, 0.022188809285704162, 0.022188809285704162, 0.044377618571408324, 0.056719242667573846, 0.056719242667573846, 0.7940693973460339, 0.14246324106210223, 0.14246324106210223, 0.07123162053105112, 0.07123162053105112, 0.64108458477946, 0.0727014469610664, 0.0727014469610664, 0.0727014469610664, 0.727014469610664, 0.05018926985205084, 0.05018926985205084, 0.05018926985205084, 0.05018926985205084, 0.8030283176328135, 0.32372449939489983, 0.4447124436131958, 0.09809833314996966, 0.05231911101331715, 0.01961966662999393, 0.01961966662999393, 0.042509277698320184, 0.38113937950702376, 0.3448403909825453, 0.17890215772778667, 0.05185569789211208, 0.02592784894605604, 0.015556709367633623, 0.09090431311343458, 0.09090431311343458, 0.09090431311343458, 0.636330191794042, 0.21721869563020996, 0.06723435817125546, 0.6413123394796675, 0.046546863349330704, 0.010343747410962379, 0.020687494821924758, 0.06489918951680793, 0.06489918951680793, 0.7787902742016951], \"Term\": [\"abnormally\", \"abnormally\", \"abnormally\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"actor_critic\", \"actor_critic\", \"actor_critic\", \"actor_critic\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"aggregator\", \"aggregator\", \"aggregator\", \"aggregator\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"amplifier\", \"amplifier\", \"amplifier\", \"amplifier\", \"analog\", \"analog\", \"analog\", \"analog\", \"analog\", \"analog\", \"analog_vlsi\", \"analog_vlsi\", \"analog_vlsi\", \"analog_vlsi\", \"analogy\", \"analogy\", \"analogy\", \"analogy\", \"analogy\", \"analogy\", \"analogy_make\", \"analogy_make\", \"analogy_make\", \"analogy_make\", \"analogy_make\", \"animation\", \"animation\", \"animation\", \"animation\", \"animation\", \"anna\", \"anna\", \"anna\", \"anna\", \"anna\", \"anna_chip\", \"anna_chip\", \"anna_chip\", \"anna_chip\", \"anna_chip\", \"api\", \"api\", \"api\", \"api\", \"apid\", \"apid\", \"apid\", \"apid\", \"apid_lspi\", \"apid_lspi\", \"apid_lspi\", \"apid_lspi\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"array\", \"array\", \"array\", \"array\", \"assertion\", \"assertion\", \"assertion\", \"assertion\", \"asset_allocation\", \"asset_allocation\", \"asset_allocation\", \"asset_allocation\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"auction\", \"auction\", \"auction\", \"bag\", \"bag\", \"bag\", \"bag\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis_function\", \"basis_function\", \"basis_function\", \"basis_function\", \"basis_function\", \"basis_function\", \"bcis\", \"bcis\", \"bcis\", \"belief_state\", \"belief_state\", \"belief_state\", \"belief_state\", \"belief_state\", \"bellman_error\", \"bellman_error\", \"bellman_error\", \"bellman_error\", \"best_arm\", \"best_arm\", \"best_arm\", \"best_arm\", \"best_arm\", \"bilingual\", \"bilingual\", \"bilingual\", \"bind\", \"bind\", \"bind\", \"bind\", \"bind\", \"blcp\", \"blcp\", \"blcp\", \"blcp\", \"blcp\", \"board\", \"board\", \"board\", \"board\", \"board\", \"boost\", \"boost\", \"boost\", \"boost\", \"boost\", \"booster\", \"booster\", \"booster\", \"booster\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"bug\", \"buggy\", \"buggy\", \"buggy\", \"bus\", \"bus\", \"bus\", \"bus\", \"bus\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"bypass_operation\", \"bypass_operation\", \"bypass_operation\", \"bypass_operation\", \"cal\", \"cal\", \"cal\", \"cal\", \"calibrator\", \"calibrator\", \"calibrator\", \"cand\", \"cand\", \"cand\", \"cand\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"causal_ordere\", \"causal_ordere\", \"causal_ordere\", \"causal_ordere\", \"ccn\", \"ccn\", \"ccn\", \"ccn\", \"ccrypt\", \"ccrypt\", \"ccrypt\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"channel\", \"channel\", \"channel\", \"channel\", \"channel\", \"channel\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"co_regularization\", \"co_regularization\", \"co_regularization\", \"co_regularization\", \"code\", \"code\", \"code\", \"code\", \"code\", \"communication_round\", \"communication_round\", \"communication_round\", \"communication_round\", \"communication_round\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complication\", \"complication\", \"complication\", \"complication\", \"complication\", \"component\", \"component\", \"component\", \"component\", \"component\", \"component\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"coordinate\", \"coordinate\", \"coordinate\", \"coordinate\", \"coordinate\", \"coordinate\", \"coreset\", \"coreset\", \"coreset\", \"coreset\", \"coronary_artery\", \"coronary_artery\", \"coronary_artery\", \"coronary_artery\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"count\", \"count\", \"count\", \"count\", \"count\", \"count\", \"countdown\", \"countdown\", \"countdown\", \"counter\", \"counter\", \"counter\", \"counter\", \"counter\", \"counterexamplei\", \"counterexamplei\", \"counterexamplei\", \"crammer\", \"crammer\", \"crammer\", \"crammer\", \"crash\", \"crash\", \"crash\", \"crash\", \"crash\", \"cross_validation\", \"cross_validation\", \"cross_validation\", \"cross_validation\", \"cross_validation\", \"cross_validation\", \"crystal\", \"crystal\", \"crystal\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue_invariant\", \"cue_invariant\", \"cue_invariant\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cyclic_line\", \"cyclic_line\", \"cyclic_line\", \"cyclic_line\", \"dag\", \"dag\", \"dag\", \"dag\", \"dagger\", \"dagger\", \"dagger\", \"dagger\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"day\", \"day\", \"day\", \"day\", \"day\", \"debt\", \"debt\", \"debt\", \"debt\", \"debug\", \"debug\", \"debug\", \"debug\", \"debug\", \"debugging\", \"debugging\", \"debugging\", \"decode\", \"decode\", \"decode\", \"decode\", \"decode\", \"decode\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"determinism\", \"determinism\", \"determinism\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"deterministic\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"disentangle\", \"disentangle\", \"disentangle\", \"disentangle\", \"disentangle\", \"disentangle\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"dof\", \"dof\", \"dof\", \"dof\", \"dropout\", \"dropout\", \"dropout\", \"dsp\", \"dsp\", \"dsp\", \"dsp\", \"dsp\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg_source\", \"eeg_source\", \"eeg_source\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"eof\", \"eof\", \"eof\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"examplei\", \"examplei\", \"examplei\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"false\", \"false\", \"false\", \"false\", \"false\", \"false_positive\", \"false_positive\", \"false_positive\", \"false_positive\", \"false_positive\", \"fcp\", \"fcp\", \"fcp\", \"fcp\", \"feasible_region\", \"feasible_region\", \"feasible_region\", \"feasible_region\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature_selection\", \"feature_selection\", \"feature_selection\", \"feature_selection\", \"feature_selection\", \"feature_selection\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedforward\", \"feedforward\", \"feedforward\", \"feedforward\", \"female\", \"female\", \"female\", \"female\", \"female\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"file\", \"file\", \"file\", \"file\", \"file\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fitc\", \"fitc\", \"fitc\", \"fitc\", \"fitc\", \"fixed_point\", \"fixed_point\", \"fixed_point\", \"fixed_point\", \"fixed_point\", \"fixed_point\", \"floating_gate\", \"floating_gate\", \"floating_gate\", \"floating_gate\", \"floating_gate\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follower\", \"follower\", \"follower\", \"follower\", \"frank_wolfe\", \"frank_wolfe\", \"frank_wolfe\", \"frank_wolfe\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"fu\", \"fu\", \"fu\", \"fu\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"gap_safe\", \"gap_safe\", \"gap_safe\", \"gender\", \"gender\", \"gender\", \"gender\", \"gender\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"green_node\", \"green_node\", \"green_node\", \"green_node\", \"gun\", \"gun\", \"gun\", \"hardware\", \"hardware\", \"hardware\", \"hardware\", \"hardware\", \"hash\", \"hash\", \"hash\", \"heap\", \"heap\", \"heap\", \"helimination\", \"helimination\", \"helimination\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"highway_network\", \"highway_network\", \"highway_network\", \"highway_network\", \"homotopy_path\", \"homotopy_path\", \"homotopy_path\", \"homotopy_path\", \"homotopy_path\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"ibp\", \"ibp\", \"ibp\", \"illusory\", \"illusory\", \"illusory\", \"illusory\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image_identification\", \"image_identification\", \"image_identification\", \"image_identification\", \"imbalance\", \"imbalance\", \"imbalance\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"implement\", \"inactive\", \"inactive\", \"inactive\", \"inactive\", \"inactive\", \"inducer\", \"inducer\", \"inducer\", \"inducer\", \"inducing_input\", \"inducing_input\", \"inducing_input\", \"inducing_input\", \"inducing_input\", \"infeasible\", \"infeasible\", \"infeasible\", \"infeasible\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"instruction\", \"instruction\", \"instruction\", \"instruction\", \"instruction\", \"interconnect\", \"interconnect\", \"interconnect\", \"interconnect\", \"interconnect\", \"interconnects_architecture\", \"interconnects_architecture\", \"interconnects_architecture\", \"interconnects_architecture\", \"interference\", \"interference\", \"interference\", \"interference\", \"inverter\", \"inverter\", \"inverter\", \"inverter\", \"investor\", \"investor\", \"investor\", \"investor\", \"item\", \"item\", \"item\", \"item\", \"item\", \"kernel_embedde\", \"kernel_embedde\", \"kernel_embedde\", \"kernel_embedding\", \"kernel_embedding\", \"kernel_embedding\", \"knn_graph\", \"knn_graph\", \"knn_graph\", \"knn_graph\", \"knob\", \"knob\", \"knob\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"kxk\", \"kxk\", \"kxk\", \"kxk\", \"ladd\", \"ladd\", \"ladd\", \"ladd\", \"ladd\", \"lamp\", \"lamp\", \"lamp\", \"language\", \"language\", \"language\", \"language\", \"language\", \"larq\", \"larq\", \"larq\", \"larq\", \"larq\", \"lars_td\", \"lars_td\", \"lars_td\", \"lars_td\", \"lars_td\", \"late_visual\", \"late_visual\", \"late_visual\", \"late_visual\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"lattice\", \"lattice\", \"lattice\", \"lattice\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"lc_td\", \"lc_td\", \"lc_td\", \"lc_td\", \"lc_td\", \"lcp\", \"lcp\", \"lcp\", \"lcp\", \"lcp\", \"ldeep\", \"ldeep\", \"ldeep\", \"ldeep\", \"ldeep\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learnedge\", \"learnedge\", \"learnedge\", \"learnedge\", \"learner\", \"learner\", \"learner\", \"learner\", \"learner\", \"learnhull\", \"learnhull\", \"learnhull\", \"learnhull\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"level_alignment\", \"level_alignment\", \"level_alignment\", \"liblit\", \"liblit\", \"liblit\", \"likelihood_ratio\", \"likelihood_ratio\", \"likelihood_ratio\", \"likelihood_ratio\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear_complementarity\", \"linear_complementarity\", \"linear_complementarity\", \"linear_complementarity\", \"linear_complementarity\", \"lisp\", \"lisp\", \"lisp\", \"lisp\", \"lisp\", \"lisp_interpret\", \"lisp_interpret\", \"lisp_interpret\", \"lisp_interpret\", \"lisp_interpret\", \"lmul\", \"lmul\", \"lmul\", \"lmul\", \"lmul\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lure\", \"lure\", \"lure\", \"lure\", \"male\", \"male\", \"male\", \"male\", \"marker\", \"marker\", \"marker\", \"marker\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"mgomp\", \"mgomp\", \"mgomp\", \"mgomp\", \"mgomp\", \"minimal\", \"minimal\", \"minimal\", \"minimal\", \"minimal\", \"minimal\", \"minimal_weight\", \"minimal_weight\", \"minimal_weight\", \"minimal_weight\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"misspecifie\", \"misspecifie\", \"misspecifie\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake_bound\", \"mistake_bound\", \"mistake_bound\", \"mistake_bound\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"mortality\", \"mortality\", \"mortality\", \"mortality\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"natural_image\", \"natural_image\", \"natural_image\", \"natural_image\", \"natural_image\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"nonparanormal\", \"nonparanormal\", \"nonparanormal\", \"nonparanormal\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"od\", \"od\", \"od\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"order_optimality\", \"order_optimality\", \"order_optimality\", \"order_optimality\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pd_array\", \"pd_array\", \"pd_array\", \"pd_array\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"permutation\", \"permutation\", \"permutation\", \"permutation\", \"permutation_complexity\", \"permutation_complexity\", \"permutation_complexity\", \"permutation_complexity\", \"pes\", \"pes\", \"pes\", \"pes\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"photodiode\", \"photodiode\", \"photodiode\", \"photodiode\", \"pinpoint\", \"pinpoint\", \"pinpoint\", \"player\", \"player\", \"player\", \"player\", \"player\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pointer\", \"pointer\", \"pointer\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy_iteration\", \"policy_iteration\", \"policy_iteration\", \"policy_iteration\", \"policy_iteration\", \"populational\", \"populational\", \"populational\", \"populational\", \"portfolio\", \"portfolio\", \"portfolio\", \"portfolio\", \"pq\", \"pq\", \"pq\", \"pq\", \"predicate\", \"predicate\", \"predicate\", \"preoperative\", \"preoperative\", \"preoperative\", \"preoperative\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price_auction\", \"price_auction\", \"price_auction\", \"principal_component\", \"principal_component\", \"principal_component\", \"principal_component\", \"principal_component\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing_stage\", \"processing_stage\", \"processing_stage\", \"processing_stage\", \"processor\", \"processor\", \"processor\", \"processor\", \"processor\", \"processor\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"pseudo_ensemble\", \"pseudo_ensemble\", \"pseudo_ensemble\", \"qf\", \"qf\", \"qf\", \"qf\", \"qmax\", \"qmax\", \"qmax\", \"qmax\", \"qmax\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"questionable\", \"questionable\", \"questionable\", \"questionable\", \"radial_basis\", \"radial_basis\", \"radial_basis\", \"radial_basis\", \"radial_basis\", \"radial_basis\", \"radial_basis\", \"rail\", \"rail\", \"rail\", \"rail\", \"rat\", \"rat\", \"rat\", \"rat\", \"rat\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"receptive_field\", \"receptive_field\", \"receptive_field\", \"receptive_field\", \"receptive_field\", \"receptive_field\", \"recollect\", \"recollect\", \"recollect\", \"recollect\", \"recollection\", \"recollection\", \"recollection\", \"recollection\", \"recollection\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"reilly\", \"reilly\", \"reilly\", \"reilly\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"renal_failure\", \"renal_failure\", \"renal_failure\", \"renal_failure\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response_latency\", \"response_latency\", \"response_latency\", \"response_latency\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward_distribution\", \"reward_distribution\", \"reward_distribution\", \"reward_distribution\", \"rich_recollection\", \"rich_recollection\", \"rich_recollection\", \"rich_recollection\", \"right_motor\", \"right_motor\", \"right_motor\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk_prediction\", \"risk_prediction\", \"risk_prediction\", \"risk_prediction\", \"robot\", \"robot\", \"robot\", \"robot\", \"root\", \"root\", \"root\", \"root\", \"root\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"round\", \"round\", \"round\", \"round\", \"round\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"sacking\", \"sacking\", \"sacking\", \"sacking\", \"sacking\", \"sajda\", \"sajda\", \"sajda\", \"sajda\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"score\", \"score\", \"score\", \"score\", \"score\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"seller\", \"seller\", \"seller\", \"seller\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic_correlation\", \"semantic_correlation\", \"semantic_correlation\", \"sense_amplifi\", \"sense_amplifi\", \"sense_amplifi\", \"sense_amplifi\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sequencer\", \"sequencer\", \"sequencer\", \"sequencer\", \"sequencer\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sgn\", \"sgn\", \"sgn\", \"sgn\", \"shortest_path\", \"shortest_path\", \"shortest_path\", \"shortest_path\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"singularity\", \"singularity\", \"singularity\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"sketch\", \"sketch\", \"sketch\", \"sketch\", \"slot\", \"slot\", \"slot\", \"slot\", \"smoking\", \"smoking\", \"smoking\", \"software\", \"software\", \"software\", \"software\", \"software\", \"software\", \"software\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"sparse_refire\", \"sparse_refire\", \"sparse_refire\", \"spatial_phase\", \"spatial_phase\", \"spatial_phase\", \"spectral_clustere\", \"spectral_clustere\", \"spectral_clustere\", \"spectral_clustere\", \"speedup\", \"speedup\", \"speedup\", \"speedup\", \"speedup\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sprite\", \"sssl\", \"sssl\", \"sssl\", \"sssl\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"stick_breake\", \"stick_breake\", \"stick_breake\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"storagec\", \"storagec\", \"storagec\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"stroke\", \"stroke\", \"stroke\", \"stroke\", \"structured_prediction\", \"structured_prediction\", \"structured_prediction\", \"structured_prediction\", \"student\", \"student\", \"student\", \"student\", \"student\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"subgradient\", \"subgradient\", \"subgradient\", \"successful\", \"successful\", \"successful\", \"successful\", \"successful\", \"successful\", \"successful\", \"surgery\", \"surgery\", \"surgery\", \"surgery\", \"surplus\", \"surplus\", \"surplus\", \"sy\", \"sy\", \"sy\", \"sy\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tag\", \"tag\", \"tag\", \"tag\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"tdsi\", \"tdsi\", \"tdsi\", \"tdsi\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"temporal_binde\", \"temporal_binde\", \"temporal_binde\", \"temporal_binde\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor_power\", \"tensor_power\", \"tensor_power\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"thompson_sample\", \"thompson_sample\", \"thompson_sample\", \"thompson_sample\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timestep\", \"timestep\", \"timestep\", \"timestep\", \"toss\", \"toss\", \"toss\", \"tournament\", \"tournament\", \"tournament\", \"tournament\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"tunneling\", \"tunneling\", \"tunneling\", \"tunneling\", \"tz\", \"tz\", \"tz\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unity\", \"unity\", \"unity\", \"unity\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unweighte\", \"unweighte\", \"unweighte\", \"unweighte\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"uv\", \"uv\", \"uv\", \"uv\", \"validation\", \"validation\", \"validation\", \"validation\", \"validation\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variability\", \"variability\", \"variability\", \"variability\", \"variability\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable_selection\", \"variable_selection\", \"variable_selection\", \"variable_selection\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vfe\", \"vfe\", \"vfe\", \"vfe\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"vjk\", \"vjk\", \"vjk\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"volume_conduction\", \"volume_conduction\", \"volume_conduction\", \"warm_start\", \"warm_start\", \"warm_start\", \"warm_start\", \"warm_start\", \"wei_bull\", \"wei_bull\", \"wei_bull\", \"wei_bull\", \"weibull\", \"weibull\", \"weibull\", \"weibull\", \"weibull\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wer\", \"wer\", \"wer\", \"wer\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word_representation\", \"word_representation\", \"word_representation\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 3, 1, 5, 7, 6, 8, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el4751337409906151043562663644\", ldavis_el4751337409906151043562663644_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el4751337409906151043562663644\", ldavis_el4751337409906151043562663644_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el4751337409906151043562663644\", ldavis_el4751337409906151043562663644_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhjELJ_z5rLm"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "\n",
        "We started with understanding why evaluating the topic model is essential. Next, we reviewed existing methods and scratched the surface of topic coherence, along with the available coherence measures. Then we built a default LDA model using Gensim implementation to establish the baseline coherence score and reviewed practical ways to optimize the LDA hyperparameters.\n",
        "\n",
        "Hopefully, this article has managed to shed light on the underlying topic evaluation strategies, and intuitions behind it.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
        "2. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\n",
        "3. https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
        "4. https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb\n",
        "5. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "6. http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
        "7. http://palmetto.aksw.org/palmetto-webapp/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}